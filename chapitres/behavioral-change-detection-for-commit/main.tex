\chapter{Behavioural Change Detection for Commit}

\begin{center}
	\begin{mdframed}
		\large
	When a developer pushes a change to an application's codebase, in the form of a commit, the newly introduced behavior may be incorrect. To prevent such regressions, developers rely on a continuous integration (CI) server to run a test suite on the application, at every commit. However, if the test suite lacks the test cases that specify the behavioral changes introduced by this commit, or the changes introduced by previous commits, the bug goes undetected.
	
	In this paper, we propose an approach that takes a program, its test suite, and a commit as input, and generates test methods that detect the behavioral changes of the commit, i.e., the behavioral difference between the pre-commit and post-commit versions of the program.
	In essence, this helps developers evolve the test suite (i.e., the application's specification) at the same time they evolve the application.
	We call our approach DCI (Detecting behavioral changes in CI). It works by generating variations of the existing test cases through (i) assertion amplification and (ii) a search-based exploration of input spaces. 
	We evaluate our approach on a curated set of 60 commits from 6 open source Java projects. 
	Our study exposes the characteristics of commits in modern open-source software and the ability of our approach to generate test methods that detect behavioral changes.
	\end{mdframed}
\end{center}

\graphicspath{{.}{chapitres/behavioral-change-detection-for-commit/}}

\minitoc

% -----------------------
%  Introduction
% -----------------------
\section{Introduction}

% intro CI
%In the context of collaborative software development, 
In collaborative software projects,
developers work in parallel on the same code base. Every time a developer integrates her changes, she submits them in the form of a \emph{commit} to a version control system.
The \emph{Continuous Integration} (CI) server~\cite{fowler2006continuous} merges the commit with the master branch, compiles and automatically runs the test suite to check that the commit behaves as expected.
Its ability to detect bugs early makes CI an essential contribution to quality assurance~\cite{Hilton:2016:UsageCI,duvall2007continuous}.

However, the effectiveness of Continuous Integration depends on one key property: each commit should include at least one test case $t_{new}$ that specifies the intended change.
For instance, assume one wants to integrate a bug fix.
In this case, the developer is expected to include a new test method, $t_{new}$, that specifies the program's desired behavior after the bug fix is applied.
This can be mechanically verified: $t_{new}$ should fail on the version of the code that does not include the fix (the \emph{pre-commit} version), and pass on the version that includes the fix (the \emph{post-commit} version).
However, many commits either do not include a $t_{new}$ or $t_{new}$ does not meet this fail/pass criterion.
The reason is that developers sometimes cut corners because of lack of time, expertise or discipline. This is the problem we address in this paper.

In this paper, we aim to automatically generate test methods for each commit that is submitted to the CI.
In particular, we generate a test case $t_{gen}$ that specifies the behavioral change of each commit.
We consider a generated test case $t_{gen}$ to be relevant if it satisfies the following property: $t_{gen}$ \textit{passes} on the pre-commit version and \textit{fails} on the post-commit version.
To do so, we developed a new approach, called DCI, that works in two steps.
First, we analyze the test cases of the pre-commit version and select the ones that exercise the parts of the code modified by the commit.
Second, our test generation techniques produce variant test cases that either add assertions~\cite{TaoXie2006} to existing tests or explore new inputs following a search-based test input generation approach~\cite{tonella}.
This process of automatic generation of $t_{gen}$ from existing tests is called \emph{test amplification}~\cite{zhang2012}.
We evaluate our approach on a benchmark of 50 commits selected from 5 open source Java projects, constructed with a novel and systematic methodology.
We analyzed 1510 commits and selected those that introduce a behavioral change (\eg, we do not want to generate tests for commits that only change comments).
We also make sure that all selected commits contain a developer-written test case that detects the behavioral change.
In our protocol, the developer's test case acts as a ground-truth to analyze the tests generated by DCI.
Overall, we found 50 commits that satisfy the two essential properties we are looking for:
1) the commit introduces a behavioral change;
2) the commit has a human written test we can use for ground truth.

To sum up, our contributions are:
\begin{itemize}
%\item The problem statement of behavioral change detection in the context of Continuous Integration.
\item DCI (\textbf{D}etecting behavioral changes in \textbf{CI}), an approach based on \emph{test amplification} to generate new tests that detect the behavioral change introduced by a commit.
\item An open-source implementation of DCI for Java.
\item A curated benchmark of 60 commits that introduce a behavioral change and include a test case to detect it, selected from 6 notable open source Java projects\footnote{\url{https://github.com/danglotb/DCI}}.
\item A comprehensive evaluation based on 4 research questions that combines quantitative and qualitative analysis with manual assessment.
\end{itemize}

In \Autoref{sec:background} we motivate the need to have commits include a test case that specifies the behavioral change. In \Autoref{sec:techniques} we introduce our technical contribution: an approach for commit-based test selection and amplification. \Autoref{sec:evaluation} introduces our benchmark of commits, the evaluation protocol and the results of our experiments on 50 real commits. 
Section \ref{sec:threats} relates the threats validity and actions that have been taken to overcome them. In Section  \ref{sec:related_work}, we expose the related work, their evaluation and the differences with our work and eventually we conclude in Section \ref{sec:conclusion}.

% -----------------------
%  MOTIVATION
% -----------------------
\section{Motivation \& Background}
\label{sec:background}

In this section, we introduce an example to motivate the need to generate new tests that specifically target the behavioral change introduced by a commit.
Then we introduce the key concepts on which we elaborate our solution to address this challenging test generation task.

\subsection{Motivating Example}

On August 10, a developer pushed a commit to the master branch of the XWiki-commons project. 
The change\footnote{\url{https://github.com/xwiki/xwiki-commons/commit/7e79f77}}, displayed in \Autoref{fig:motivating_example}, adds a comparison to ensure the equality of the objects returned by \texttt{getVersion()}.
The developer did not write a test method nor modify an existing one. 

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.95\linewidth, trim=7.3cm 8.6cm 4.4cm 14.5cm, clip]{img/motivating_example.pdf}}
\caption{Commit 7e79f77 on XWiki-Commons that changes the behavior without a test.}
\label{fig:motivating_example}
\end{figure}

In this commit, the intent is to take into account the \texttt{version} (from method \texttt{getVersion}) in the \texttt{equals} method.
This change impacts the behavior of all methods that use it, \texttt{equals} being a highly used method.
Such a central behavioral change may impact the whole program, and the lack of a test case for this new behavior may have dramatic consequences in the future.
Without a test case, this change could be reverted and go undetected by the test suite and the Continuous Integration server, \ie the build would still pass.
Yet, a user of this program would encounter new errors, because of the changed behavior.
The developer took a risk when committing this change without a test case.

Our work on automatic test amplification in continuous integration aims at mitigating such risk: test amplification aims at ensuring that every commit include a new test method or a modification of an existing test method.
In this paper, we study how to automatically obtain a test method that highlights the behavioral change introduced by a commit.
This test method allows to identify the behavioral difference between the two versions of the program. Our goal is to use this new test method to ensure that any changed behavior can be caught in the future.

What we propose is as follows: when Continuous Integration is triggered, rather than just executing the test suite to find regressions, it could also run an analysis of the commit to know if it contains a behavioral change, in the form of a new method or the modification of an existing one.
If there is no appropriate test case to detect a behavioral change, our approach would provide one. DCI would take as input the commit and a test suite, and generate a new test case that detects the behavioral change.

\subsection{Practibility}
\label{subsec:practicability}

We describe a complete scenario to sum up our vision of our approach's usage.

A developer commits a change into the program.
The Continuous Integration service is triggered;
the CI analyzes the commit.
There are two potential outcomes:
1) the developer provided a new test case or a modification to an existing one. In this case, the CI runs as usual, \eg it executes the test suite;
2) the developer did not provide a new test nor the modification of an existing one, the CI runs DCI on the commit to obtain a test method that detects the behavioral change and present it to the developer.
The developer can then validate the new test method that detects the behavioral change.
Following our definition, the new test method passes on the pre-commit version but fails on the post-commit version.
The current amplified test method cannot be added to the test suite, since it fails.
However, this test method is still useful, since one has only to negate the failing assertions, \eg change an \texttt{assertTrue} into an \texttt{assertFalse}, to obtain a valid and passing test method that explicitly executes the new behavior.
This can be done manually or automatically with approaches such as ReAssert\cite{ReAssert}.

From our experience, unit tests (vs integration test) are the best target for DCI
The reasons are behind the very nature of unit tests:
First, they have a small scope, which allow DCI to intensify its search while an integration test, that contains a lot of code, would make DCI explore the neighborhood in different ways.
Second, that is a consequence of the first, the unit tests are fast to be executed against integration test.
Since DCI needs to execute 5 times the tests under amplification, it means that DCI would be executed faster when it amplifies unit tests than when it amplified integration tests.

DCI has been designed to be easy to use.
The only cost of DCI is the time to set it up: in the ideal, happy-path case, it is meant to be a single command line through Maven goals.
Once DCI is set up in continuous integration, it automatically runs at each commit and developers directly benefit from amplified test methods that strengthen the existing test suite.

%
%   Behavioral Change
%
\subsection{Behavioral Change}
\label{subsec:behavioral:change}

A \emph{behavioral change} is a source-code modification that triggers a new state for some inputs \cite{saff2004experimental}.
Considering the pre-commit version $P$ and the post-commit version $P'$ of a program, the commit introduces a behavioral change if it is possible to implement a test case that can trigger and observe the change, \ie, it passes on $P$ and fails on $P'$, or the opposite.
In short, the behavioral change must have an impact on the observable behavior of the program.

%
%   Behavioral Change Detection
%
\subsection{Behavioral Change Detection}
\label{subsec:behavioral:change:detection}

Behavioral change detection is the task of identifying or generating a test or an input that distinguishes a behavioral change between two versions of the same program.
In this paper, we propose a novel approach to detect behavioral changes based on test amplification.

%
%   Test amplification
%
\subsection{Test Amplification}

Test amplification is the idea of improving existing tests with respect to a specific test criterion~\cite{zhang2012}.
We start from an existing test suite and create variant tests that improve a given test objective.
For instance, a test amplification tool may improve the code coverage of the test suite.
In this paper, our test objective is to improve the test suite's detection of behavioral changes introduced by commits.

% -----------------------
%  CONTRIBUTIONS
% -----------------------

\section{Behavioral Change Detection Approach}
\label{sec:techniques}

We propose an approach to produce test methods that detect the behavioral changes introduced by commits. 
We call our approach DCI (\textbf{D}etecting behavioral changes in \textbf{CI}), and propose it be used during continuous integration.
%This approach is meant to be fully automated and used during continuous integration.

\subsection{Overview of DCI}
\label{subsec:global_overview}

DCI takes as input a program, its test suite, and a commit modifying the program.
The commit, as done in version control systems, is basically the diff between two consecutive versions of the program.
%
% output
DCI outputs new test methods that detect the behavioral difference between the pre- and post-commit versions of the program.
The new tests pass on a given version, but fail on the other, demonstrating the presence of a behavioral change captured.% by an assertion.

DCI computes the code coverage of the diff and selects test methods accordingly. % to this diff coverage.
Then, it applies two kinds of test amplification to generate new test methods that detect the behavioral change.
%
\Autoref{fig:global_approach} sums up the different phases of the approach:
1) Compute the diff coverage and select the tests to be amplified;
2) Amplify the selected tests based on the pre-commit version;
3) Execute amplified test methods against the post-commit version, and keep the failing test methods.
This process produces test methods that pass on the pre-commit version, fail on the post-commit version, hence they detect at least one behavioral change introduced by a given commit.

\begin{figure}
    \fbox{\includegraphics[width=.95\linewidth]{img/global_flow.pdf}}
    \caption{Overview of our approach to detect behavioral changes in commits.}
    \label{fig:global_approach}
\end{figure}

\subsection{Test Selection and Diff Coverage}
\label{subsec:compute_diff_coverage}
%We need to know which tests executed what lines of the diff.
DCI implements a feature that:
\begin{enumerate*}
\item reports the diff coverage of a commit, and
\item selects the set of unit tests that execute the diff.
\end{enumerate*}
%
To do so, DCI first computes the code coverage for the whole test suite.
Second, it identifies the test methods that hit the statements modified by the diff. 
Third, it produces the two outcomes elicited earlier: the diff coverage, computed as the ratio of statements in the diff covered by the test suite over the total number of statements in the diff and the list of test methods that cover the diff.
%
Then, we select only test methods that are present in pre-commit version (\ie, we ignore the test methods added in the commit, if any).
The final list of test methods that cover the diff is then used to seed the amplification process.

\subsection{Test Amplification}

Once we have the initial tests that cover the diff, we want to make them detect the behavioral change and assess the new behavior.
This process of extending the scope of a test case is called test amplification~\cite{zhang2012}.
In DCI, we build upon Xie's technique~\cite{TaoXie2006} and Tonella's evolutionary algorithm~\cite{tonella} to perform test amplification.

%   A-AMPL
\subsubsection{Assertion Amplification}
\label{subsec:aampl}

A test method consists of a setup and assertions.
The former is responsible for putting the program under test into a specific state; the latter is responsible for verifying that the actual state of the program at the end of the test is the expected one.
To do this, assertions compare actual values against expected values: if the assertion holds, the program is considered correct, if not, the test case has revealed the presence of a bug.

Assertion amplification (\aampl) has been proposed by \cite{TaoXie2006}.
It takes as input a program and its test suite, and it synthesizes new assertions on public methods that capture the program state.
The targeted public methods are those that take no parameter, return a result, and match a Java naming convention of getters, \eg the method starts with \emph{get} or \emph{is}. The standard method \emph{toString()} is also used.
If a method used returns a complex Java Object, \aampl recursively uses getters on this object to generate deeper assertions.

In case the test method sets the program into an incorrect state and an exception is thrown, \aampl generates a test for this exception by wrapping the test method body in a \emph{try/catch} block. 
It also inserts a \emph{fail} statement at the end of the body of the \emph{try}, \ie it means that if the exception is not thrown the test method fails.

\begin{algorithm}[h]
\begin{algorithmic}[1]
\Require{Program $P$}
\Require{Test Suite $TS$}
\Ensure{An Amplified Test Suite $ATS$}
\State{$ATS \leftarrow \emptyset$}
\For{$Test$ in $TS$}
    \State{$NoAssertTest \leftarrow removeAssertions(Test)$}
    \State{$InstrTest \leftarrow instrument(NoAssertTest)$}
    \State{$execute(InstrTest)$}
    \State{$AmplTest \leftarrow NoAssertTest.clone()$}
    \For{$Observ$ in $InstrTest.observations()$}
        \State{$Assert \leftarrow generateAssertion(Observ)$}
        \State{$AmplTest \leftarrow AmplTest.add(Assert)$}
    \EndFor
    \State{$ATS.add(select(AmplTest))$}
    \State{$ATS.add(AmplTest)$}
\EndFor
\Return $ATS$
\end{algorithmic}
\caption{\aampl: Assertion amplification algorithm.}
\label{algo:aampl}
\end{algorithm}

We present \aampl's pseudo-code in Algorithm \Autoref{algo:aampl}. First, it initializes an empty set of tests $ATS$ (Line 1). 
For each $Test$ method in the test suite $TS$ (Line 2), it removes the existing assertions to obtain $NoAssertTest$ (Line 3). 
Then, it instruments $NoAssertTest$ with observation points (Line 4) that allow retrieving values from the program at runtime, which results in $InstrTest$. 
In order to collect the values, it executes $InstrTest$ (Line 5).
Eventually, for each observation $Observ$ of the set of observations from $InstrTest$ (Line 7 to 10), it generates an assertion (Line 8) and adds it to the amplified tests $AmplTest$ (Line 9).
At the end, it selects amplified test according to a specific test criterion using the method $select()$ (Line 11) and add selected amplified test methods to the set of test methods $AmplTest$, in other words, an amplified test suite (Line 13).

To sum up, \aampl increases the number of assertions. 
By construction, it specifies more behaviors than the original test suite.
DCI$_{AAMPL}$ is the \aampl mode for DCI.

%
%   I-AMPL
%
\subsubsection{Search-based Amplification}
\label{subsec:sbampl}
Search-based test amplification consists in running stochastic transformations on test code~\cite{tonella}.
%
For DCI$_{AAMPL}$, this process consists in
%of search-based amplification is as follows:
%\begin{enumerate}
a) generating a set of original test methods by applying code transformations;
b) running \aampl to synthesize new assertions for these amplified test methods;
%\item select amplified tests to be kept, according to a given test criterion;
c) repeating this process $nb$ times\footnote{by default, $nb=3$}, each time seeding with the previously amplified test methods.
%\end{enumerate}
%
This final step allows the search-based algorithm to explore more inputs, and thus improve the chance of triggering new behaviors.

\begin{algorithm}[h]
\begin{algorithmic}[1]
\Require{Program $P$}
\Require{Program $P'$}
\Require{Test Suite $TS$}
\Require{Iterations number $Nb$}
\Ensure{An Amplified Test Suite $ATS$}
\State{$ATS \leftarrow \emptyset$}
\State{$TmpTests \leftarrow \emptyset$}
\For{$Test$ in $TS$}
    \State{$TmpTests \leftarrow Test$}
    \For{$i \leftarrow 0, i < Nb$}
        \State{$TransformedTests \leftarrow transform(TmpTests)$}
        \State{$AmplifiedTests \leftarrow aampl(TransformedTests)$}
        %\STATE{$ATS.add(AmplifiedTests)$}
        \State{$ATS.add(select(AmplifiedTests))$}
        \State{$TmpTests \leftarrow AmplifiedTests$}
    \EndFor
\EndFor
\Return $ATS$
\end{algorithmic}
\caption{\sbampl: Search based amplification algorithm}
\label{algo:sbampl}
\end{algorithm}

We present the search-based amplification algorithm in Algorithm \Autoref{algo:sbampl}.
This algorithm is a basic Hill Climbing algorithm.
It takes as input a program with two distinct versions $P$ and $P'$, its test suite $TS$ and a number of iterations $nb$, (in our case $nb=3$).
It produces an amplified test suite that contains test methods that pass on $P$ but fail on $P'$.
To do so, it initializes an empty set of amplified test methods $ATS$ (Line 1), which will be the final output, and $TmpTests$ (Line 2) which is a temporary set.
Then, for each test method in the test suite $TS$ (Line 3), it applies the following operations: 
1) transform the current set of test methods (Line 6) to obtain $TransformedTests$;
2) apply \aampl on $TransformedTests$ (Line 7, see \Autoref{algo:aampl}) to obtain $AmplifiedTests$; 
3) select amplified test methods using the method $select()$, and add them to $ATS$ 
(the method $select()$ executes the amplified tests on $P'$ and keeps only tests that fail, \ie that detect a behavioral change);
and Finally, 4) affects $AmplifiedTests$ to $TmpTests$ in order to stack transformations.

In our study, we consider the following test transformations:
\begin{itemize}
\item On numbers:
    \begin{enumerate*}
        \item add 1 to an integer
        \item minus 1 to an integer
        \item replace an integer by zero
        \item replace an integer by the maximum value (Integer.MAX\_VALUE in Java)
        \item replace an integer by the minimum value (\texttt{Integer.MIN\_VALUE} in Java).
    \end{enumerate*}
\item On booleans:
    \begin{enumerate*}
        \item negate the value.
    \end{enumerate*}
\item On string literals:
    \begin{enumerate*}
        \item replace a string with another existing string 
        \item replace a string with white space, or a system path separator, or a system file separator.
        \item add 1 random character to the string
        \item remove 1 random character from the string
        \item replace 1 random character in the string by another random character
        \item replace the string with a random string of the same size
        \item replace the string with the \texttt{null} value
    \end{enumerate*}
\item On methods :
    \begin{enumerate*}
        \item remove a method call
        \item duplicate a method call
    \end{enumerate*}.
\end{itemize}

DCI$_{SBAMPL}$ is the search-based amplification mode for DCI.

\subsection{Execution and Change Detection}
\label{sec:change-detection}

The final step performed by DCI consists in checking whether that the amplified test methods detect behavioral changes.
Because DCI amplifies test methods using the pre-commit version, all amplified test methods pass on this version, by construction. 
Consequently, for the last step, DCI runs the amplified test methods only on the post-commit version. 
Every test that fails is in fact detecting a behavioral change introduced by the commit, and is a success. DCI keeps the tests that successfully detect behavioral changes.

\subsection{Implementation}
\label{sub:implementation}

DCI is implemented in Java and is built on top of the OpenClover and Gumtree~\cite{falleri:hal-01054552} libraries.
It computes the global coverage of the test suite with OpenClover, which instruments and executes the test suite.
Then, it uses Gumtree to have an AST representation of the diff.
DCI matches the diff with the test that executes those lines. 
Through its Maven plugin, DCI can be seamlessly implemented into continuous integration.
DCI is publicly available on \gh.\footnote{\url{https://github.com/STAMP-project/dspot.git}}

\section{Evaluation}
\label{sec:evaluation}

To evaluate the DCI approach, we design an experimental protocol to answer the following research questions:

\newcommand{\rqcharacteristics}{\RQ{1}{What are the characteristics of commits with behavioral changes in the context of continuous integration?}}
\newcommand{\rqdetection}{\RQ{1}{To what extent are DCI$_{AAMPL}$ and DCI$_{SBAMPL}$ able to produce amplified test methods that detect the behavioral changes?}}
\newcommand{\rqselection}{\RQ{3}{What is the effectiveness of our test selection method?}}
\newcommand{\rqhuman}{\RQ{4}{How do human and generated tests that detect behavioral changes differ?}}
\newcommand{\rqiteration}{\RQ{2}{What  is the impact of the number of iteration performed by  DCI$_{SBAMPL}$?}}

\noindent
\begin{itemize}
% \item \rqcharacteristics
\item \rqdetection
\item \rqiteration
\item \rqselection
\item \rqhuman
\end{itemize}

\subsection{Benchmark}
\label{sec:benchmark}
To the best of our knowledge, there is no benchmark of commits in Java with behavioral changes in the literature. Consequently, we devise a project and commit selection procedure in order to construct a benchmark for our approach.

\paragraph{Project selection}
We need software projects that are
1) publicly-available,
2) written in Java,
3) and use continuous integration.
%
We pick the projects from the dataset in \cite{descartes} and \cite{dspot-emse}, which is composed of mature Java projects from \gh.

\paragraph{Commit selection}
%To select commits, we apply the following procedure.
We take commits in inverse chronological order, from newest to oldest.
%this result in commits buildable and analyzable with the current version of build and test tools;
We select the first ten commits that match the following criteria:
1) the commit modifies Java files (most behavioral changes are source code changes.\footnote{We are aware that behavioral changes can be introduced in other ways, such as modifying dependencies or configuration files \cite{Test:Coverage:Evolution}.});
% ground-truth
2) the commit provides or modifies a manually written test that detects a behavioral change. 
To verify this property, we execute the test on the pre-commit version. 
If it fails, it means that the test detects at least 1 behavioral change.
We will use this test as a \textit{ground-truth test} in \textbf{RQ4}.
3) The changes of the commit must be covered by the pre-commit test suite.
To do so, we compute the diff coverage. 
If the coverage is 0\%, we discard the commit. 
We do this because if the change is not covered, we cannot select any test methods to be amplified, which is what we want to evaluate.

Together, these criteria ensure that all selected commits:
1) introduce behavioral changes,
2) provide or modify a manually written test case that detects a behavioral change (which will be used as ground-truth for comparing generated tests), and
3) that there is at least 1 test in the pre-commit version of the program that executes the diff and can be used to seed the amplification process.
4) There is no structural change in the commit between both versions, \eg no change in method signature and deletion of classes (this is ensured since the pre-commit test suite compiles and runs against the post-commit version of the program and vice-versa.)

\paragraph{Final benchmark}
\begin{table}[h]
\centering
\def\arraystretch{1}
\setlength\tabcolsep{0.8pt}
\caption{Considered Period for Selecting Commits.}
\begin{tabular}{lc|rr|cccc}
\hline
project &
LOC &
\begin{tabular}{c}start\\date\end{tabular}&
\begin{tabular}{c}end\\date\end{tabular}&
\begin{tabular}{c}\#total\\commits\end{tabular}&
\begin{tabular}{c}\#discarded\\commits\end{tabular}&
\begin{tabular}{c}\#matching\\commits\end{tabular}&
\begin{tabular}{c}\#selected\\commits\end{tabular}\\
\hline
\scriptsize{commons-io}	&	59607	&	9/10/2015	&	9/29/2018	&	385	&	375	&	16(4.16\%)	&	10	\\
\rowcolor[HTML]{EFEFEF}
\scriptsize{commons-lang}	&	77410	&	11/22/2017	&	10/9/2018	&	227	&	217	&	13(5.73\%)	&	10	\\
\scriptsize{gson}	&	49766	&	6/14/2016	&	10/9/2018	&	159	&	149	&	13(8.18\%)	&	10	\\
\rowcolor[HTML]{EFEFEF}
\scriptsize{jsoup}	&	20088	&	12/21/2017	&	10/10/2018	&	50	&	40	&	11(22.00\%)	&	10	\\
\scriptsize{mustache.java}	&	10289	&	7/6/2016	&	04/18/2019	&	68	&	58	&	11(16.18\%)	&	10	\\
\rowcolor[HTML]{EFEFEF}
\scriptsize{xwiki-commons}	&	87289	&	10/31/2017	&	9/29/2018	&	687	&	677	&	23(3.35\%)	&	10	\\
\end{tabular}
\label{tab:benchmark}
\end{table}

\Autoref{tab:benchmark} shows the main statistics on the benchmark dataset. % construction.
The first column is the name of the considered project;
The second column is the date at which we did the analysis;
The third column is the date of the oldest commit for the project;
The fourth, fifth, sixth and seventh are respectively the total number of commit we analyze, the total number of commits we discard, the number of commits that match all our criteria but the third (there is no test in the pre-commit that execute the change and the number of commit we select).
We note that our benchmark is only composed of recent commits from notable open-source projects and is available on \gh at \url{https://github.com/STAMP-project/dspot-experiments}.

\subsection{Protocol}
\label{subsec:protocol}

%To answer \textbf{RQ1},
%we compute and study the main metrics of the commits in the benchmark. 
%we collect the following information for each commit:
%1) the date of the commit;
%2) the number of tests in the test suite at the date of the commit;
%3) the number of tests added or modified by the commit;
%4) the size of the diff, in terms of line insertions and deletions;
%5) the coverage of the diff by the test suite.

To answer \textbf{RQ1}, we run DCI$_{AAMPL}$ and DCI$_{SBAMPL}$ on the benchmark projects.
We then report the total number of behavioral changes successfully detected by DCI, \ie the number of commits for which DCI generates at least 1 test method that passes on the pre-commit version but fails on the post-commit version.
We also discuss 1 case study of a successful behavioral change detection.

To answer \textbf{RQ2}, we run DCI$_{SBAMPL}$ for 1, 2 and 3 iterations on the benchmark projects.
We report the number of behavioral changes successfully detected for each number of iterations in the main loop.
We report also the number amplified test methods that detect the behavioral changes for each commit for 10 different seeds to study the impact of the randomness on the output of DSpot
We perform a Kruskal-Wallis test statistic on these numbers.

The null hypothesis is the following: The population median of all of the groups are equal

The alternative hypothesis is: at least one population median of one group is different from the population median of at least one other group

We take a confidence level of 95\%, or $\alpha = 0.05$.

For \textbf{RQ3}, the test selection method is considered effective if the tests selected to be amplified semantically relate to the code changed by the commit. 
To assess this, we perform a manual analysis.% as follows.
We randomly select 1 commit per project in the benchmark, and we manually analyze whether the automatically selected tests for this commit are semantically related to the behavioral changes in the commit. 
%This research question aims at validating the test selection according to a diff of DCI.

To answer \textbf{RQ4}, we use the ground-truth tests written or modified by developers in the selected commits. % written by the humans.
We manually compare the amplified test methods that detect behavioral changes to the human tests, for 1 commit per project.
%For this, we take only consider commits for which DCI has been able to produce at least one test method that detects the behavioral change. 

\subsection{Results}
\label{subsec:result}
\begin{table}
\centering
\small
\def\arraystretch{0.3}%  1 is the default, change whatever you need
\setlength\tabcolsep{.35pt} % default value: 6pt
\caption{Performance evaluation of DCI on 60 commits from 6 large open-source projects.}
%\rotvertical{
\label{tab:overall_result}
\begin{tabular}{l|c|rcccc|c|cc|cc|cc|cc}
\input{chapitres/behavioral-change-detection-for-commit/table_content}
\end{tabular}
%}
\end{table}

The overall results are reported in \Autoref{tab:overall_result}.
The first column is the shortened commit id;
the second column is the commit date;
the third column column is the total number of test methods executed when building that version of the project;
the fourth and fifth columns are respectively the number of tests modified or added by the commit, and the size of the diff in terms of line additions (in green) and deletions (in red);
the sixth and seventh columns are respectively the diff coverage and the number of tests DCI selected;
the eighth column provides the amplification results for DCI$_{AAMPL}$, and it is either a \cmark with the number of amplified tests that detect a behavioral change or a \textit{-} if DCI did not succeed in generating a test that detects a change;
the ninth column displays the time spent on the amplification phase;
The tenth and the eleventh are respectively a \cmark with the number of amplified tests for DCI$_{SBAMPL}$  (or - if a change is not detected) for 3 iterations.

\subsubsection{Characteristics of commits with behavioral changes in the context of continuous integration}
\label{subsubsec:answerq1}

%We consider the first columns (under \textbf{RQ1} meta column) of \Autoref{tab:overall_result}, which describe the characteristics of our novel benchmark.
%The columns under the \textbf{RQ1} meta column
In  this section, we describe the the characteristics of commits introducing behavioral changes in the context of continuous integration
The first five columns in \Autoref{tab:overall_result} describe the characteristics of our benchmark.
% date
The commit dates show that the benchmark is only composed of recent commits.
The most recent is \textsc{gson\#b1fb9ca}, authored 9/22/18, and the oldest is \textsc{commons-io\#5d072ef}, authored 9/10/15.
% now the number of tests
The number of test methods at the time of the commit shows two aspects of our benchmark:
1) we only have strongly tested projects;
2) we see that the number of tests evolve over time
%, witnessing the presence of 
due to test evolution.
%The fourth column shows that 
Every commit in the benchmark comes with test modifications (new tests or updated tests), and commit sizes are quite diverse.
%For instance, the first commit c6b8a38 has two modified tests.
%The fifth column shows the size of the commit diff in terms of added and removed lines.
%Overall, our selection criteria include a diversity of commit sizes. 
The three smallest commits are \textsc{commons-io\#703228a}, \textsc{gson\#44cad04} and \textsc{jsoup\#e5210d1} with 6 modifications, and the largest is \textsc{Gson\#45511fd} with 334 modifications.
%
Finally, 
%we consider the sixth column which gives the code coverage of the diff itself. 
on average, commits have 66.11\% coverage. 
The distribution of diff coverage is reported graphically by \Autoref{fig:histdiffcoverage}: 
in commons-io all selected commits have more than 75\% coverage.
In XWiki-Commons, only 50\% of commits have more than 75\% coverage. Overall, 31 / 60 commits have at least 75\% of the changed lines covered.
This validates the correct implementation of our selection criteria that ensures the presence of a test specifying the behavioral change.

\begin{figure}
\centering
\includegraphics[width=.95\linewidth]{img/diff_cov_hist.png}
\caption{Distribution of diff coverage per project of our benchmark.}
\label{fig:histdiffcoverage}
\end{figure}

% recap of RQ1.
Thanks to our selection criteria, we have a curated benchmark of 50 commits with a behavioral change, coming from notable open-source projects, and covering a diversity of commit sizes. The benchmark is publicly available and documented for future research on this topic.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% RESULTS RQ2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\rqdetection}
\label{subsubsec:answerq2}

We now focus on the last 4 columns of \Autoref{tab:overall_result}.
For instance, for \textsc{commons-io\#f00d97a} (4$^{th}$ row), DCI$_{AAMPL}$ generated 39 amplified tests that detect the behavioral change. 
For \textsc{commons-io\#81210eb} (8$^{th}$ row), only the \sbampl version of DCI detects the change.
%
Overall, using only \aampl, DCI generates amplified tests that detect 9 out of 60 behavioral changes.
Meanwhile, using \sbampl only, DCI generates amplified tests that detect 28 out of 60 behavioral changes.

Regarding the number of generated tests.
\DCII generates a large number of test cases, compared to \DCIA only (15 versus 6708, see column ``total'' at the bottom of the table). 
Both \DCIA and \DCIA can generate amplified tests, however since \DCIA does not produce a large amount of test methods the developers do not have to triage a large set of test cases. 
Also, since \DCIA only adds assertions, the amplified tests are easier to understand than the ones generated by \DCII.

% time
\DCII takes more time than \DCIA (for successful cases 38.7 seconds versus 3.3 hours on average).
The difference comes from the time consumed during the exploration of the input space in the case of \DCII, while \DCIA focuses on the amplification of assertions only, which represents a much smaller space of solutions. 

Overall, DCI successfully generates amplified tests that detect a behavioral change in 46\% of the commits in our benchmark(28 out of 60).
Recall that the 60 commits that we analyze are real changes that fix bugs in complex code bases.
They represent modifications, sometimes deep in the code, that represent challenges with respect to testability~\cite{voas1995software}.
Consequently, the fact DCI can generate test cases that detect behavioral changes, is considered an achievement.
The commits for which DCI fails to detect the change can be considered as a target for future research on this topic.

Now, we manually analyze a successful case where DCI detects the behavioral change.
We select commit \textsc{3fadfdd}\footnote{\url{https://github.com/apache/commons-lang/commit/3fadfdd}} from commons-lang, which is succinct enough to be discussed in the paper.
The diff is shown in \Autoref{fig:diff_commons_lang_success}.

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.95\linewidth, trim=4.5cm 14.85cm 6.5cm 4.3cm, clip]{img/diff/success-diff-commons-lang.pdf}}
\caption{Diff of commit \textsc{3fadfdd} from commons-lang.}
\label{fig:diff_commons_lang_success}
\end{figure}

The developer added a method call to a method that escapes specials characters in a string. 
The changes come with a new test method that specifies the new behavior. 

DCI starts the amplification from the \texttt{testNestingPerson} test method defined in \texttt{JsonToStringStyleTest}. The test is selected for amplification because it triggers the execution of the changed line.

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.95\linewidth, trim=3.7cm 46.5cm 9.65cm 38.3cm, clip]{img/amplified/success-ampl-commons-lang.pdf}}
\caption{Test generated by DCI that detects the behavioral change of \textsc{3fadfdd} from commons-lang.}
\label{fig:amplified_commons_lang_success}
\end{figure}

We show in \Autoref{fig:amplified_commons_lang_success} 
the resulting amplified test method.
From this test method, DCI generates an amplified test method shown in \Autoref{fig:amplified_commons_lang_success}. 
In this generated test, \sbampl applies 2 input transformations: 1 duplication of method call and 1 character replacement in an existing String literal.
The latter transformation is the key transformation: DCI replaced an 's' inside "person" by '/' resulting in "per/on" where "/" is a special character that must be escaped (Line 2). 
Then, DCI generated 11 assertions, based on the modified inputs. 
The amplified test the behavioral change:
in the pre-commit version, the expected value is: \texttt{"\{ ... per/on":\{"name":"Jane Doe" ...\}"} while in the post-commit version it is \texttt{"\{ ... per\textbackslash/on":\{"name":"Jane Doe" ...\}"} (Line 3).
 
%recap RQ2
\begin{mdframed}
Answer to \textbf{RQ1}: Overall, DCI is capable of detecting the behavioral changes in a total of 28/60 commits. Individually, \DCII finds changes in 28/60, while \DCIA in 9/60 commits.
Since \DCII also uses \aampl to generate assertions, all \DCIA's commits are contained in \DCII's. However, the search-based algorithm, through exploration, finds many more behavioral changes, making it more effective albeit at the cost of execution time.
\end{mdframed}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% RESULTS RQ2 : ITERATION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{\rqiteration}
\label{subsubsec:answerqiteration}

The results are reported in \Autoref{tab:overall_result_iteration}
This table can be read as follow:
the first column is the name of the project;
the second column is the commit identifier;
then, the third, fourth, fifth, sixth, seventh and eighth provide the amplification results and execution time for each number of iteration 1, 2, and 3. A \cmark indicates with the number of amplified tests that detect a behavioral change and a \textit{-} denotes that DCI did not succeed in generating a test that detects a change.

% number of detection
Overall, \DCII generates amplified tests that detect 21, 23, and 24 out of 60 behavioral changes for respectively $iteration=1$, $iteration=2$ and $iteration=3$.
The more iteration \DCII does, the more it explores, the more it generates amplified tests that detect the behavioral changes but the more it takes time also.
% difference in number of behavioral changes we detect
When \DCII is used with $iteration=3$, it generates amplified test methods that detect 3 more behavioral changes than when it is used with $iteration=1$ and 1 then when it is used with $iteration=2$. It represents an increase of 14\% and 4\% for respectively $iteration=1$ and $iteration=2$.

% number of test generated
In average, \DCII generates 18, 53, and 116 amplified tests for respectively $iteration=1$, $iteration=2$ and $iteration=3$. 
This number increases by 544\% from $iteration=1$ to $iteration=3$.
This increase is explained by the fact that \DCII explores more with more iteration and thus is able to generate more amplified test methods that detect the behavioral changes.

% time
In average \DCII takes 23, 64, and 105 minutes to perform the amplification for respectively $iteration=1$, $iteration=2$ and $iteration=3$.
This number increases by 356\% from $iteration=1$ to $iteration=3$.

\begin{table*}
\small
\def\arraystretch{1}%  1 is the default, change whatever you need
\setlength\tabcolsep{6pt} % default value: 6pt
\caption{Evaluation of the impact of the number of iteration done by \DCII on 60 commits from 6 open-source projects.}
%\rotvertical{
\label{tab:overall_result_iteration}
\begin{tabular}{l|c|cc|cc|cc}
\input{chapitres/behavioral-change-detection-for-commit/table_content_iterations}
\end{tabular}
%}
\end{table*}

\paragraph{Impact of the randomness}

The number of amplified test methods obtained by the different seeds are reported in \Autoref{tab:overall_result_seeds}.
The result of the Kruskal-Wallis test is:  p-value=0.96.
$p-value>\alpha$ which means that we keep the null hypothesis: 
The population median of all of the groups are equal.
This means that, in general, the choice of the seeds has not a significant impact of the overall result of DCI.

\begin{table*}
\small
\def\arraystretch{.5}%  1 is the default, change whatever you need
\setlength\tabcolsep{3pt} % default value: 6pt
\caption{Number of amplified test methods obtained by DCI for 10 different seeds. The first column is the id of the commit. The second column is the result obtained with the default seed, used during the evaluation for \rqdetection. The ten following columns are the result obtained for the 10 different seeds.}
\label{tab:overall_result_seeds}
\begin{tabular}{l|c|llllllllll}
\input{chapitres/behavioral-change-detection-for-commit/table_content_seeds.tex}
\end{tabular}
\end{table*}

%recap RQ2
\begin{mdframed}
Answer to \textbf{RQ2}: \DCII detects  21, 23, and 24 behavioral changes out of 60 for respectively $iteration=1$, $iteration=2$ and $iteration=3$.
The number of iteration done by \DCII impacts the number of behavioral changes detected, the number of amplified test methods obtained and the execution time.
\end{mdframed}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% RESULTS RQ3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{\rqselection}
\label{subsubsec:answerq3}

To answer \textbf{RQ3}, there is no quantitative approach to take, because there is no ground truth data or metrics to optimize. 
Per our protocol described in \Autoref{subsec:protocol}, we answer this question based on manual analysis:
we randomly selected 1 commit per project, and we analyzed the relevance of the selected tests for amplification.

In order to give an intuition of what we consider as a relevant test selection for amplification, let us look at an example. 
If \texttt{TestX} is selected for amplification, following a change to method \texttt{X}, we consider this as relevant. The key is that DCI will generate an amplified test \texttt{TestX'} that is a variant of \texttt{TestX}, and, consequently, the developer will directly get the intention of the new test \texttt{TestX'} and what behavioral change it detects.

\textsc{Commons-io\#c6b8a38}\footnote{\url{https://github.com/apache/commons-io/commit/c6b8a38}}: our test selection returns 3 test methods: \texttt{testContentEquals}, \texttt{testCopyURLToFileWithTimeout} and \texttt{testCopyURLToFile} from the same test class: \texttt{FileUtilsTestCase}.
The considered commit modifies the method \texttt{copyToFile} from \texttt{FileUtils}. 
Two test methods out 3 (\texttt{testCopyURLToFileWithTimeout} and \texttt{testCopyURLToFile}) there is a link between the changed file and the intention of tests to be amplified. 
The selection is thus considered relevant.

\textsc{Commons-lang\#f56931c}\footnote{\url{https://github.com/apache/commons-lang/commit/f56931c}}: our test selection returns 39 test methods from 5 test classes: \texttt{FastDateFormat\_ParserTest}, \texttt{FastDateParserTest}, \texttt{DateUtilsTest}, \texttt{FastDateParser\_TimeZoneStrategyTest} and \texttt{FastDateParser\_MoreOrLessTest}.
This commit modifies the behavior of two methods: \texttt{simpleQuote} and \texttt{setCalendar} of class \texttt{FastDateParser}.
Our manual analysis reveals two intentions:
1) test behaviors related to parsing, 
1) test behaviors related to dates.
While this is meaningful, a set of 39 methods is clearly not a focused selection, not as focused as for the previous example.
It is considered as an half-success.

 \textsc{Gson\#9e6f2ba}\footnote{\url{https://github.com/google/gson/commit/9e6f2ba}}: our test selection returns 9 test methods from 5 different test classes.
 Three out of those five classes \texttt{JsonElementReaderTest}, \texttt{JsonReaderPathTest} and \texttt{JsonParserTest} relate to the class modified in the commit(\texttt{JsonTreeReader}).
The selection is thus considered relevant but unfocused.

 \textsc{Jsoup\#e9feec9}\footnote{\url{https://github.com/jhy/jsoup/commit/e9feec9}}, our test selection returns the 4 test methods defined in the \texttt{XmlTreeBuilderTest} class : \texttt{caseSensitiveDeclaration}, \texttt{handlesXmlDeclarationAsDeclaration}, \texttt{testDetectCharsetEncodingDeclaration} and \texttt{testParseDeclarationAttributes}.
 The commit modifies the behavior of the class \texttt{XmlTreeBuilder}.
Here, the test selection is relevant.
Actually, the ground-truth manual test added in the commit is also in the \texttt{XmlTreeBuilderTest} class.
If DCI proposes a new test there to capture the behavioral change, the developer will understand its relevance and its relation to the change.

\textsc{Mustache.java\#88718bc}\footnote{\url{https://github.com/spullara/mustache.java/commit/88718bc}}, our test selection returns the \texttt{testInvalidDelimiters} test method defined in the \texttt{com.github.mustachejava.InterpreterTest} test class.
The commit improves an error message when an invalid delimiter is used.
Here, the test selection is relevant since it selected \texttt{testInvalidDelimiters} which is the dedicated test to the usage of the test invalid delimiters.
This ground-truth test method is also in the test class \texttt{com.github.mustachejava.InterpreterTest}.

\textsc{Xwiki-commons\#848c984}\footnote{\url{https://github.com/xwiki/xwiki-commons/commit/848c984}} our test selection returns a single test method \texttt{createReference} from test class \texttt{XWikiDocumentTest}.
The main modification of this commit is on class \texttt{XWikiDocument}.
Since \texttt{XWikiDocumentTest} is the test class dedicated to \texttt{XWikiDocument}, this is considered as a success.

\begin{mdframed}
Answer to \textbf{RQ3}: 
In 4 out of 6 of the manually analyzed cases, the tests selected to be amplified relate, semantically, to the modified application code. 
In the 2 remaining cases, we selected over and above the tests to be amplified.
That is, we select tests whose intention is semantically pertinent to the change, but we also include tests that are not.
%the test intention is captured but not focused.
%, the test class name, method name and initialization code 
%This indicates that in the selected tests by DCI give the right context for the developer to understand what behavioral change is specified.
However, even in this case, DCI's test selection provides developers with important and targeted context to better understand the behavioral change at hand.
\end{mdframed}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% RESULTS RQ4
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\rqhuman}
\label{subsubsec:answerq4}

When DCI generates an amplified test method that detects the behavioral change, we can compare it to the ground truth version (the test added in the commit) to see whether it captures the same behavioral change.
For each project, we select 1 successful application of DCI, and we compare the DCI test against the human test.
If they capture the same behavioral change, it means they have the same intention and we consider the amplification a success.

%%% COMMONS IO

\textsc{commons-io\#81210eb}\footnote{\url{https://github.com/apache/commons-io/commit/81210eb}}: This commit modifies the behavior of the \texttt{read()} method in \texttt{BoundedReader}.
\Autoref{fig:ampl_commons-io} shows the test generated by \DCII.
This test is amplified from the existing \texttt{readMulti} test, which indicates that the intention is to test the read functionality.
The first line of the test is the construction of a \texttt{BoundedReader} object which is also the class modified by the commit.
\DCII modified the second parameter of the constructor call (transformed $3$ into a $0$) and generated two assertions (only 1 is shown).
The first assertion, associated to the new test input, captures the behavioral difference.
Overall, this can be considered as a successful amplication.

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.97\linewidth, trim=3.6cm 9.3cm 14.7cm 36.8cm, clip]{img/amplified/ampl-commons-io.pdf}}
\caption{Test generated by \DCII that detects the behavioral change introduced by commit \textsc{81210eb} in commons-io.}
\label{fig:ampl_commons-io}
\end{figure}

Now, let us look at the human test contained in the commit, shown in \Autoref{fig:diff_commons-io}.
It captures the behavioral change with the timeout (the test timeouts on the pre-commit version and goes fast enough on the post-commit version). 
Furthermore, it only indirectly calls the changed method through a call to \texttt{readLine}.

In this case, the DCI test can be considered better than the developer test because
1) it relies on assertions and not on timeouts, and
2) it directly calls the changed method (\texttt{read}) instead of indirectly. 

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.97\linewidth, trim=4cm 6.7cm 12.6cm 25.8cm, clip]{img/diff/diff-commons-io.pdf}}
\caption{Developer test for commit \textsc{81210eb} of commons-io.}
\label{fig:diff_commons-io}
\end{figure}


%%% COMMONS LANG
\textsc{commons-lang\#e7d16c2}\footnote{\url{https://github.com/apache/commons-lang/commit/e7d16c2}}: this commit escapes special characters before adding them to a \texttt{StringBuffer}.
\Autoref{fig:ampl_commons-lang} shows the amplified test method obtained by \DCII.
The assertion at the bottom of the excerpt is the one that detects the behavioral change.
This assertion compares the content of the \texttt{StringBuilder} against an expected string.
In the pre-commit version, no special character is escaped, \eg '\textbackslash n'.
In the post-commit version, the DCI test fails since the code now escapes the special character \textbackslash.

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.97\linewidth, trim=2.6cm 8.35cm 10.8cm 20.85cm, clip ]{img/amplified/ampl-commons-lang.pdf}}
\caption{Test generated by \DCII that detects the behavioral change of \textsc{e7d16c2} in commons-lang.}
\label{fig:ampl_commons-lang}
\end{figure}

Let's have a look to the human test method shown in \Autoref{fig:diff_commons-lang}.
Here, the developer specified the new escaping mechanism with 5 different inputs.
%
The main difference between the human test and the amplified test is that the human test is more readable and uses 5 different inputs.
However, the amplified test generated by DCI is valid since it detects the behavioral change correctly.

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.97\linewidth, trim=4.5cm 9.2cm 5.8cm 39.2cm, clip]{img/diff/diff-commons-lang.pdf}}
\caption{Developer test for \textsc{e7d16c2} of commons-lang.}
\label{fig:diff_commons-lang}
\end{figure}


%% GSON
\textsc{gson\#44cad04}\footnote{\url{https://github.com/google/gson/commit/44cad04}}: This commit allows Gson to deserialize a number represented as a string.
\Autoref{fig:ampl_gson} shows the relevant part of the test generated by DCI$_{SBAMPL}$, based on \texttt{testNumberDeserialization} of \texttt{PrimitiveTest} as a seed.
First, we see that the test selected as a seed is indeed related to the change in the deserialization feature.
The DCI test detects the behavioral change at lines 3 and 4.
On the pre-commit version, line 3 throws a \texttt{JsonSyntaxException}.
On the post-commit version, line 4 throws a \texttt{NumberFormatException}.
In other words, the behavioral change is detected by a different exception (different type and not thrown at the same line).
\footnote{Interestingly, the number is parsed lazily, only when needed. Consequently, the exception is thrown when invoking the \texttt{longValue()} method and not when invoking \texttt{parse()}}.

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.95\linewidth, trim=3.7cm 19.5cm 13.4cm 64.2cm, clip]{img/amplified/ampl-gson.pdf}}
\caption{Test generated by DCI that detects the behavioral change of commit \textsc{44cad04} in Gson.}
\label{fig:ampl_gson}
\end{figure}

We compare it against the developer-written ground-truth method, shown in \Autoref{fig:diff_gson}. 
This short test verifies that the program handles a number-as-string correctly.
For this example, the DCI test does indeed detect the behavioral change, but in an indirect way.
On the contrary, the developer test is shorter and directly targets the changed behavior, which is better.

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.95\linewidth, trim=4cm 9.3cm 12.5cm 20.6cm ,clip]{img/diff/diff-gson.pdf}}
\caption{Provided test by the developer for \textsc{44cad04} of Gson.}
\label{fig:diff_gson}
\end{figure}



%% JSOUP
\textsc{jsoup\#3676b13}\footnote{\url{https://github.com/jhy/jsoup/commit/3676b13}}: This change is a pull request (\ie a set of commits) and introduces 5 new behavioral changes. There are two improvements: skip the first new lines in pre tags and support deflate encoding, and three bug fixes: throw exception when parsing some urls, add spacing when output text, and no collapsing of attribute with empty values.
\Autoref{fig:ampl_jsoup} shows an amplified test obtained using \DCII.
This amplified test has 15 assertions and a duplication of method call.
Thanks to this duplication and assertion generated on the \texttt{toString()} method, this test is able to capture the behavioral change introduced by the commit.

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.95\linewidth, trim=3.7cm 82cm 7cm 30.8cm ,clip]{img/amplified/ampl-jsoup.pdf}}
\caption{Test generated by \DCII that detects the behavioral change of \textsc{3676b13} of Jsoup.}
\label{fig:ampl_jsoup}
\end{figure}

As before, we compare it to the developer's test. 
The developer uses the \texttt{Element} and \texttt{outerHtml()} methods rather than \texttt{Attribute} and \texttt{toString()}.
However, the method \texttt{outerHtml()} in \texttt{Element} will call the \texttt{toString()} method of \texttt{Attribute}.
For this behavioral change, it concerns the \texttt{Attribute} and not the \texttt{Element}.
So, the amplified test is arguably better, since it is closer to the change than the developer's test.
But, \DCII generates amplified tests that detect 2 of 5 behavioral changes: adding spacing when output text and no collapsing of attribute with empty values only, so regarding the quantity of changes, the human tests are more complete.

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.95\linewidth, trim=4.3cm 19.6cm 4.8cm 131.5cm , clip]{img/diff/diff-jsoup.pdf}}
\caption{Provided test by the developer for \textsc{3676b13} of Jsoup.}
\label{fig:diff_jsoup}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% MUSTACHE JAVA
\textsc{Mustache.java\#774ae7a}\footnote{\url{https://github.com/spullara/mustache.java/commit/774ae7a}}: This commit fixes an issue with the usage of a dot in a  relative path on Window in the method \texttt{getReader} of class \texttt{ClasspathResolver}.
The test method \texttt{getReaderNullRootDoesNotFindFileWithAbsolutePath} has been used as seed by DCI. It modifies the existing string literal with another string used somewhere else in the test class and generates 3 new assertions.
The behavioral change is detected thanks to the modified strings: it produces the right test case containing a space.

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.95\linewidth]{img/amplified/ampl-mustache.png}}
\caption{Test generated by \DCII that detects the behavioral change of \textsc{774ae7a} of Mustache.java.}
\label{fig:ampl_mustache}
\end{figure}

The developer proposed two tests that verify that the object reader is not null when getting it with dots in the path.
There are shown in \Autoref{fig:diff_mustache}.
These tests invoke the method \texttt{getReader} which is the modified method in the commit.
%
The difference is that the \DCII's amplified test method provides a non longer valid input for the method \texttt{getReader}.
However, providing such inputs produce errors afterward which signal the behavioral change.
In this case, the amplified test is complementary to the human test since it verifies that the wrong inputs are no longer supported and that the system immediately throws an error.

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.95\linewidth]{img/diff/diff-mustache.png}}
\caption{Developer test for \textsc{774ae7a} of Mustache.java.}
\label{fig:diff_mustache}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% XWIKI COMMONS
\textsc{xwiki-commons\#d3101ae}\footnote{\url{https://github.com/xwiki/xwiki-commons/commit/d3101ae}}: This commit fixes a bug in the \texttt{merge} method of class \texttt{DefaultDiffManager}.
\Autoref{fig:ampl_xwiki} shows the amplified test method obtained by \DCIA.
DCI used \texttt{testMergeCharList} as a seed for the amplification process, and generates 549 new assertions.
Among them, 1 assertion captures the behavioral change between the two versions of the program: 
``assertEquals(0, result.getLog().getLogs(LogLevel.ERROR).size());''.
The behavioral change that is detected is the presence of a new logging statement in the diff. After verification, there is indeed such a behavioral change in the diff, with the addition of a call to ``logConflict'' in the newly handled case.

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.95\linewidth, trim=3.5cm 11.5cm 9.2cm 18.2cm, clip ]{img/amplified/ampl-xwiki.pdf}}
\caption{Test generated by \DCIA that detects the behavioral change of \textsc{d3101ae} of XWiki.}
\label{fig:ampl_xwiki}
\end{figure}

The developer's test is shown in \Autoref{fig:diff_xwiki}.
This test method directly calls method \texttt{merge}, which is the method that has been changed. 
What is striking in this test is the level of clarity: the variable names, the explanatory comments and even the vertical space formatting are impossible to achieve with \DCIA and makes the human test clearly of better quality but also longer to write. % above when she has the time to write a good test. 
%
Yet, \DCIA's amplified tests capture a behavioral change that was not specified in the human test.
In this case, amplified tests can be complementary.

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.95\linewidth, trim=4.2cm 6.7cm 5cm 75cm, clip]{img/diff/diff-xwiki.pdf}}
\caption{Developer test for \textsc{d3101ae} of XWiki.}
\label{fig:diff_xwiki}
\end{figure}


\begin{mdframed}
Answer to \textbf{RQ4}: 
In 2 of 5 cases, the DCI test is complementary to the human test.
In 1 case, the DCI test can be considered better than the human test.
In 2 cases, the human test is better than the DCI test.
Even though human tests can be better, DCI can be complementary and catch missed cases, or can provide added-value when developers do not have the time to add a test.
\end{mdframed}

%\pagebreak


% -----------------------
%  Main Observed Limitation
% -----------------------
\section{Discussion about the scope of DCI}
\label{sec:limitation}

In this section, we overview the current limitations of DCI and the key factors that prevent DCI from amplifying test methods that detect behavioral changes.

\textbf{Time consumption}
From our experiments, we see that the time consumption to complete the amplification is the main limitation of DCI.
\textsc{jsoup\#2c4e79b}, almost 5 hours have been spent with no result.
For the sake of our experimentation, we choose to use a pre-defined number of iteration to bound the exploration.
In practice, we recommend to set a time budget (\eg at most one hour per pull-request).

\textbf{Importance of test seeds}
By construction, DCI's effectiveness is correlated to the test methods used as seed.
For example, see the row of \texttt{commons-lang\#c8e61af} in \Autoref{tab:overall_result_iteration}
where one can observe that whatever the number of iteration, DCI takes the same time to complete the amplification.
The reason is that the seed tests are only composed of assertions statements.
Such tests are bad seeds for DCI and they prevent any good input amplification.

\textbf{False positive}
The risk of false positives is a potential  limitation of your approach.
A false positive would be an amplified test method that passes or fails on both versions, which means that the amplified test method does not detect the behavioral difference between both versions.
We manually  analyzed 6 commits and none of them are false positives.
While this is not a proof that DCI would never produce such confusing test methods, we are confident in the soundness of our implementation.

% -----------------------
%  THREATS
% -----------------------
\section{Threats to validity}
\label{sec:threats}

% bug in the implementation
An internal threat is the potential bugs in the implementation of DCI. However, we heavily tested our prototype with JUnit test cases to mitigate this threat.

% benchmark
In our benchmark, there are 60 commits. Our result may be not be generalizable to all programs. But we carefully selected real and diverse applications from GitHub, all having a strong test suite. We believe that the benchmark reflects real programs, and we have good confidence in the results.

% flaky amplified test
Last but not least threat is the potential flakiness of generated test methods.
However we take care that our approach does not produce flaky test methods, and we make sure to observe a stable and different state of the program between different executions. 
To do this, we execute 3 times each amplified test in order to check weather or not there are stable.
If the outcome of a at least one execution is different than the others, we discard the amplified test.

% weak statiscal test
For the evaluation of the randomness, we use a Kruskal-Willis, which is known to be weaker than ANOVA test.
To perform an ANOVA test, the data must fulfull the following criteria:
1) The samples are independent;
2) Each sample is from a normally distributed population;
3) The population standard deviations of the groups are all equal. This property is known as homoscedasticity.
The two first are fulfilled while the third is not:
\begin{table*}
\def\arraystretch{1}%  1 is the default, change whatever you need
\setlength\tabcolsep{3pt} % default value: 6pt
\caption{Standard deviations of the number of amplified tests obtained for each seed.}
\begin{tabular}{c|cccccccccc}
seed     & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
\hline
std     & 63.38&63.55&62.56&61.27&61.33&61.66&63.76&60.91&61.25&63.35
\end{tabular}
\end{table*}
Since the standard deviations are not all equal, the associated p-value would not be valid.
This is why we choose to use a Kruskal-Willis test.

In addition to this, we perform it for only 11 seeds, which a small samples.

% -----------------------
%  RELATED - WORK
% -----------------------
\section{Related Work}
\label{sec:related_work}

\subsection{Commit-based test generation}

Person \etal~\cite{dse} present differential symbolic execution (DSE). DSE combines symbolic execution and a new approximation technique to highlight behavioral changes.
They use symbolic execution summary to find equivalences and difference and generate a set of inputs that trigger different behavior.
This is done in three steps: 1) they execute both versions of the modified method; 2) they find equivalences and differences, thanks to the analysis of symbolic execution summary;  3) they generate a set of inputs that trigger the different behaviors in both versions.
The main difference with our work is that they have the strong assumption to have a program whose semantics is fully handled by the symbolic execution engine. 
In the context of Java, to our knowledge, no symbolic execution engine works on arbitrary Java program.
Symbolic execution engines do not scale to the size and complexity of the programs we targeted.
On the contrary, our approach, being more lightweight, is meant to work on all Java programs.

Marinescu and Cadar~\cite{marinescu2013katch} present Katch, a system that aims at covering the code included in a patch.
This approach first determines the differences of a program and its previous version.
It targets modified and not executed by the existing test suite lines.
Then, it selects the closest input to each target from existing tests using a static minimum distance over the control flow graph.
The proposal is evaluated on Unix tools. 
They examine patches from a period of 3 years. In average, they automatically increase coverage from 35\% to 52\% with respect to the manually written test suite.
Contrary to our work, they only aim at increasing the coverage, not at detecting behavioral changes.

A posterior work of the same group~\cite{palikareva2016shadow,Kuchta:2018:SSE:3276753.3208952} focuses on finding test inputs that execute different behaviors in two program versions. 
They devise a technique, named ShaddowKlee, built on top of Klee \cite{klee}. 
They require the code to be annotated at changed places. 
Then they select from the test suite those test cases that cover the changed code. The unified program is used in a two stage dynamic symbolic execution guided by the selected test cases.
They first look for branch points where the conditions are evaluated in both program versions.
Then, a constraint solver generates new test inputs for divergent scenarios. The program versions are then normally executed with the generated inputs and the result is validated to check the presence of a bug or of an intended difference.
The evaluation of the proposed method is based on the CoREBench~\cite{bohme2014corebench} data set that contains documented regression bugs of the GNU Coreutils program suite. 


Noller \etal~\cite{jpfshadow} aim at detecting regression bugs. 
They apply shadow symbolic execution, originally from Palikevera~\cite{dse,palikareva2016shadow} that has been discussed in the previous paragraph, on Java programs. 
Their approach has been implemented as an extension of Java Path Finder Symbolic (jpf-symbc)\cite{jpfsymb}, named jpf-shadow. 
Shadow symbolic execution generate test inputs that trigger the new program behavior. 
They use a merged version of both version of the same program, i.e. the previous version, so called old, and the changed version, called new.
This is done by instrumenting the code with method calls ``change()''.
The method change() takes two inputs: the old statement and the new one.
Then, a first step collects divergence points, i.e. conditional where the old version and the new version do not take the same branch.
On small examples, they show that jpf-shadow generates less unit test cases yet cover the same number of path. 
Jpf-shadow only aims at covering the changes and not at detecting the behavioral change with an assertion.

Menarini \etal~\cite{semantics:code:review} proposes a tool, GETTY, based on invariants mined by Daikon.
GETTY provides to code reviewers a summary of the behavioral changes, based on the difference of invariants for various combinations of programs and test suites.
They evaluate GETTY on 6 open source project, and showed that their behavioral change summaries can detect bugs earlier than with normal code review.
While they provide a summary, DCI provides a concrete test method with assertions that detect the behavioral changes. 

Lahiri \etal~\cite{differential-assertion-checking} propose differential assertion checking (DAC): checking two versions of a program with respect to a set of assertions. DAC is based on filtering false alarms of verification analysis. They evaluate DAC on a set of small example.
The main difference is that DAC requires to manually write specifications, while DCI is completely automated with normal code as input.

Yang \etal~\cite{Yang:2014:PDI:2568225.2568319} introduce IProperty, a  way to annotate  correctness properties of programs. They evaluate their approach on the triangle problem.
The key novelty of our work is to perform an evaluation on real commits from large scale open source software.

Campos \etal~\cite{Campos:2014:CTG:2642937.2643002} extended EvoSuite to adapt test generation techniques to continuous integration.
Their contribution is the design of a time budget allocation strategy: it allocates more time budget to specific classes that are involved in the changes.
They evaluated their approach on 10 projects from the SF100 corpus, on 8 of the most popular open-source projects from GitHub, and on 5 industrial projects.
They limit their evaluation to the 100 last consecutive commits.
They observe an increase of +58\% branch coverage, +69\% thrown undeclared exceptions, while reducing the time consumption by up to 83\% compared to the baseline.
The major difference compared to our approach, they do not aim at specifically obtaining test methods that detect the behavioral changes but rather obtain better branch coverage and detect undeclared exceptions. They also do not generate any assertions.
However, from the point of view practitioners, integrating a time budget strategy into DCI would increase its usability, practicability and potential adoption.


\subsection{Behavioral change detection}

Evans \etal \cite{evans2007differential} devise the differential testing.
This approach aims at alleviating the test repair problem and detects more changes than regression testing alone.
They use an automated characterization test generator (ACTG) to generate test suite for both version of the program.
They then categorizes the tests of these 2 test suites into 3 groups:
1) $T_{preserved}$ which are the tests that pass on the both versions;
2) $T_{regressed}$ which are the tests that pass on the previous version but not on the new one;
3) $T_{progressed}$ which are the tests that pass on the new version but not on the previous one;
Then, they define also $T_{different}$ which is the union of both $T_{regressed}$ and $T_{progressed}$.
The approach is to execute $T_{different}$ on both versions and observe progressed and regressed behaviors.
They evaluate their approach on a small use case from the SIR dataset on 38 diffrent changes, for version of the program.
They showed that their approach detects 21\%, 34\%, and 21\% more behavior changes than regression testing alone for respectively version 1, version 2 and version 3.
In DCI, the amplified test methods obtained would lie into the $T_{regressed}$ group.
However, we could also amplified test methods using the new version of the program and obtain a $T_{progressed}$.
We would obtain a $T_{different}$ of amplified test methods and it might improve the performance of DCI.
About the evaluation, we run experimentation of 60 commits which the double than their dataset, and on real 
projects and real commits from \gh.

Wei Jin \etal \cite{automated-behavioral-regression-testing} propose BEhavioral Regression
Testing BERT.
BERT aims at assisting practitioners during development to identify potential regression.
It has been implemented as a plugin for the IDE Eclipse.
Each time a developer make a change in their code base and Eclipse compiles, BERT is triggered.
BERT works in 3 phases:
1) it analyzes what are the classes modified and runs a test generation tools, such as Randoop, to create new test input for these classes.
2) it executes the generated tests on both version of the program and collect multiples values such as the values of the fields of objects, the returned values by methods, etc.
3) it produces a report containing all the differences of behaviors based on the collected values.
Then the developer used this report to decide whether or not the changes are correct.
They evaluated BERT on a small and artificial project, showing that about 60\% of the automatically generated test inputs were able to reveal the behavioral difference that indicates the regression fault
In addition to this proof-of-concept, they evaluated in on JODA-time, which is a mature and widely used library.
They evaluated on 54 pairs of versions.
They reported 36 behavioral differences.
However, they could establish only for one of them was a regression fault.
There are two major differences with DCI:
1) DCI works at commit level and not to the class changes level.
2) DCI produces real and actionable test methods.

Taneja \etal \cite{Taneja:2008:DAR:1642931.1642986} present DiffGen, a tool that generate regression tests for two version of the same class.
Their approach works as follow:
First, they detect the changes between the two version of the class.
It is done using the textual representation and at method level.
Second, they generate what they call a test driver, which is a class that contains a method for each modified method.
These methods takes as input an instance of the old version of the class and the inputs required by the modified method.
They also make all the field public to compare their values between the old version and the new one.
These comparison have the form of branches.
The intuition is if the test generator engine is able to cover these branches, it will reveal the behavioral differences.
Third, they generate test using a test generator and the test driver.
Eventually, they execute the generated tests to see whether or not there is a behavioral difference.
They evaluated DiffGen on 8 artificial classes from the state of the art.
They compared the mutation score of their generated test suite to an existing method from the state of the art.
They showed that that DiffGen has an Improvement Factor IF2 varying from 23.4\% to 100\% for all the subjects.
They also performed an evaluation on larger subjects from the SIR dataset.
They detected 5 more faults than the state of the art.
DiffGen must modified the application code to be efficient while DCI does not required any modification of it.
Thus, is makes generated tests by DiffGen unused by developers since they must expose all the fields of their classes.

\subsection{Test amplification}

Yoo \etal \cite{Yoo:2012:TDR:2237756.2237758} devise Test Data Regeneration(TDR). They use hill climbing on existing test data (set of input) that meets a test objective (\eg cover all branch of a function).
The algorithm is based on \emph{neighborhood} and a \emph{fitness} functions as the classical hill climbing algorithm.
The key difference with DCI is that they at fulfilling a test criterion, such as branch coverage, while we aim at obtaining test methods that detect the behavioral changes.

It can be noted that several test generation techniques start from a seed and evolve it to produce a good test suite. This is the case for techniques such as concolic test generation \cite{godefroid2005dart}, search-based test generation \cite{fraser2012seed}, or random  test generation \cite{groce2007randomized}.
The key novelty of DCI relies in the very nature of the tests we used as seed.
DCI uses complete program, which creates objects, manipulates the state of these objects, calls methods on these objects and asserts properties on their behavior. That is to say real and complex object-oriented tests as seed

\subsection{Continuous Integration}

Hilton \etal~\cite{Hilton:2016:UsageCI} conduct a study on the usage, costs and benefits of CI.
To do this, they use three sources:  open-source code, builds from Travis, and they surveyed 442 engineers.
Their studies show that the usage of CI services such as Travis is widely used and became the trend.
The fact that CI is widely used shows that relevance of behavioral change detection.

Zampetti \etal~\cite{static:analysis:in:ci} investigate the usage of Automated Static Code Analysis Tools (ASCAT) in CI.
There investigation is done on 20 projects on \gh.
According to their findings, coding guideline checkers are the most used static analysis tools in CI.
This paper shows that dynamic analysis, such as DCI, is the next step for getting more added-value from CI.   

Spieker \etal~\cite{Spieker:RL:selection} elaborate a new approach for test case prioritization in continuous integration based on reinforcement learning.
Test case prioritization is different from behavioral change detection.

Waller \etal~\cite{Waller:2015:IPB:2735399.2735416} study the portability of performance tests in continuous integration.
They show little variations of performance tests between runs (every night) and claim that the performance tests must be integrated in the CI, early as possible in the development of Software.
Performance testing is also one kind of dynamic analysis for the CI, but different in nature from behavioral change detection.

\section{Conclusion}
\label{sec:conclusion}

%Conclusion
In this paper, we have studied the problem of behavioral change detection for continuous integration. 
We have proposed a novel technique called DCI, which uses assertion generation and search-based transformation of test code to generate tests that automatically detect behavioral changes in commits.
We have evaluated our technique on a curated set of 50 commits coming from real-world, large open-source Java projects.

%future works
We plan to work on an automated continuous integration bot for behavioral change detection that will:
1) check if a behavioral change is already specified in a commit (\ie a test case that correctly detects the behavioral change is provided);
2) if not, execute behavioral change detection and test generation;
3) propose the synthesized test method to the developers to complement the commit.
Such a bot can work in concert with other continuous integration bots, such as bots for automated program repair \cite{repairnator}.