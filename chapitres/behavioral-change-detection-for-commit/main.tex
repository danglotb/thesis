\chapter{Test Amplification For Commit Behavioral Changes Detection}
\label{chap:dci}


\begin{chaptersummary}
	In this chapter, I detail an extension of \dspot, called \DCI(\dspot-CI), and its evaluation.
	When developers use a version control management such as git, they make changes in the software in the form of commits.
	\DCI consists of obtaining amplified test methods that detect the behavioral changes introduced by commits.
	This evaluation has be done on 60 commits from 6 open-source projects on \gh.
	The result is that \DCI has been able to obtain amplified test methods detecting \todo{XX} behavioral changes.
	
	To sum up, the contributions of this chapter are:
	\begin{itemize}
		\item \DCI (\textbf{D}spot-\textbf{CI}), a complete approach to obtain automatically test methods that detect behavioral changes.
		\item An open-source implementation of \DCI for Java.
		\item A curated benchmark of 60 commits that introduce a behavioral change and include a test case to detect it, selected from 6 notable open source Java projects\footnote{\url{https://github.com/STAMP-project/dspot-experiments.git}}.
		\item A comprehensive evaluation based on 4 research questions that combines quantitative and qualitative analysis with manual assessment.
	\end{itemize}
	Note that this chapter is a to be published article\cite{}.
	The remainder of this chapter is as follows:
	\autoref{sec:dci:background} motivates this chapter and gives the background.
	\autoref{sec:dci:techniques} exposes the technical extension of \dspot: an approach for commit-based test selection. 
	\autoref{sec:dci:evaluation} introduces our benchmark of commits, the evaluation protocol and the results of our experiments on 50 real commits. 
	\autoref{sec:dci:threats} relates the threats validity and actions that have been taken to overcome them. 
	and \autoref{sec:dci:conclusion} concludes this chapter;
\end{chaptersummary}

\graphicspath{{.}{chapitres/behavioral-change-detection-for-commit/}}

\minitoc

% -----------------------
%  Introduction
% -----------------------
\section{Introduction}
\label{sec:dci:introduction}

% intro CI
\subsection{Collaborative software development} 
\label{subsec:dci:introduction:collaborative-software-development}
In collaborative software projects, developers work in parallel on the same code base. 
Every time a developer integrates her changes, she submits them in the form of a \emph{commit} to a version control system.
The \emph{Continuous Integration} (CI) server~\cite{fowler2006continuous} merges the commit with the master branch, compiles and automatically runs the test suite to check that the commit behaves as expected.
Its ability to detect bugs early makes CI an essential contribution to quality assurance~\cite{Hilton:2016:UsageCI,duvall2007continuous}.
However, the effectiveness of Continuous Integration depends on one key property: 
each commit should include at least one test case $t_{new}$ that specifies the intended change.
For instance, assume one wants to integrate a bug fix.
In this case, the developer is expected to include a new test method, $t_{new}$, that specifies the program's desired behavior after the bug fix is applied.
This can be mechanically verified: $t_{new}$ should fail on the version of the code that does not include the fix (the \emph{pre-commit} version), and pass on the version that includes the fix (the \emph{post-commit} version).
However, many commits either do not include a $t_{new}$ or $t_{new}$ does not meet this fail/pass criterion.
The reason is that developers sometimes cut corners because of lack of time, expertise or discipline. 
This is the problem addressed in this chapter.

\subsection{Goal}
\label{subsec:dci:introduction:goal}

The goal is to automatically generate test methods for each commit that is submitted to the CI.
In particular, a test method $t_{gen}$ that specifies the behavioral change of each commit.
A generated test method $t_{gen}$ is considered to be relevant if it satisfies the following property: 
$t_{gen}$ \textit{passes} on the pre-commit version and \textit{fails} on the post-commit version.
To do so, I developed a new approach, called \DCI (\textbf{D}etecting behavioral changes in \textbf{CI}), and propose it be used during continuous integration., that works in two steps:
First, it analyzes the test methods of the pre-commit version and select the ones that exercise the parts of the code modified by the commit.
Second, it applies \dspot on this subset of test methods.
The test selection is done only on amplified test methods that are relevant, \ie $t_{gen}$ \textit{passes} on the pre-commit version and \textit{fails} on the post-commit version.

% -----------------------
%  MOTIVATION
% -----------------------
\section{Motivation \& Background}
\label{sec:dci:background}

In this section, I introduce an example to motivate the need to generate new tests that specifically target the behavioral change introduced by a commit.
Then I introduce the key concepts on which the solution has been elaborated to address this challenging test generation task.

\subsection{Motivating Example}
\label{subsec:dci:background:example}

On August 10, a developer pushed a commit to the master branch of the XWiki-commons project. 
The change\footnote{\url{https://github.com/xwiki/xwiki-commons/commit/7e79f77}}, displayed in \autoref{fig:motivating_example}, adds a comparison to ensure the equality of the objects returned by \texttt{getVersion()}.
The developer did not write a test method nor modify an existing one. 

\begin{figure}[h]
	\centering
	\fbox{\includegraphics[width=.95\linewidth, trim=7.3cm 8.6cm 4.4cm 14.5cm, clip]{img/motivating_example.pdf}}
	\caption{Commit 7e79f77 on XWiki-Commons that changes the behavior without a test.}
	\label{fig:motivating_example}
\end{figure}

In this commit, the intent is to take into account the \texttt{version} (from method \texttt{getVersion}) in the \texttt{equals} method.
This change impacts the behavior of all methods that use it, the method \texttt{equals} being a highly used.
Such a central behavioral change may impact the whole program, and the lack of a test method for this new behavior may have dramatic consequences in the future.
Without a test method, this change could be reverted and go undetected by the test suite and the Continuous Integration server, \ie the build would still pass.
Yet, a user of this program would encounter new errors, because of the changed behavior.
The developer took a risk when committing this change without a test case.

\DCI aims at mitigating such risk: 
ensuring that every commit include a new test method or a modification of an existing test method.
In this chapter, I study \dspot's ability to automatically obtain a test method that highlights the behavioral change introduced by a commit.
This test method allows to identify the behavioral difference between the two versions of the program. 
The goal is to use this new test method to ensure that any changed behavior can be caught in the future.

Following, the vision of \DCI's usage:
when Continuous Integration is triggered, 
rather than just executing the test suite to find regressions, 
it could also run an analysis of the commit to know if it contains a behavioral change, 
in the form of a new method or the modification of an existing one.
If there is no appropriate test method to detect a behavioral change, the approach would provide one. 
\DCI would take as input the commit and a test suite, and generate a new test method that detects the behavioral change.

\subsection{Practicability}
\label{subsec:dci:background:practicability}

Following, the vision of an integration scenario of \DCI:

A developer commits a change into the program.
The Continuous Integration service is triggered;
the CI analyzes the commit.
There are two potential outcomes:
1) the developer provided a new test method or a modification to an existing one. 
In this case, the CI runs as usual, \eg it executes the test suite;
2) the developer did not provide a new test nor the modification of an existing one, the CI runs \DCI on the commit to obtain a test method that detects the behavioral change and present it to the developer.
The developer can then validate the new test method that detects the behavioral change.
Following the test selection\autoref{subsec:dci:introduction:goal}, the new test method passes on the pre-commit version but fails on the post-commit version.
The current amplified test method cannot be added to the test suite, since it fails.
However, this test method is still useful, since one has only to negate the failing assertions, \eg change an \texttt{assertTrue} into an \texttt{assertFalse}, to obtain a valid and passing test method that explicitly executes the new behavior.
This can be done manually or automatically with approaches such as ReAssert\cite{ReAssert}.

\DCI has been designed to be easy to use.
The only cost of \DCI is the time to set it up: in the ideal, happy-path case, it is meant to be a single command line through Maven goals.
Once \DCI is set up in continuous integration, it automatically runs at each commit and developers directly benefit from amplified test methods that strengthen the existing test suite.

%
%   Behavioral Change
%
\subsection{Behavioral Change}
\label{subsec:dci:background:behavioral-change}

A \emph{behavioral change} is a source-code modification that triggers a new state for some inputs \cite{saff2004experimental}.
Considering the pre-commit version $P$ and the post-commit version $P'$ of a program, the commit introduces a behavioral change if it is possible to implement a test method that can trigger and observe the change, \ie, it passes on $P$ and fails on $P'$, or the opposite.
In short, the behavioral change must have an impact on the observable behavior of the program.

%
%   Behavioral Change Detection
%
\subsection{Behavioral Change Detection}
\label{subsec:dci:background:behavioral-change-detection}

\emph{Behavioral change detection} is the task of identifying a behavioral change between two versions of the same program.
In this chapter, I propose a novel approach to detect behavioral changes based on test amplification.

% -----------------------
%  CONTRIBUTIONS
% -----------------------

\section{Behavioral Change Detection Approach}
\label{sec:dci:techniques}

\subsection{Overview of \DCI}
\label{sec:dci:techniques:overview}

\DCI takes as input a program, its test suite, and a commit modifying the program.
The commit, as done in version control systems, is basically the diff between two consecutive versions of the program.

% output
\DCI outputs new test methods that detect the behavioral difference between the pre- and post-commit versions of the program.
The new tests pass on a given version, but fail on the other, demonstrating the presence of a behavioral change captured.

\DCI computes the code coverage of the diff and selects test methods accordingly.
Then it applies \dspot to amplify selected test methods.
The resulting amplified test methods detect the behavioral change.

\autoref{fig:global_approach} sums up the different phases of the approach:
1) Compute the diff coverage and select the tests to be amplified;
2) Amplify the selected tests based on the pre-commit version;
3) Execute amplified test methods against the post-commit version, and keep the failing test methods.
This process produces test methods that pass on the pre-commit version, fail on the post-commit version, hence they detect at least one behavioral change introduced by a given commit.

\begin{figure}
    \fbox{\includegraphics[width=.95\linewidth]{img/global_flow.pdf}}
    \caption{Overview of the approach to detect behavioral changes in commits.}
    \label{fig:global_approach}
\end{figure}

\subsection{Test Selection and Diff Coverage}
\label{subsec:compute_diff_coverage}
\DCI implements a feature that:
\begin{enumerate*}
\item reports the diff coverage of a commit, and
\item selects the set of unit tests that execute the diff.
\end{enumerate*}
%
To do so, \DCI first computes the code coverage for the whole test suite.
Second, it identifies the test methods that hit the statements modified by the diff. 
Third, it produces the two outcomes elicited earlier: the diff coverage, computed as the ratio of statements in the diff covered by the test suite over the total number of statements in the diff and the list of test methods that cover the diff.
%
Then, it selects only test methods that are present in pre-commit version (\ie, it ignores the test methods added in the commit, if any).
The final list of test methods that cover the diff is then used to seed the amplification process.

\subsection{Test Amplification}
\label{sec:dci:techniques:amplification}

Once \DCI have the initial tests that cover the diff, \DCI amplifies them using \dspot.
Since \DCI uses \dspot, \DCI have also two mode:
1)\DCIA that uses only \Aampl and
2)\DCII that uses both \Aampl and \Iampl.

\subsection{Execution and Change Detection}
\label{sec:dci:techniques:execution-change-detection}

The final step performed by \DCI consists in checking whether that the amplified test methods detect behavioral changes.
Because \DCI amplifies test methods using the pre-commit version, all amplified test methods pass on this version, by construction. 
Consequently, for the last step, \DCI runs the amplified test methods only on the post-commit version. 
Every test that fails is in fact detecting a behavioral change introduced by the commit, and is a success.
\DCI keeps the tests that successfully detect behavioral changes.
Note that if the amplified test method is not executable on the post-commit version, \eg the API has been modified, the amplified test method is discarded.

\subsection{Implementation}
\label{sec:dci:techniques:implementation}

\DCI is implemented in Java and is built on top of the OpenClover and Gumtree~\cite{falleri:hal-01054552} libraries.
It computes the global coverage of the test suite with OpenClover, which instruments and executes the test suite.
Then, it uses Gumtree to have an AST representation of the diff.
\DCI matches the diff with the test that executes those lines. 
Through its Maven plugin, \DCI can be seamlessly implemented into continuous integration.
\DCI is publicly available on \gh.\footnote{\url{https://github.com/STAMP-project/dspot/tree/master/dspot-diff-test-selection}}

\section{Evaluation}
\label{sec:dci:evaluation}

The evaluation of \DCI relies on 4 research questions:

\newcommand{\rqdetection}{\RQ{1}{To what extent are \DCIA and \DCII able to produce amplified test methods that detect the behavioral changes?}}
\newcommand{\rqiteration}{\RQ{2}{What is the impact of the number of iteration performed by \DCII?}}
\newcommand{\rqselection}{\RQ{3}{What is the effectiveness of our test selection method?}}
\newcommand{\rqhuman}{\RQ{4}{How do human and generated tests that detect behavioral changes differ?}}

\noindent
\rqdetection\\
\rqiteration\\
\rqselection\\
\rqhuman\\

\subsection{Benchmark}
\label{subsec:dci:evaluation:benchmark}
To the best of my knowledge, there is no benchmark of commits in Java with behavioral changes in the literature. 
Consequently, I devise a project and commit selection procedure in order to construct a benchmark for the evaluation.

\paragraph{Project selection}
The evaluation needs software projects that are

1) publicly-available;

2) written in Java;

3) and use continuous integration.

The projects has been selected from the dataset in \cite{descartes} and \cite{Danglot2019}, which is composed of mature Java projects from \gh.

\paragraph{Commit selection}
%To select commits, we apply the following procedure.
Commits has been taken in inverse chronological order, from newest to oldest.
%this result in commits buildable and analyzable with the current version of build and test tools;
I select the first ten commits that match the following criteria:

1) the commit modifies Java files (most behavioral changes are source code changes.
It is known that behavioral changes can be introduced in other ways, such as modifying dependencies or configuration files \cite{Test:Coverage:Evolution}.
However, such modifications are not the target of \DCI.

2) the commit provides or modifies a manually written test that detects a behavioral change. 
To verify this property, I execute the test on the pre-commit version. 
If it fails, it means that the test detects at least 1 behavioral change.
This test will be used as a \textit{ground-truth test} in \textbf{RQ4}.

3) the changes of the commit must be covered by the pre-commit test suite.
To do so, I compute the diff coverage. 
If the coverage is 0\%, the commit is discarded. 
This is done because if the change is not covered, any test methods cannot be selected to be amplified, which is what a part of the evaluation.

Together, these criteria ensure that all selected commits:

1) introduce behavioral changes;

2) at least one test method can be used as ground-truth since it detects a behavioral change;

3) at least one test method executes the diff and can be used to seed the amplification process;

4) there is no structural change in the commit between both versions, \eg no change in method signature and deletion of classes (this is ensured since the pre-commit test suite compiles and runs against the post-commit version of the program and vice-versa).

\paragraph{Final benchmark}
\begin{table}[h]
\centering
\def\arraystretch{1}
\setlength\tabcolsep{0.8pt}
\caption{Considered Period for Selecting Commits.}
\begin{tabular}{lc|rr|cccc}
\hline
project &
LOC &
\begin{tabular}{c}start\\date\end{tabular}&
\begin{tabular}{c}end\\date\end{tabular}&
\begin{tabular}{c}\#total\\commits\end{tabular}&
\begin{tabular}{c}\#discarded\\commits\end{tabular}&
\begin{tabular}{c}\#matching\\commits\end{tabular}&
\begin{tabular}{c}\#selected\\commits\end{tabular}\\
\hline
\scriptsize{commons-io}	&	59607	&	9/10/2015	&	9/29/2018	&	385	&	375	&	16(4.16\%)	&	10	\\
\rowcolor[HTML]{EFEFEF}
\scriptsize{commons-lang}	&	77410	&	11/22/2017	&	10/9/2018	&	227	&	217	&	13(5.73\%)	&	10	\\
\scriptsize{gson}	&	49766	&	6/14/2016	&	10/9/2018	&	159	&	149	&	13(8.18\%)	&	10	\\
\rowcolor[HTML]{EFEFEF}
\scriptsize{jsoup}	&	20088	&	12/21/2017	&	10/10/2018	&	50	&	40	&	11(22.00\%)	&	10	\\
\scriptsize{mustache.java}	&	10289	&	7/6/2016	&	04/18/2019	&	68	&	58	&	11(16.18\%)	&	10	\\
\rowcolor[HTML]{EFEFEF}
\scriptsize{xwiki-commons}	&	87289	&	10/31/2017	&	9/29/2018	&	687	&	677	&	23(3.35\%)	&	10	\\
\end{tabular}
\label{tab:benchmark}
\end{table}

\autoref{tab:benchmark} shows the main statistics on the benchmark dataset. % construction.
The first column is the name of the considered project;
The second column is the date at which the analysis has been done;
The third column is the date of the oldest commit for the project;
The fourth, fifth, sixth and seventh are respectively the total number of commit analyzed, 
the total number of commits discarded, 
the number of commits that match all criteria but the third (there is no test in the pre-commit that execute the change).
Note that this benchmark is available on \gh at \url{https://github.com/STAMP-project/dspot-experiments}.

\subsection{Protocol}
\label{subsec:dci:evaluation:protocol}

To answer \textbf{RQ1}, \DCIA and \DCII is executed on the benchmark projects.
The total number of behavioral changes successfully detected by \DCI is reported.
That is to say the number of commits for which \DCI generates at least 1 test method that passes on the pre-commit version but fails on the post-commit version.
Also, 1 case study of a successful behavioral change detection is discussed.

To answer \textbf{RQ2}, \DCII with 1, 2 and 3 iterations is executed on the benchmark projects.
The number of behavioral changes successfully detected for each number of iterations in the main loop is reported.
Also, the number amplified test methods that detect the behavioral changes for each commit for 10 different seeds to study the impact of the randomness on the output of \dspot is reported.
A Kruskal-Wallis test statistic is performed on these numbers.

The null hypothesis is the following: 

The population median of all of the groups are equals.

The alternative hypothesis is: at least one population median of one group is different from the population median of at least one other group.

The confidence level is set at 95\%, or $\alpha = 0.05$.

For \textbf{RQ3}, the test selection method is considered effective if the tests selected to be amplified semantically relate to the code changed by the commit. 
To assess this, 1 commit per project in the benchmark is selected.
Then the automatically selected tests for this commit is manually analyzed to tell whether there are semantically related to the behavioral changes in the commit. 

To answer \textbf{RQ4}, the ground-truth tests written or modified by developers in the selected commits is used.
This ground-truth test method is compared to the amplified test methods that detect behavioral changes, for 1 commit per project.

\subsection{Results}
\label{subsec:result}
\begin{table}
\centering
\small
\def\arraystretch{0.3}%  1 is the default, change whatever you need
\setlength\tabcolsep{.35pt} % default value: 6pt
\caption{Performance evaluation of \DCI on 60 commits from 6 large open-source projects.}
\label{tab:overall_result}
\begin{tabular}{l|c|rcccc|c|cc|cc|cc|cc}
\input{chapitres/behavioral-change-detection-for-commit/table_content}
\end{tabular}
\end{table}

The overall results are reported in \autoref{tab:overall_result}.
The first column is the shortened commit id;
the second column is the commit date;
the third column column is the total number of test methods executed when building that version of the project;
the fourth and fifth columns are respectively the number of tests modified or added by the commit, and the size of the diff in terms of line additions (in green) and deletions (in red);
the sixth and seventh columns are respectively the diff coverage and the number of tests \DCI selected;
the eighth column provides the amplification results for \DCIA, and it is either a \cmark with the number of amplified tests that detect a behavioral change or a \textit{-} if \DCI did not succeed in generating a test that detects a change;
the ninth column displays the time spent on the amplification phase;
The tenth and eleventh columns are respectively a \cmark with the number of amplified tests for \DCII or a \textit{-} if \DCI did not succeed in generating a test that detects a change; for 3 iterations.

\subsubsection{Characteristics of commits with behavioral changes in the context of continuous integration}
\label{subsubsec:dci:evaluation:characteristics}

This section describes the characteristics of commits introducing behavioral changes in the context of continuous integration.
The number of test methods at the time of the commit shows two aspects of our benchmark:
1) there are only strongly tested projects;
2) the number of tests evolve over time due to test evolution.
Every commit in the benchmark comes with test modifications (new tests or updated tests), and commit sizes are quite diverse.
The three smallest commits are \textsc{commons-io\#703228a}, \textsc{gson\#44cad04} and \textsc{jsoup\#e5210d1} with 6 modifications, and the largest is \textsc{Gson\#45511fd} with 334 modifications.
%
Finally, on average, commits have 66.11\% coverage.
The distribution of diff coverage is reported graphically by \autoref{fig:histdiffcoverage}: 
in commons-io all selected commits have more than 75\% coverage.
In XWiki-Commons, only 50\% of commits have more than 75\% coverage. 
Overall, 31 / 60 commits have at least 75\% of the changed lines covered.
This validates the correct implementation of the selection criteria that ensures the presence of a test specifying the behavioral change.

\begin{figure}
\centering
\includegraphics[width=.95\linewidth]{img/diff_cov_hist.png}
\caption{Distribution of diff coverage per project of our benchmark.}
\label{fig:histdiffcoverage}
\end{figure}

Thanks to the selection criteria, 
a curated benchmark of 50 commits with a behavioral change is available for the evaluation.
This benchmark comes from notable open-source projects, and covers a diversity of commit sizes. 
The benchmark is publicly available and documented for future research on this topic.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% RESULTS RQ1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\rqdetection}
\label{subsubsec:dci:evaluation:rq1}

The last 4 columns of \autoref{tab:overall_result} are dedicated to \textbf{RQ$_1$}.
For example, \DCIA and \DCII generated respectively 1 and 39 amplified test methods that detect the behavioral change for \textsc{commons-io\#f00d97a} (4$^{th}$ row).
In the other hands, only \DCII has been able to obtain amplified test methods for \textsc{commons-io\#81210eb} (8$^{th}$ row).

Overall, \DCIA generates amplified tests that detect 9 out of 60 behavioral changes.
Meanwhile, \DCII generates amplified tests that detect \TODO{25} out of 60 behavioral changes.

Regarding the number of generated tests.
\DCII generates a large number of test methods, compared to \DCIA only (15 versus 6708, see column ``total'' at the bottom of the table). 
Both \DCIA and \DCII can generate amplified tests, however since \DCIA does not produce a large amount of test methods the developers do not have to triage a large set of test cases.
Also, since \DCIA only adds assertions, the amplified tests are easier to understand than the ones generated by \DCII.

% time
\DCII takes more time than \DCIA (for successful cases 38.7 seconds versus 3.3 hours on average).
The difference comes from the time consumed during the exploration of the input space in the case of \DCII, while \DCIA focuses on the amplification of assertions only, which represents a much smaller space of solutions. 

Overall, \DCI successfully generates amplified tests that detect a behavioral change in 46\% of the commits in our benchmark(25 out of 60).
Recall that the 60 commits analyzed are real changes in complex code bases.
They represent modifications, sometimes deep in the code, that are challenges with respect to testability~\cite{voas1995software}.
Consequently, the fact \DCI generates test cases that detect behavioral changes, is considered an achievement.
The commits for which \DCI fails to detect the change can be considered as a target for future research on this topic.

A successful detection by an amplified test method is analyzed.
Commit \textsc{3fadfdd}\footnote{\url{https://github.com/apache/commons-lang/commit/3fadfdd}} from commons-lang has been selected because it is succinct enough to be discussed.
The diff is shown in \autoref{fig:diff_commons_lang_success}.

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.95\linewidth, trim=4.5cm 14.85cm 6.5cm 4.3cm, clip]{img/diff/success-diff-commons-lang.pdf}}
\caption{Diff of commit \textsc{3fadfdd} from commons-lang.}
\label{fig:diff_commons_lang_success}
\end{figure}

The developer added a method call to a method that escapes specials characters in a string. 
The changes come with a new test method that specifies the new behavior.

\DCI starts the amplification from the \texttt{testNestingPerson} test method defined in \texttt{JsonToStringStyleTest}. 
The test is selected for amplification because it triggers the execution of the changed line.

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.95\linewidth, trim=3.7cm 46.5cm 9.65cm 38.3cm, clip]{img/amplified/success-ampl-commons-lang.pdf}}
\caption{Test generated by DCI that detects the behavioral change of \textsc{3fadfdd} from commons-lang.}
\label{fig:amplified_commons_lang_success}
\end{figure}

The resulting amplified test method is shown in \autoref{fig:amplified_commons_lang_success}.
From this test method, \DCI generates an amplified test method shown in \autoref{fig:amplified_commons_lang_success}. 
In this generated test, \sbampl applies 2 input transformations: 1 duplication of method call and 1 character replacement in an existing String literal.
The latter transformation is the key transformation: \DCI replaced an 's' inside "person" by '/' resulting in "per/on" where "/" is a special character that must be escaped (Line 2). 
Then, \DCI generated 11 assertions, based on the modified inputs. 
The amplified test the behavioral change:
in the pre-commit version, the expected value is: \texttt{"\{ ... per/on":\{"name":"Jane Doe" ...\}"} while in the post-commit version it is \texttt{"\{ ... per\textbackslash/on":\{"name":"Jane Doe" ...\}"} (Line 3).
 
\begin{mdframed}
Answer to \textbf{RQ1}: Overall, \DCI detects the behavioral changes in a total of 25/60 commits. 
Individually, \DCII finds changes in 25/60, while \DCIA in 9/60 commits.
Since \DCII also uses \aampl to generate assertions, all \DCIA's commits are contained in \DCII's. 
However, the search-based algorithm, through exploration, finds many more behavioral changes, making it more effective albeit at the cost of execution time.
\end{mdframed}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% RESULTS RQ2 : ITERATION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{\rqiteration}
\label{subsubsec:dci:evaluation:rq2}

The results are reported in \autoref{tab:overall_result_iteration}
This table can be read as follow:
the first column is the name of the project;
the second column is the commit identifier;
then, the third, fourth, fifth, sixth, seventh and eighth provide the amplification results and execution time for each number of iteration 1, 2, and 3.
A \cmark indicates with the number of amplified tests that detect a behavioral change and a \textit{-} denotes that \DCI did not succeed in generating a test that detects a change.

Overall, \DCII generates amplified tests that detect \todo{21}, \todo{23}, and \todo{24} out of 60 behavioral changes for respectively $iteration=1$, $iteration=2$ and $iteration=3$.
The more iteration \DCII does, the more it explores, the more it generates amplified tests that detect the behavioral changes but the more it takes time also.
When \DCII is used with $iteration=3$, it generates amplified test methods that detect \todo{3} more behavioral changes than when it is used with $iteration=1$ and 1 more than when it is used with $iteration=2$. It represents an increase of \todo{14\%} and \todo{4\%} for respectively $iteration=1$ and $iteration=2$.

In average, \DCII generates 18, 53, and 116 amplified tests for respectively $iteration=1$, $iteration=2$ and $iteration=3$. 
This number increases by 544\% from $iteration=1$ to $iteration=3$.
This increase is explained by the fact that \DCII explores more with more iteration and thus is able to generate more amplified test methods that detect the behavioral changes.

In average \DCII takes 23, 64, and 105 minutes to perform the amplification for respectively $iteration=1$, $iteration=2$ and $iteration=3$.
This number increases by 356\% from $iteration=1$ to $iteration=3$.

\begin{table*}
\small
\def\arraystretch{0.7}%  1 is the default, change whatever you need
\setlength\tabcolsep{4pt} % default value: 6pt
\caption{Evaluation of the impact of the number of iteration done by \DCII on 60 commits from 6 open-source projects.}
%\rotvertical{
\label{tab:overall_result_iteration}
\begin{tabular}{l|c|cc|cc|cc}
\input{chapitres/behavioral-change-detection-for-commit/table_content_iterations}
\end{tabular}
%}
\end{table*}

\paragraph{Impact of the randomness}

The number of amplified test methods obtained by the different seeds are reported in \autoref{tab:overall_result_seeds}.
The result of the Kruskal-Wallis test is:  $p-value=0.96$.
$p-value>\alpha$ which means that the null hypothesis holds:
The population median of all of the groups are equal.
On other words, the choice of the seeds has not a significant impact of the overall result of \DCI.

\begin{table*}
\small
\def\arraystretch{.5}%  1 is the default, change whatever you need
\setlength\tabcolsep{3pt} % default value: 6pt
\caption{Number of amplified test methods obtained by \DCI for 10 different seeds. The first column is the id of the commit. The second column is the result obtained with the default seed, used during the evaluation for \rqdetection. The ten following columns are the result obtained for the 10 different seeds.}
\label{tab:overall_result_seeds}
\begin{tabular}{l|c|llllllllll}
\input{chapitres/behavioral-change-detection-for-commit/table_content_seeds.tex}
\end{tabular}
\end{table*}

%recap RQ2
\begin{mdframed}
Answer to \textbf{RQ2}: \DCII detects  \TODO{21}, \TODO{23}, and \TODO{24} behavioral changes out of 60 for respectively $iteration=1$, $iteration=2$ and $iteration=3$.
The number of iteration done by \DCII impacts the number of behavioral changes detected, the number of amplified test methods obtained and the execution time.
\end{mdframed}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% RESULTS RQ3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{\rqselection}
\label{subsubsec:dci:evaluation:rq3}

To answer \textbf{RQ3}, there is no quantitative approach to take, because there is no ground-truth data or metrics to optimize. 
Per the protocol described in \autoref{subsec:protocol}, the answer to this question is based on manual analysis:
1 commit per project is randomly selected.
Then the relevance of the selected tests for amplification is analyzed.

Following an example, in order to give an intuition of what are the characteristics of the test selection for amplification to be relevant.
The selection is considered relevant If \texttt{TestX} is selected for amplification, following a change to method \texttt{X}.
The key is that \DCI will generate an amplified test \texttt{TestX'} that is a variant of \texttt{TestX}, and, consequently, the developer will directly get the intention of the new test \texttt{TestX'} and what behavioral change it detects.

\textsc{Commons-io\#c6b8a38}\footnote{\url{https://github.com/apache/commons-io/commit/c6b8a38}}: the test selection returns 3 test methods: \texttt{testContentEquals}, \texttt{testCopyURLToFileWithTimeout} and \texttt{testCopyURLToFile} from the same test class: \texttt{FileUtilsTestCase}.
The considered commit modifies the method \texttt{copyToFile} from \texttt{FileUtils}.
There is a link between the changed file and the intention of 2 out 3 tests to be amplified.
The selection is thus considered relevant.

\textsc{Commons-lang\#f56931c}\footnote{\url{https://github.com/apache/commons-lang/commit/f56931c}}: the test selection returns 39 test methods from 5 test classes: \texttt{FastDateFormat\_ParserTest}, \texttt{FastDateParserTest}, \texttt{DateUtilsTest}, \texttt{FastDateParser\_TimeZoneStrategyTest} and \texttt{FastDateParser\_MoreOrLessTest}.
This commit modifies the behavior of two methods: \texttt{simpleQuote} and \texttt{setCalendar} of class \texttt{FastDateParser}.
When manually analyzed, it reveals two intentions:
1) test behaviors related to parsing;
1) test behaviors related to dates.
While this is meaningful, a set of 39 methods is clearly not a focused selection, not as focused as for the previous example.
The selection can be considered relevant, but not focused.

\textsc{Gson\#9e6f2ba}\footnote{\url{https://github.com/google/gson/commit/9e6f2ba}}: the test selection returns 9 test methods from 5 different test classes.
3 out of those 5 classes \texttt{JsonElementReaderTest}, \texttt{JsonReaderPathTest} and \texttt{JsonParserTest} relate to the class modified in the commit(\texttt{JsonTreeReader}).
The selection is thus considered relevant but unfocused.

\textsc{Jsoup\#e9feec9}\footnote{\url{https://github.com/jhy/jsoup/commit/e9feec9}}, the test selection returns the 4 test methods defined in the \texttt{XmlTreeBuilderTest} class \texttt{caseSensitiveDeclaration}, \texttt{handlesXmlDeclarationAsDeclaration}, \texttt{testDetectCharsetEncodingDeclaration} and \texttt{testParseDeclarationAttributes}.
The commit modifies the behavior of the class \texttt{XmlTreeBuilder}.
Here, the test selection is relevant.
Actually, the ground-truth manual test added in the commit is also in the \texttt{XmlTreeBuilderTest} class.
If \DCI proposes a new test there to capture the behavioral change, the developer will understand its relevance and its relation to the change.

\textsc{Mustache.java\#88718bc}\footnote{\url{https://github.com/spullara/mustache.java/commit/88718bc}}, the test selection returns the \texttt{testInvalidDelimiters} test method defined in the \texttt{com.github.mustachejava.InterpreterTest} test class.
The commit improves an error message when an invalid delimiter is used.
Here, the test selection is relevant since it selected \texttt{testInvalidDelimiters} which is the dedicated test to the usage of the test invalid delimiters.
This ground-truth test method is also in the test class \texttt{com.github.mustachejava.InterpreterTest}.

\textsc{Xwiki-commons\#848c984}\footnote{\url{https://github.com/xwiki/xwiki-commons/commit/848c984}} the test selection returns a single test method \texttt{createReference} from test class \texttt{XWikiDocumentTest}.
The main modification of this commit is on class \texttt{XWikiDocument}.
Since \texttt{XWikiDocumentTest} is the test class dedicated to \texttt{XWikiDocument}, the selection is considered relevant.

\begin{mdframed}
Answer to \textbf{RQ3}: 
In 4 out of 6 of the manually analyzed cases, the tests selected to be amplified relate, semantically, to the modified application code. 
In the 2 remaining cases, it selected over and above the tests to be amplified.
That is, it selects tests whose intention is semantically pertinent to the change, but it also includes tests that are not.
However, even in this case, \DCI's test selection provides developers with important and targeted context to better understand the behavioral change at hand.
\end{mdframed}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% RESULTS RQ4
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\rqhuman}
\label{subsubsec:dci:evaluation:rq4}

When \DCI generates an amplified test method that detects the behavioral change, it can be compared to the ground truth version (the test added in the commit) to see whether it captures the same behavioral change.
For each project, I select 1 successful application of \DCI, and compare the \DCI test against the human test.
If they capture the same behavioral change, it means they have the same intention and the amplification is considered as a success.

%%% COMMONS IO

\textsc{commons-io\#81210eb}\footnote{\url{https://github.com/apache/commons-io/commit/81210eb}}: This commit modifies the behavior of the \texttt{read()} method in \texttt{BoundedReader}.
\autoref{fig:ampl_commons-io} shows the test generated by \DCII.
This test is amplified from the existing \texttt{readMulti} test, which indicates that the intention is to test the read functionality.
The first line of the test is the construction of a \texttt{BoundedReader} object which is also the class modified by the commit.
\DCII modified the second parameter of the constructor call (transformed $3$ into a $0$) and generated two assertions (only 1 is shown).
The first assertion, associated to the new test input, captures the behavioral difference.
Overall, this can be considered as a successful amplification.

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.97\linewidth, trim=3.6cm 9.3cm 14.7cm 36.8cm, clip]{img/amplified/ampl-commons-io.pdf}}
\caption{Test generated by \DCII that detects the behavioral change introduced by commit \textsc{81210eb} in commons-io.}
\label{fig:ampl_commons-io}
\end{figure}

Now, look at the human test contained in the commit, shown in \autoref{fig:diff_commons-io}.
It captures the behavioral change with the timeout (the test timeouts on the pre-commit version and goes fast enough on the post-commit version). 
Furthermore, it only indirectly calls the changed method through a call to \texttt{readLine}.

In this case, the \DCI test can be considered better than the developer test because
1) it relies on assertions and not on timeouts, and
2) it directly calls the changed method (\texttt{read}) instead of indirectly. 

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.97\linewidth, trim=4cm 6.7cm 12.6cm 25.8cm, clip]{img/diff/diff-commons-io.pdf}}
\caption{Developer test for commit \textsc{81210eb} of commons-io.}
\label{fig:diff_commons-io}
\end{figure}

%%% COMMONS LANG
\textsc{commons-lang\#e7d16c2}\footnote{\url{https://github.com/apache/commons-lang/commit/e7d16c2}}: this commit escapes special characters before adding them to a \texttt{StringBuffer}.
\autoref{fig:ampl_commons-lang} shows the amplified test method obtained by \DCII.
The assertion at the bottom of the excerpt is the one that detects the behavioral change.
This assertion compares the content of the \texttt{StringBuilder} against an expected string.
In the pre-commit version, no special character is escaped, \eg '\textbackslash n'.
In the post-commit version, the amplified test fails since the code now escapes the special character \textbackslash.

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.97\linewidth, trim=2.6cm 8.35cm 10.8cm 20.85cm, clip ]{img/amplified/ampl-commons-lang.pdf}}
\caption{Test generated by \DCII that detects the behavioral change of \textsc{e7d16c2} in commons-lang.}
\label{fig:ampl_commons-lang}
\end{figure}

Let's have a look to the human test method shown in \autoref{fig:diff_commons-lang}.
Here, the developer specified the new escaping mechanism with 5 different inputs.

The main difference between the human test and the amplified test is that the human test is more readable and uses 5 different inputs.
However, the amplified test generated by \DCI is valid since it detects the behavioral change correctly.

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.97\linewidth, trim=4.5cm 9.2cm 5.8cm 39.2cm, clip]{img/diff/diff-commons-lang.pdf}}
\caption{Developer test for \textsc{e7d16c2} of commons-lang.}
\label{fig:diff_commons-lang}
\end{figure}

%% GSON
\textsc{gson\#44cad04}\footnote{\url{https://github.com/google/gson/commit/44cad04}}: This commit allows Gson to deserialize a number represented as a string.
\autoref{fig:ampl_gson} shows the relevant part of the test generated by \DCII, based on \texttt{testNumberDeserialization} of \texttt{PrimitiveTest} as a seed.
The \DCI test detects the behavioral changes at lines 3 and 4.
On the pre-commit version, line 3 throws a \texttt{JsonSyntaxException}.
On the post-commit version, line 4 throws a \texttt{NumberFormatException}.
In other words, the behavioral change is detected by a different exception (different type and not thrown at the same line).
\footnote{Interestingly, the number is parsed lazily, only when needed. 
Consequently, the exception is thrown when invoking the \texttt{longValue()} method and not when invoking \texttt{parse()}}.

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.95\linewidth, trim=3.7cm 19.5cm 13.4cm 64.2cm, clip]{img/amplified/ampl-gson.pdf}}
\caption{Test generated by DCI that detects the behavioral change of commit \textsc{44cad04} in Gson.}
\label{fig:ampl_gson}
\end{figure}

The amplified test methods is now compared against the developer-written ground-truth method, shown in \autoref{fig:diff_gson}. 
This short test verifies that the program handles a number-as-string correctly.
For this example, the \DCI test does indeed detect the behavioral change, but in an indirect way.
On the contrary, the developer test is shorter and directly targets the changed behavior, which is better.

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.95\linewidth, trim=4cm 9.3cm 12.5cm 20.6cm ,clip]{img/diff/diff-gson.pdf}}
\caption{Provided test by the developer for \textsc{44cad04} of Gson.}
\label{fig:diff_gson}
\end{figure}

%% JSOUP
\textsc{jsoup\#3676b13}\footnote{\url{https://github.com/jhy/jsoup/commit/3676b13}}: This change is a pull request (\ie a set of commits) and introduces 5 new behavioral changes.
There are two improvements: skip the first new lines in pre tags and support deflate encoding, and three bug fixes: throw exception when parsing some urls, add spacing when output text, and no collapsing of attribute with empty values.
\autoref{fig:ampl_jsoup} shows an amplified test obtained using \DCII.
This amplified test has 15 assertions and a duplication of method call.
Thanks to this duplication and these generated assertions on the \texttt{toString()} method, this test is able to capture the behavioral change introduced by the commit.

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.95\linewidth, trim=3.7cm 82cm 7cm 30.8cm ,clip]{img/amplified/ampl-jsoup.pdf}}
\caption{Test generated by \DCII that detects the behavioral change of \textsc{3676b13} of Jsoup.}
\label{fig:ampl_jsoup}
\end{figure}

As before, the amplified test method is compared to the developer's test. 
The developer uses the \texttt{Element} and \texttt{outerHtml()} methods rather than \texttt{Attribute} and \texttt{toString()}.
However, the method \texttt{outerHtml()} in \texttt{Element} will call the \texttt{toString()} method of \texttt{Attribute}.
For this behavioral change, it concerns the \texttt{Attribute} and not the \texttt{Element}.
So, the amplified test is arguably better, since it is closer to the change than the developer's test.
But, \DCII generates amplified tests that detect 2 of 5 behavioral changes: adding spacing when output text and no collapsing of attribute with empty values only, so regarding the quantity of changes, the human tests are more complete.

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.95\linewidth, trim=4.3cm 19.6cm 4.8cm 131.5cm , clip]{img/diff/diff-jsoup.pdf}}
\caption{Provided test by the developer for \textsc{3676b13} of Jsoup.}
\label{fig:diff_jsoup}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% MUSTACHE JAVA
\textsc{Mustache.java\#774ae7a}\footnote{\url{https://github.com/spullara/mustache.java/commit/774ae7a}}: This commit fixes an issue with the usage of a dot in a relative path on Window in the method \texttt{getReader} of class \texttt{ClasspathResolver}.
The test method \texttt{getReaderNullRootDoesNotFindFileWithAbsolutePath} has been used as seed by \DCI.
It modifies the existing string literal with another string used somewhere else in the test class and generates 3 new assertions.
The behavioral change is detected thanks to the modified strings: it produces the right test case containing a space.

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.95\linewidth]{img/amplified/ampl-mustache.png}}
\caption{Test generated by \DCII that detects the behavioral change of \textsc{774ae7a} of Mustache.java.}
\label{fig:ampl_mustache}
\end{figure}

The developer proposed two tests that verify that the object reader is not null when getting it with dots in the path.
There are shown in \autoref{fig:diff_mustache}.
These tests invoke the method \texttt{getReader} which is the modified method in the commit.
%
The difference is that the \DCII's amplified test method provides a non longer valid input for the method \texttt{getReader}.
However, providing such inputs produce errors afterward which signal the behavioral change.
In this case, the amplified test is complementary to the human test since it verifies that the wrong inputs are no longer supported and that the system immediately throws an error.

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.95\linewidth]{img/diff/diff-mustache.png}}
\caption{Developer test for \textsc{774ae7a} of Mustache.java.}
\label{fig:diff_mustache}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% XWIKI COMMONS
\textsc{xwiki-commons\#d3101ae}\footnote{\url{https://github.com/xwiki/xwiki-commons/commit/d3101ae}}: This commit fixes a bug in the \texttt{merge} method of class \texttt{DefaultDiffManager}.
\autoref{fig:ampl_xwiki} shows the amplified test method obtained by \DCIA.
\DCI used \texttt{testMergeCharList} as a seed for the amplification process, and generates 549 new assertions.
Among them, 1 assertion captures the behavioral change between the two versions of the program: 
``assertEquals(0, result.getLog().getLogs(LogLevel.ERROR).size());''.
The behavioral change that is detected is the presence of a new logging statement in the diff. 
After verification, there is indeed such a behavioral change in the diff, with the addition of a call to ``logConflict'' in the newly handled case.

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.95\linewidth, trim=3.5cm 11.5cm 9.2cm 18.2cm, clip ]{img/amplified/ampl-xwiki.pdf}}
\caption{Test generated by \DCIA that detects the behavioral change of \textsc{d3101ae} of XWiki.}
\label{fig:ampl_xwiki}
\end{figure}

The developer's test is shown in \autoref{fig:diff_xwiki}.
This test method directly calls method \texttt{merge}, which is the method that has been changed. 
What is striking in this test is the level of clarity: 
the variable names, the explanatory comments and even the vertical space formatting are impossible to achieve with \DCIA and makes the human test clearly of better quality but also longer to write.

Yet, \DCIA's amplified tests capture a behavioral change that was not specified in the human test.
In this case, amplified tests can be complementary.

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=.95\linewidth, trim=4.2cm 6.7cm 5cm 75cm, clip]{img/diff/diff-xwiki.pdf}}
\caption{Developer test for \textsc{d3101ae} of XWiki.}
\label{fig:diff_xwiki}
\end{figure}


\begin{mdframed}
	% commons-io is better
	% commons-lang is correct (but not as good as the human's)
	% gson is kind of valid but indirect
	% jsoup is partially better, it test better some new behaviors but not all of them // complementary
	% mustche is complementary
	% xwikiis complementary
Answer to \textbf{RQ4}: 
In 3 of 6 cases, the \DCI test is complementary to the human test.
In 1 case, the \DCI test can be considered better than the human test.
In 2 cases, the human test is better than the \DCI test.
Even though human tests can be better, \DCI can be complementary and catch missed cases, or can provide added-value when developers do not have the time to add a test.
\end{mdframed}

%\pagebreak


% -----------------------
%  Main Observed Limitation
% -----------------------
\section{Limitations}
\label{sec:dci:limitations}

\textbf{Time consumption}
From the experiments, it is deducible that the time consumption to complete the amplification is the main limitation of \DCI.
\textsc{jsoup\#2c4e79b}, almost 5 hours have been spent with no result.
For the sake of the experimentation, a pre-defined number of iteration has been choose to bound the exploration.
In practice, setting a time budget would be recommended (\eg at most one hour per pull-request).

\textbf{Importance of test seeds}
By construction, \DCI's effectiveness is correlated to the test methods used as seed.
For example, see the row of \texttt{commons-lang\#c8e61af} in \autoref{tab:overall_result_iteration}
where one can observe that whatever the number of iteration, \DCI takes the same time to complete the amplification.
The reason is that the seed tests are only composed of assertions statements.
Such tests are bad seeds for \DCI and they prevent any good input amplification.

\textbf{False positive}
The risk of false positives is a potential limitation of the approach.
A false positive would be an amplified test method that passes or fails on both versions, which means that the amplified test method does not detect the behavioral difference between both versions.
I manually analyzed 6 commits and none of them are false positives.
While this is not a proof that \DCI would never produce such confusing test methods, It encourages to be confident in the soundness of the implementation.

% -----------------------
%  THREATS
% -----------------------
\section{Threats to validity}
\label{sec:dci:threats}

% bug in the implementation
An internal threat is the potential bugs in the implementation of \DCI.
However, it is heavily tested, with \junit test suite to mitigate this threat.

% benchmark
In the benchmark, there are 60 commits. 
The result may be not be generalizable to all programs. 
But real and diverse applications from \gh have been carefully selected, all having a strong test suite. 
The benchmark reflects real programs, and provides an high confidence in the result.

% weak statiscal test
For the evaluation of the randomness, a Kruskal-Willis test has been used, which is known to be weaker than ANOVA test.
To perform an ANOVA test, the data must fulfil the following criteria:
1) The samples are independent;
2) Each sample is from a normally distributed population;
3) The population standard deviations of the groups are all equal. This property is known as homoscedasticity.
The two first are fulfilled while the third is not:
\begin{table*}
\def\arraystretch{1}%  1 is the default, change whatever you need
\setlength\tabcolsep{3pt} % default value: 6pt
\caption{Standard deviations of the number of amplified tests obtained for each seed.}
\begin{tabular}{c|cccccccccc}
seed     & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
\hline
std     & 63.38&63.55&62.56&61.27&61.33&61.66&63.76&60.91&61.25&63.35
\end{tabular}
\end{table*}
Since the standard deviations are not all equal, the associated p-value would not be valid.
This is why a Kruskal-Willis test has been chosen.

In addition to this, only 11 seeds has been used to perform it, which a small samples.

However, the seeds used in \dspot's algorithm seems to not have an impact on the overall results.

\section{Conclusion}
\label{sec:dci:conclusion}