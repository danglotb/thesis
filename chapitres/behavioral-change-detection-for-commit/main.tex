\chapter{Test Amplification For Behavioral Changes Detection Of Commits}
\label{chap:dci}

\graphicspath{{.}{chapitres/behavioral-change-detection-for-commit/}}

\minitoc

In collaborative software projects, developers work in parallel on the same code base. 
Every time a developer integrates her changes, she submits them in the form of a \emph{commit} to a version control system.
The \emph{Continuous Integration} (CI) server~\cite{fowler2006continuous} merges the commit with the master branch, compiles and automatically runs the test suite to check that the commit behaves as expected.
Its ability to detect bugs early makes CI an essential contribution to quality assurance~\cite{Hilton:2016:UsageCI,duvall2007continuous}.
However, the effectiveness of Continuous Integration depends on one key property: 
each commit should include at least one test case $t_{new}$ that specifies the intended change.

For instance, assume one wants to integrate a bug fix.
In this case, the developer is expected to include a new test method, $t_{new}$, that specifies the program's desired behavior after the bug fix is applied.
This can be mechanically verified: $t_{new}$ should fail on the version of the code that does not include the fix (the \emph{pre-commit} version), and pass on the version that includes the fix (the \emph{post-commit} version).
However, many commits either do not include a $t_{new}$ or $t_{new}$ does not meet this fail/pass criterion.
The reason is that developers sometimes cut corners because of lack of time, expertise or discipline. 
This is the problem addressed in this chapter.

In this chapter, I detail an extension of \dspot, called \DCI(\dspot-CI), and its evaluation.
The goal is to automatically generate test methods for each commit that is submitted to the CI.
In particular, to generate a test method $t_{gen}$ that specifies the behavioral change of each commit.

\DCI works in two steps:
First, it analyzes the test methods of the pre-commit version and selects the ones that exercise the parts of the code modified by the commit.
Second, it applies \dspot on this subset of test methods.
The test selection is done only on amplified test methods that are relevant, \ie $t_{gen}$ \textit{passes} on the pre-commit version and \textit{fails} on the post-commit version.

This evaluation has been performed on 60 commits from 6 open-source projects on \gh.
The result is that \DCI has been able to obtain amplified test methods detecting 25 behavioral changes.

To sum up, the contributions of this chapter are:
\begin{itemize}
	\item \DCI (\textbf{D}spot-\textbf{CI}), a complete approach to obtain automatically test methods that detect behavioral changes.
	\item An open-source implementation of \DCI for Java.
	\item A curated benchmark of 60 commits that introduce a behavioral change and include a test case to detect it, selected from 6 notable open source Java projects\footnote{\url{https://github.com/STAMP-project/dspot-experiments.git}}.
	\item A comprehensive evaluation based on 4 research questions that combines quantitative and qualitative analysis with manual assessment.
\end{itemize}

Note that this chapter is a to be published article~\cite{DBLP:journals/corr/abs-1902-08482}.
The remainder of this chapter is as follows:
\autoref{sec:dci:background} motivates this chapter and gives the background.
\autoref{sec:dci:techniques} exposes the technical extension of \dspot: an approach for commit-based test selection. 
\autoref{sec:dci:evaluation} introduces our benchmark of commits, the evaluation protocol and the results of our experiments on 50 real commits. 
\autoref{sec:dci:threats} relates the threats validity and actions that have been taken to overcome them. 
and \autoref{sec:dci:conclusion} concludes this chapter.

% -----------------------
%  MOTIVATION
% -----------------------
\section{Motivation \& Background}
\label{sec:dci:background}

In this section, I introduce an example to motivate the need to generate new tests that specifically target the behavioral change introduced by a commit.
Then I introduce the key concepts on which the solution has been elaborated to address this challenging test generation task.

\subsection{Motivating Example}
\label{subsec:dci:background:example}

On August 10, a developer pushed a commit to the master branch of the XWiki-commons project. 
The change\footnote{\url{https://github.com/xwiki/xwiki-commons/commit/7e79f77}}, displayed in \autoref{fig:motivating_example}, adds a comparison to ensure the equality of the objects returned by \texttt{getVersion()}.
The developer did not write a test method nor modify an existing one. 

\begin{figure}[h]
	\centering
	\caption{Commit 7e79f77 on XWiki-Commons that changes the behavior without a test.}
	\label{fig:motivating_example}
	\fbox{\includegraphics[width=.95\linewidth, trim=7.3cm 8.6cm 4.4cm 14.5cm, clip]{img/motivating_example.pdf}}
\end{figure}

In this commit, the intent is to take into account the \texttt{version} (from method \texttt{getVersion}) in the \texttt{equals} method.
This change impacts the behavior of all methods that use it, the method \texttt{equals} being a highly used.
Such a central behavioral change may impact the whole program, and the lack of a test method for this new behavior may have dramatic consequences in the future.
Without a test method, this change could be reverted and go undetected by the test suite and the Continuous Integration server, \ie the build would still pass.
Yet, a user of this program would encounter new errors, because of the changed behavior.
The developer took a risk when committing this change without a test case.

\DCI aims at mitigating such risk: 
ensuring that every commit include a new test method or a modification of an existing test method.
In this chapter, I study \dspot's ability to automatically obtain a test method that highlights the behavioral change introduced by a commit.
This test method allows to identify the behavioral difference between the two versions of the program. 
The goal is to use this new test method to ensure that any changed behavior can be caught in the future.

Following, the vision of \DCI's usage:
when Continuous Integration is triggered, 
rather than just executing the test suite to find regressions, 
it could also run an analysis of the commit to know if it contains a behavioral change, 
in the form of a new method or the modification of an existing one.
If there is no appropriate test method to detect a behavioral change, the approach would provide one. 
\DCI would take as input the commit and a test suite, and generate a new test method that detects the behavioral change.

\subsection{Practicability}
\label{subsec:dci:background:practicability}

Following, the vision of an integration scenario of \DCI:

A developer commits a change into the program.
The Continuous Integration service is triggered;
the CI analyzes the commit.
There are two potential outcomes:
1) the developer provided a new test method or a modification to an existing one. 
In this case, the CI runs as usual, \eg it executes the test suite;
2) the developer did not provide a new test nor the modification of an existing one, the CI runs \DCI on the commit to obtain a test method that detects the behavioral change and present it to the developer.
The developer can then validate the new test method that detects the behavioral change.
Following the test selection, the new test method passes on the pre-commit version but fails on the post-commit version.
The current amplified test method cannot be added to the test suite, since it fails.
However, this test method is still useful, since one has only to negate the failing assertions, \eg change an \texttt{assertTrue} into an \texttt{assertFalse}, to obtain a valid and passing test method that explicitly executes the new behavior.
This can be done manually or automatically with approaches such as ReAssert\cite{ReAssert}.

\DCI has been designed to be easy to use.
The only cost of \DCI is the time to set it up: in the ideal, happy-path case, it is meant to be a single command line through Maven goals.
Once \DCI is set up in continuous integration, it automatically runs at each commit and developers directly benefit from amplified test methods that strengthen the existing test suite.

%
%   Behavioral Change
%
\subsection{Behavioral Change}
\label{subsec:dci:background:behavioral-change}

A \emph{behavioral change} is a source-code modification that triggers a new state for some inputs \cite{saff2004experimental}.
Considering the pre-commit version $P$ and the post-commit version $P'$ of a program, the commit introduces a behavioral change if it is possible to implement a test method that can trigger and observe the change, \ie, it passes on $P$ and fails on $P'$, or the opposite.
In short, the behavioral change must have an impact on the observable behavior of the program.

%
%   Behavioral Change Detection
%
\subsection{Behavioral Change Detection}
\label{subsec:dci:background:behavioral-change-detection}

\emph{Behavioral change detection} is the task of identifying a behavioral change between two versions of the same program.
In this chapter, I propose a novel approach to detect behavioral changes based on test amplification.

% -----------------------
%  CONTRIBUTIONS
% -----------------------

\section{Behavioral Change Detection Approach}
\label{sec:dci:techniques}

\subsection{Overview of \DCI}
\label{sec:dci:techniques:overview}

\DCI takes as input a program, its test suite, and a commit modifying the program.
The commit, as done in version control systems, is basically the diff between two consecutive versions of the program.

% output
\DCI outputs new test methods that detect the behavioral difference between the pre- and post-commit versions of the program.
The new tests pass on a given version, but fail on the other, demonstrating the presence of a behavioral change captured.

\DCI computes the code coverage of the diff and selects test methods accordingly.
Then it applies \dspot to amplify selected test methods.
The resulting amplified test methods detect the behavioral change.

\autoref{fig:global_approach} sums up the different phases of the approach:
1) Compute the diff coverage and select the tests to be amplified;
2) Amplify the selected tests based on the pre-commit version;
3) Execute amplified test methods against the post-commit version, and keep the failing test methods.
This process produces test methods that pass on the pre-commit version, fail on the post-commit version, hence they detect at least one behavioral change introduced by a given commit.

\begin{figure}
    \caption{Overview of the approach to detect behavioral changes in commits.}
	\label{fig:global_approach}
    \fbox{\includegraphics[width=.95\linewidth]{img/global_flow.pdf}}
\end{figure}

\subsection{Test Selection and Diff Coverage}
\label{subsec:compute_diff_coverage}
\DCI implements a feature that:
\begin{enumerate*}
\item reports the diff coverage of a commit, and
\item selects the set of unit tests that execute the diff.
\end{enumerate*}
%
To do so, \DCI first computes the code coverage for the whole test suite.
Second, it identifies the test methods that hit the statements modified by the diff. 
Third, it produces the two outcomes elicited earlier: the diff coverage, computed as the ratio of statements in the diff covered by the test suite over the total number of statements in the diff and the list of test methods that cover the diff.
%
Then, it selects only test methods that are present in pre-commit version (\ie, it ignores the test methods added in the commit, if any).
The final list of test methods that cover the diff is then used to seed the amplification process.

\subsection{Test Amplification}
\label{sec:dci:techniques:amplification}

Once \DCI have the initial tests that cover the diff, \DCI amplifies them using \dspot.
Since \DCI uses \dspot, \DCI have also two mode:
1)\DCIA that uses only \Aampl and
2)\DCII that uses both \Aampl and \Iampl.

\subsection{Execution and Change Detection}
\label{sec:dci:techniques:execution-change-detection}

The final step performed by \DCI consists in checking whether that the amplified test methods detect behavioral changes.
Because \DCI amplifies test methods using the pre-commit version, all amplified test methods pass on this version, by construction. 
Consequently, for the last step, \DCI runs the amplified test methods only on the post-commit version. 
Every test that fails is in fact detecting a behavioral change introduced by the commit, and is a success.
\DCI keeps the tests that successfully detect behavioral changes.
Note that if the amplified test method is not executable on the post-commit version, \eg the API has been modified, the amplified test method is discarded.

\subsection{Implementation}
\label{sec:dci:techniques:implementation}

\DCI is implemented in Java and is built on top of the OpenClover and Gumtree~\cite{falleri:hal-01054552} libraries.
It computes the global coverage of the test suite with OpenClover, which instruments and executes the test suite.
Then, it uses Gumtree to have an AST representation of the diff.
\DCI matches the diff with the test that executes those lines. 
Through its Maven plugin, \DCI can be seamlessly implemented into continuous integration.
\DCI is publicly available on \gh.\footnote{\url{https://github.com/STAMP-project/dspot/tree/master/dspot-diff-test-selection}}

\section{Evaluation}
\label{sec:dci:evaluation}

The evaluation of \DCI relies on 4 research questions:

\newcommand{\rqdetection}{\RQ{1}{To what extent are \DCIA and \DCII able to produce amplified test methods that detect the behavioral changes?}}
\newcommand{\rqiteration}{\RQ{2}{What is the impact of the number of iteration performed by \DCII?}}
\newcommand{\rqselection}{\RQ{3}{What is the effectiveness of our test selection method?}}
\newcommand{\rqhuman}{\RQ{4}{How do human and generated tests that detect behavioral changes differ?}}

\subsection{Research Questions}
\label{subsec:dci:evaluation:research-questions}

\noindent
\rqdetection\\
\rqiteration\\
\rqselection\\
\rqhuman\\

\subsection{Benchmark}
\label{subsec:dci:evaluation:benchmark}
To the best of my knowledge, there is no benchmark of commits in Java with behavioral changes in the literature. 
Consequently, I devise a project and commit selection procedure in order to construct a benchmark for the evaluation.

\paragraph{Project selection}
The evaluation needs software projects that are

1) publicly-available;

2) written in Java;

3) and use continuous integration.

The projects has been selected from the dataset in \cite{descartes} and \cite{Danglot2019}, which is composed of mature Java projects from \gh.

\paragraph{Commit selection}
%To select commits, we apply the following procedure.
Commits has been taken in inverse chronological order, from newest to oldest.
%this result in commits buildable and analyzable with the current version of build and test tools;
I select the first ten commits that match the following criteria:

1) the commit modifies Java files (most behavioral changes are source code changes.
It is known that behavioral changes can be introduced in other ways, such as modifying dependencies or configuration files \cite{Test:Coverage:Evolution}.
However, such modifications are not the target of \DCI.

2) the commit provides or modifies a manually written test that detects a behavioral change. 
To verify this property, I execute the test on the pre-commit version. 
If it fails, it means that the test detects at least 1 behavioral change.
This test will be used as a \textit{ground-truth test} in \textbf{RQ4}.

3) the changes of the commit must be covered by the pre-commit test suite.
To do so, I compute the diff coverage. 
If the coverage is 0\%, the commit is discarded. 
This is done because if the change is not covered, any test methods cannot be selected to be amplified, which is what a part of the evaluation.

Together, these criteria ensure that all selected commits:

1) introduce behavioral changes;

2) at least one test method can be used as ground-truth since it detects a behavioral change;

3) at least one test method executes the diff and can be used to seed the amplification process;

4) there is no structural change in the commit between both versions, \eg no change in method signature and deletion of classes (this is ensured since the pre-commit test suite compiles and runs against the post-commit version of the program and vice-versa).

\paragraph{Final benchmark}
\begin{table}[h]
	\centering
	\scriptsize
    \rowcolors{2}{white}{gray!25}
	\def\arraystretch{1}
	\setlength\tabcolsep{4pt}
	\caption{Selected projects and commits dataset.}
	\begin{tabular}{lcrrcccc}
		\toprule
		project &
		LOC &
		\begin{tabular}{c}start\\date\end{tabular}&
		\begin{tabular}{c}end\\date\end{tabular}&
		\begin{tabular}{c}\#total\\commits\end{tabular}&
		\begin{tabular}{c}\#discarded\\commits\end{tabular}&
		\begin{tabular}{c}\#matching\\commits\end{tabular}&
		\begin{tabular}{c}\#selected\\commits\end{tabular}\\
		\midrule
		\scriptsize{commons-io}	&	59607	&	9/10/2015	&	9/29/2018	&	385	&	375	&	16(4.16\%)	&	10	\\
		\scriptsize{commons-lang}	&	77410	&	11/22/2017	&	10/9/2018	&	227	&	217	&	13(5.73\%)	&	10	\\
		\scriptsize{gson}	&	49766	&	6/14/2016	&	10/9/2018	&	159	&	149	&	13(8.18\%)	&	10	\\
		\scriptsize{jsoup}	&	20088	&	12/21/2017	&	10/10/2018	&	50	&	40	&	11(22.00\%)	&	10	\\
		\scriptsize{mustache.java}	&	10289	&	7/6/2016	&	04/18/2019	&	68	&	58	&	11(16.18\%)	&	10	\\
		\scriptsize{xwiki-commons}	&	87289	&	10/31/2017	&	9/29/2018	&	687	&	677	&	23(3.35\%)	&	10	\\
		\midrule
		\scriptsize{summary}	&	304449	&	9/10/2015	&	04/18/2019	&	avg(262.67)	&	avg(252.67)	&	avg(14.50(9.93\%))	&	60	\\
		\bottomrule
	\end{tabular}
	\label{tab:benchmark}
\end{table}

\autoref{tab:benchmark} shows the main descriptive statistics on the benchmark dataset.
The first column is the name of the considered project;
The second column is the number of lines of code computed with cloc;
The third column is the date of the oldest commit for the project;
The fourth column is the date of the newest commit for the project;
The fifth, sixth and seventh are respectively the total number of commit we analyze, the total number of commits discarded, the number of commits that match all the inclusion criteria but the third (there is no test in the pre-commit that execute the change), and the number of commit selected.
The last row reports a summary of the benchmark with the total number of lines of code, the oldest and the newest dates, the average number of commits analyzed, the average number of commits discarded, the average number of commits matching all the criteria but the third.
Note that the benchmark is only composed of recent commits from notable open-source projects and is available on \gh at \url{https://github.com/STAMP-project/dspot-experiments}.

\subsection{Protocol}
\label{subsec:dci:evaluation:protocol}

To answer \textbf{RQ1}, \DCIA and \DCII is executed on the benchmark projects.
The total number of behavioral changes successfully detected by \DCI is reported.
That is to say the number of commits for which \DCI generates at least 1 test method that passes on the pre-commit version but fails on the post-commit version.
Also, 1 case study of a successful behavioral change detection is discussed.

To answer \textbf{RQ2}, \DCII with 1, 2 and 3 iterations is executed on the benchmark projects.
The number of behavioral changes successfully detected for each number of iterations in the main loop is reported.
Also, the number amplified test methods that detect the behavioral changes for each commit for 10 different seeds to study the impact of the randomness on the output of \dspot is reported.
A Kruskal-Wallis test statistic is performed on these numbers.

To answer \textbf{RQ2}, we run \DCII for 1, 2 and 3 iterations on the benchmark projects.
The number of behavioral changes successfully detected for each number of iterations in the main loop is reported.
I analyze the number of amplified test methods that detect the behavioral changes for each commit for 10 different seeds in addition to the reference run with default seed, totalling 11 runs.
The objective here is to study the impact of the randomness on the output of \DCII using 1 iteration.
Since each experiment takes very long to run, I choose to use only 1 iteration for each seed.
Doing these 10 different executions using 3 iterations would result with an execution that would last almost 1000 hours of cpu-time, which is infeasible.
I compute the confidence interval on the number of successes, \ie the number of time \DCI generates at least one amplified test method that detects the behavioral change, in order to measure the uncertainty of the result.
To do this, I use Python libraries \emph{scipy} and \emph{numpy}, and consider a confidence level of 95\%. 
Per this open-science approach, the interested reader has access to both the raw data and the script computing the confidence interval. \footnote{\url{https://github.com/STAMP-project/dspot-experiments/tree/master/src/main/python/april-2019}}

For \textbf{RQ3}, the test selection method is considered effective if the tests selected to be amplified semantically relate to the code changed by the commit. 
To assess this, 1 commit per project in the benchmark is selected.
Then the automatically selected tests for this commit is manually analyzed to tell whether there are semantically related to the behavioral changes in the commit. 

To answer \textbf{RQ4}, the ground-truth tests written or modified by developers in the selected commits is used.
This ground-truth test method is compared to the amplified test methods that detect behavioral changes, for 1 commit per project.

\subsection{Results}
\label{subsec:result}

\input{chapitres/behavioral-change-detection-for-commit/table_content}

The overall results are reported in \autoref{tab:overall_result}.
This table can be read as follow:
the first column is the name of the project;
the second column is the shortened commit id;
the third column is the commit date;
the fourth column column is the total number of test methods executed when building that version of the project;
the fifth and sixth columns are respectively the number of tests modified or added by the commit, and the size of the diff in terms of line additions (in green) and deletions (in red);
the seventh and eighth columns are respectively the diff coverage and the number of tests DCI selected;
the ninth column provides the amplification results for \DCIA, and it is either a \cmark with the number of amplified tests that detect a behavioral change or a \textit{-} if DCI did not succeed in generating a test that detects a change;
the tenth column displays the time spent on the amplification phase;
The eleventh and the twelfth are respectively a \cmark with the number of amplified tests for \DCII  (or - if a change is not detected) for 3 iterations.
The last row reports the total over the 6 projects.
For the tenth and the twelfth columns of the last row, the first number is the number of successes, \ie the number of times \DCI produced at least one amplified test method that detects the behavioral change, for  \DCIA and \DCII respectively.
The numbers between brackets correspond to the total number of amplified test methods that \DCI produces in each mode.

\subsubsection{Characteristics of commits with behavioral changes in the context of continuous integration}
\label{subsubsec:dci:evaluation:characteristics}

This section describes the characteristics of commits introducing behavioral changes in the context of continuous integration.
The number of test methods at the time of the commit shows two aspects of our benchmark:
1) there are only strongly tested projects;
2) the number of tests evolve over time due to test evolution.
Every commit in the benchmark comes with test modifications (new tests or updated tests), and commit sizes are quite diverse.
The three smallest commits are \textsc{commons-io\#703228a}, \textsc{gson\#44cad04} and \textsc{jsoup\#e5210d1} with 6 modifications, and the largest is \textsc{Gson\#45511fd} with 334 modifications.
%
Finally, on average, commits have 66.11\% coverage.
The distribution of diff coverage is reported graphically by \autoref{fig:histdiffcoverage}: 
in commons-io all selected commits have more than 75\% coverage.
In XWiki-Commons, only 50\% of commits have more than 75\% coverage. 
Overall, 31 / 60 commits have at least 75\% of the changed lines covered.
This validates the correct implementation of the selection criteria that ensures the presence of a test specifying the behavioral change.

\begin{figure}
	\centering
	\caption{Distribution of diff coverage per project of our benchmark.}
	\label{fig:histdiffcoverage}
	\includegraphics[width=.95\linewidth]{img/diff_cov_hist.png}
\end{figure}

Thanks to the selection criteria, 
a curated benchmark of 50 commits with a behavioral change is available for the evaluation.
This benchmark comes from notable open-source projects, and covers a diversity of commit sizes. 
The benchmark is publicly available and documented for future research on this topic.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% RESULTS RQ1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\rqdetection}
\label{subsubsec:dci:evaluation:rq1}

The last 4 columns of \autoref{tab:overall_result} are dedicated to \textbf{RQ$_1$}.
For example, \DCIA and \DCII generated respectively 1 and 39 amplified test methods that detect the behavioral change for \textsc{commons-io\#f00d97a} (4$^{th}$ row).
In the other hands, only \DCII has been able to obtain amplified test methods for \textsc{commons-io\#81210eb} (8$^{th}$ row).

Overall, \DCIA generates amplified tests that detect 9 out of 60 behavioral changes.
Meanwhile, \DCII generates amplified tests that detect 25 out of 60 behavioral changes.

Regarding the number of generated tests.
\DCII generates a large number of test methods, compared to \DCIA only (15 versus 6708, see column ``total'' at the bottom of the table). 
Both \DCIA and \DCII can generate amplified tests, however since \DCIA does not produce a large amount of test methods the developers do not have to triage a large set of test cases.
Also, since \DCIA only adds assertions, the amplified tests are easier to understand than the ones generated by \DCII.

% time
\DCII takes more time than \DCIA (for successful cases 38.7 seconds versus 3.3 hours on average).
The difference comes from the time consumed during the exploration of the input space in the case of \DCII, while \DCIA focuses on the amplification of assertions only, which represents a much smaller space of solutions. 

Overall, \DCI successfully generates amplified tests that detect a behavioral change in 46\% of the commits in our benchmark(25 out of 60).
Recall that the 60 commits analyzed are real changes in complex code bases.
They represent modifications, sometimes deep in the code, that are challenges with respect to testability~\cite{voas1995software}.
Consequently, the fact \DCI generates test cases that detect behavioral changes, is considered an achievement.
The commits for which \DCI fails to detect the change can be considered as a target for future research on this topic.

A successful detection by an amplified test method is analyzed.
Commit \textsc{3fadfdd}\footnote{\url{https://github.com/apache/commons-lang/commit/3fadfdd}} from commons-lang has been selected because it is succinct enough to be discussed.
The diff is shown in \autoref{fig:diff_commons_lang_success}.

\begin{lstlisting}[language=diff,caption=Diff of commit \textsc{3fadfdd} from commons-lang.,label=fig:diff_commons_lang_success]
@@ -2619,7 +2619,7 @@ protected void appendFieldStart(final StringBuffer buffer, final String fieldNam

-       super.appendFieldStart(buffer, FIELD_NAME_QUOTE + fieldName
+       super.appendFieldStart(buffer, FIELD_NAME_QUOTE +
+           StringEscapeUtils.escapeJson(fieldName) + FIELD_NAME_QUOTE);
}
\end{lstlisting}

The developer added a method call to a method that escapes specials characters in a string. 
The changes come with a new test method that specifies the new behavior.

\DCI starts the amplification from the \texttt{testNestingPerson} test method defined in \texttt{JsonToStringStyleTest} showed in \autoref{fig:selected_diff_commons_lang_success}. 

\begin{lstlisting}[float,language=java,caption=Selected test method as a seed to be amplified for commit \textsc{3fadfdd} from commons-lang.,label=fig:selected_diff_commons_lang_success]
@Test
public void testPerson() {
	final Person p = new Person();
	p.name = "Jane Doe";
	p.age = 25;
	p.smoker = true;

	assertEquals(
		"{\"name\":\"Jane Doe\",\"age\":25,\"smoker\":true}",
		new ToStringBuilder(p).append("name", p.name)
		.append("age", p.age).append("smoker", p.smoker)
		.toString()
	);
}
\end{lstlisting}

The test is selected for amplification because it triggers the execution of the changed line.

\begin{lstlisting}[float,language=java,caption=Test generated by DCI that detects the behavioral change of \textsc{3fadfdd} from commons-lang.,label=fig:amplified_commons_lang_success]
@Test(timeout = 10000)
public void testPerson_literalMutationString85602() throws Exception {
	final ToStringStyleTest.Person p = new ToStringStyleTest.Person();
	p.name = "Jane Doe";
	Assert.assertEquals("Jane Doe", p.name);
	p.age = 25;
	p.smoker = true;
	String o_testPerson_literalMutationString85602__6 = 
		new ToStringBuilder(p)
			.append("n/me", p.name)
			.append("age", p.age)
			.append("smoker", p.smoker)
			.toString();
	Assert.assertEquals(
		"{\"n/me\":\"Jane Doe\",\"age\":25,\"smoker\":true}",
		o_testPerson_literalMutationString85602__6
	);
	Assert.assertEquals("Jane Doe", p.name);
}
\end{lstlisting}

The resulting amplified test method is shown in \autoref{fig:amplified_commons_lang_success}.
From this test method, \DCI generates an amplified test method shown in \autoref{fig:amplified_commons_lang_success}. 
In this generated test, \sbampl applies 2 input transformations: 1 duplication of method call and 1 character replacement in an existing String literal.
The latter transformation is the key transformation: \DCI replaced an 's' inside "person" by '/' resulting in "per/on" where "/" is a special character that must be escaped (Line 2). 
Then, \DCI generated 11 assertions, based on the modified inputs. 
The amplified test the behavioral change:
in the pre-commit version, the expected value is: \texttt{"\{ ... per/on":\{"name":"Jane Doe" ...\}"} while in the post-commit version it is \texttt{"\{ ... per\textbackslash/on":\{"name":"Jane Doe" ...\}"} (Line 3).
 
\begin{mdframed}
\textbf{RQ1}: Overall, \DCI detects the behavioral changes in a total of 25/60 commits. 
Individually, \DCII finds changes in 25/60, while \DCIA in 9/60 commits.
Since \DCII also uses \aampl to generate assertions, all \DCIA's commits are contained in \DCII's. 
However, the search-based algorithm, through exploration, finds many more behavioral changes, making it more effective albeit at the cost of execution time.
\end{mdframed}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% RESULTS RQ2 : ITERATION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{\rqiteration}
\label{subsubsec:dci:evaluation:rq2}

The results are reported in \autoref{tab:overall_result_iteration}
This table can be read as follow:
the first column is the name of the project;
the second column is the commit identifier;
then, the third, fourth, fifth, sixth, seventh and eighth provide the amplification results and execution time for each number of iteration 1, 2, and 3. 
A \cmark indicates with the number of amplified tests that detect a behavioral change and a \textit{-} denotes that DCI did not succeed in generating a test that detects a change.
The last row reports the total over the 6 projects.
For the third, fifth and the seventh columns of the last row, the first number is the number of successes, \ie the number of times that \DCI produced at least one amplified test method that detect the behavioral change, for respectively$iteration=1$, $iteration=2$ and $iteration=3$.
The numbers in parentheses are the total number of amplified test methods that \DCI produces with each number of iteration.

Overall, \DCII generates amplified tests that detect 23, 24, and 25 out of 60 behavioral changes for respectively $iteration=1$, $iteration=2$ and $iteration=3$.
The more iteration \DCII does, the more it explores, the more it generates amplified tests that detect the behavioral changes but the more it takes time also.
When \DCII is used with $iteration=3$, it generates amplified test methods that detect 2 more behavioral changes than when it is used with $iteration=1$ and 1 more than when it is used with $iteration=2$.

In average, \DCII generates 18, 53, and 116 amplified tests for respectively $iteration=1$, $iteration=2$ and $iteration=3$. 
This number increases by 544\% from $iteration=1$ to $iteration=3$.
This increase is explained by the fact that \DCII explores more with more iteration and thus is able to generate more amplified test methods that detect the behavioral changes.

In average \DCII takes 23, 64, and 105 minutes to perform the amplification for respectively $iteration=1$, $iteration=2$ and $iteration=3$.
This number increases by 356\% from $iteration=1$ to $iteration=3$.

\input{chapitres/behavioral-change-detection-for-commit/table_content_iterations}

\paragraph{Impact of the randomness}

The number of amplified test methods obtained by the different seeds are reported in \autoref{tab:overall_result_seeds}.

\begin{table*}
	\caption{Number of successes, \ie \DCI produced at least one amplified test method that detects the behavioral changes, for 11 different seeds.}
	\centering
	\label{tab:overall_result_seeds}
	\begin{tabular}{lccccccccccc}
		\toprule
		Seed	&	ref	&	1	&	2	&	3	&	4	&	5	&	6	&	7	&	8	&	9\\
		\midrule
		\rowcolor{gray!25}
		\#Success	&	23	&	18	&	17	&	17	&	17	&	19	&	21	&	18	&	21	&	18\\
		\bottomrule
	\end{tabular}
\end{table*}

This table can be read as follow:
the first column is the id of the commit.
the second column is the result obtained with the default seed, used during the evaluation for \textbf{RQ$_1$}.
the ten following columns are the results obtained for the 10 different seeds.

%('19.00', '20.34', '17.66', '1.34')
The computed confidence interval is $\left[20.34, 17.66\right]$
It means that, from our samples, with probability 0.95, the real value of the number of successes lies in this interval.

%recap RQ2
\begin{mdframed}
Answer to \textbf{RQ2}: \DCII detects  23, 24, and 25 behavioral changes out of 60 for respectively $iteration=1$, $iteration=2$ and $iteration=3$.
The number of iteration done by \DCII impacts the number of behavioral changes detected, the number of amplified test methods obtained and the execution time.
\end{mdframed}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% RESULTS RQ3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{\rqselection}
\label{subsubsec:dci:evaluation:rq3}

To answer \textbf{RQ3}, there is no quantitative approach to take, because there is no ground-truth data or metrics to optimize. 
Per the protocol described in \autoref{subsec:dci:evaluation:protocol}, the answer to this question is based on manual analysis:
1 commit per project is randomly selected.
Then the relevance of the selected tests for amplification is analyzed.

Following an example, in order to give an intuition of what are the characteristics of the test selection for amplification to be relevant.
The selection is considered relevant If \texttt{TestX} is selected for amplification, following a change to method \texttt{X}.
The key is that \DCI will generate an amplified test \texttt{TestX'} that is a variant of \texttt{TestX}, and, consequently, the developer will directly get the intention of the new test \texttt{TestX'} and what behavioral change it detects.

\textsc{Commons-io\#c6b8a38}\footnote{\url{https://github.com/apache/commons-io/commit/c6b8a38}}: the test selection returns 3 test methods: \texttt{testContentEquals}, \texttt{testCopyURLToFileWithTimeout} and \texttt{testCopyURLToFile} from the same test class: \texttt{FileUtilsTestCase}.
The considered commit modifies the method \texttt{copyToFile} from \texttt{FileUtils}.
There is a link between the changed file and the intention of 2 out 3 tests to be amplified.
The selection is thus considered relevant.

\textsc{Commons-lang\#f56931c}\footnote{\url{https://github.com/apache/commons-lang/commit/f56931c}}: the test selection returns 39 test methods from 5 test classes: \texttt{FastDateFormat\_ParserTest}, \texttt{FastDateParserTest}, \texttt{DateUtilsTest}, \texttt{FastDateParser\_TimeZoneStrategyTest} and \texttt{FastDateParser\_MoreOrLessTest}.
This commit modifies the behavior of two methods: \texttt{simpleQuote} and \texttt{setCalendar} of class \texttt{FastDateParser}.
When manually analyzed, it reveals two intentions:
1) test behaviors related to parsing;
1) test behaviors related to dates.
While this is meaningful, a set of 39 methods is clearly not a focused selection, not as focused as for the previous example.
The selection can be considered relevant, but not focused.

\textsc{Gson\#9e6f2ba}\footnote{\url{https://github.com/google/gson/commit/9e6f2ba}}: the test selection returns 9 test methods from 5 different test classes.
3 out of those 5 classes \texttt{JsonElementReaderTest}, \texttt{JsonReaderPathTest} and \texttt{JsonParserTest} relate to the class modified in the commit(\texttt{JsonTreeReader}).
The selection is thus considered relevant but unfocused.

\textsc{Jsoup\#e9feec9}\footnote{\url{https://github.com/jhy/jsoup/commit/e9feec9}}, the test selection returns the 4 test methods defined in the \texttt{XmlTreeBuilderTest} class \texttt{caseSensitiveDeclaration}, \texttt{handlesXmlDeclarationAsDeclaration}, \texttt{testDetectCharsetEncodingDeclaration} and \texttt{testParseDeclarationAttributes}.
The commit modifies the behavior of the class \texttt{XmlTreeBuilder}.
Here, the test selection is relevant.
Actually, the ground-truth manual test added in the commit is also in the \texttt{XmlTreeBuilderTest} class.
If \DCI proposes a new test there to capture the behavioral change, the developer will understand its relevance and its relation to the change.

\textsc{Mustache.java\#88718bc}\footnote{\url{https://github.com/spullara/mustache.java/commit/88718bc}}, the test selection returns the \texttt{testInvalidDelimiters} test method defined in the \texttt{com.github.mustachejava.InterpreterTest} test class.
The commit improves an error message when an invalid delimiter is used.
Here, the test selection is relevant since it selected \texttt{testInvalidDelimiters} which is the dedicated test to the usage of the test invalid delimiters.
This ground-truth test method is also in the test class \texttt{com.github.mustachejava.InterpreterTest}.

\textsc{Xwiki-commons\#848c984}\footnote{\url{https://github.com/xwiki/xwiki-commons/commit/848c984}} the test selection returns a single test method \texttt{createReference} from test class \texttt{XWikiDocumentTest}.
The main modification of this commit is on class \texttt{XWikiDocument}.
Since \texttt{XWikiDocumentTest} is the test class dedicated to \texttt{XWikiDocument}, the selection is considered relevant.

\begin{mdframed}[nobreak=true]
Answer to \textbf{RQ3}: 
In 4 out of 6 of the manually analyzed cases, the tests selected to be amplified relate, semantically, to the modified application code. 
In the 2 remaining cases, it selected over and above the tests to be amplified.
That is, it selects tests whose intention is semantically pertinent to the change, but it also includes tests that are not.
However, even in this case, \DCI's test selection provides developers with important and targeted context to better understand the behavioral change at hand.
\end{mdframed}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% RESULTS RQ4
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\rqhuman}
\label{subsubsec:dci:evaluation:rq4}

When \DCI generates an amplified test method that detects the behavioral change, it can be compared to the ground truth version (the test added in the commit) to see whether it captures the same behavioral change.
For each project, I select 1 successful application of \DCI, and compare the \DCI test against the human test.
If they capture the same behavioral change, it means they have the same intention and the amplification is considered as a success.

%%% COMMONS IO

\textsc{commons-io\#81210eb}\footnote{\url{https://github.com/apache/commons-io/commit/81210eb}}: This commit modifies the behavior of the \texttt{read()} method in \texttt{BoundedReader}.
\autoref{fig:ampl_commons-io} shows the test generated by \DCII.
This test is amplified from the existing \texttt{readMulti} test, which indicates that the intention is to test the read functionality.
The first line of the test is the construction of a \texttt{BoundedReader} object which is also the class modified by the commit.
\DCII modified the second parameter of the constructor call (transformed $3$ into a $0$) and generated two assertions (only 1 is shown).
The first assertion, associated to the new test input, captures the behavioral difference.
Overall, this can be considered as a successful amplification.

\begin{lstlisting}[float,language=java,caption=Test generated by \DCII that detects the behavioral change introduced by commit \textsc{81210eb} in commons-io.,label=fig:ampl_commons-io]
@Test(timeout = 10000)
public void readMulti_literalMutationNumber3() {
	BoundedReader mr = new BoundedReader(sr, 0);
	char[] cbuf = new char[4];
	for (int i = 0; i < (cbuf.length); i++) {
		cbuf[i] = 'X';
	}
	final int read = mr.read(cbuf, 0, 4);
	Assert.assertEquals(0, ((int) (read)));
}        
\end{lstlisting}

Now, look at the human test contained in the commit, shown in \autoref{fig:diff_commons-io}.
It captures the behavioral change with the timeout (the test timeouts on the pre-commit version and goes fast enough on the post-commit version). 
Furthermore, it only indirectly calls the changed method through a call to \texttt{readLine}.

In this case, the \DCI test can be considered better than the developer test because
1) it relies on assertions and not on timeouts, and
2) it directly calls the changed method (\texttt{read}) instead of indirectly. 

\begin{lstlisting}[float,language=java,caption=Developer test for commit \textsc{81210eb} of commons-io.,label=fig:diff_commons-io]
@Test(timeout = 5000)
public void testReadBytesEOF() {
	BoundedReader mr = new BoundedReader( sr, 3 );
	BufferedReader br = new BufferedReader( mr );
	br.readLine();
	br.readLine();
}
\end{lstlisting}

%%% COMMONS LANG
\textsc{commons-lang\#e7d16c2}\footnote{\url{https://github.com/apache/commons-lang/commit/e7d16c2}}: this commit escapes special characters before adding them to a \texttt{StringBuffer}.
\autoref{fig:ampl_commons-lang} shows the amplified test method obtained by \DCII.
The assertion at the bottom of the excerpt is the one that detects the behavioral change.
This assertion compares the content of the \texttt{StringBuilder} against an expected string.
In the pre-commit version, no special character is escaped, \eg '\textbackslash n'.
In the post-commit version, the amplified test fails since the code now escapes the special character \textbackslash.

\begin{lstlisting}[float,language=java,caption=Test generated by \DCII that detects the behavioral change of \textsc{e7d16c2} in commons-lang.,label=fig:ampl_commons-lang]
@Test(timeout = 10000)
public void testAppendSuper_literalMutationString64() {
		String o_testAppendSuper_literalMutationString64__15 = 
			new ToStringBuilder(base)
				.appendSuper((((("Integer@8888[" + (System.lineSeparator())) + "  null") 
					+ (System.lineSeparator())) + "]"))
				.append("a", "b0/|]")
				.toString();
	Assert.assertEquals("{\"a\":\"b0/|]\"}", o_testAppendSuper_literalMutationString64__15);
}
\end{lstlisting}

Let's have a look to the human test method shown in \autoref{fig:diff_commons-lang}.
Here, the developer specified the new escaping mechanism with 5 different inputs.

The main difference between the human test and the amplified test is that the human test is more readable and uses 5 different inputs.
However, the amplified test generated by \DCI is valid since it detects the behavioral change correctly.

\begin{lstlisting}[float,language=java,caption=Developer test for \textsc{e7d16c2} of commons-lang.,label=fig:diff_commons-lang]
@Test
public void testLANG1395() {
	assertEquals("{\"name\":\"value\"}",
		new ToStringBuilder(base).append("name","value").toString());
	assertEquals("{\"name\":\"\"}",
		new ToStringBuilder(base).append("name","").toString());
	assertEquals("{\"name\":\"\\\"\"}",
		new ToStringBuilder(base).append("name",'"').toString());
	assertEquals("{\"name\":\"\\\\\"}",
		new ToStringBuilder(base).append("name",'\\').toString());
	assertEquals("{\"name\":\"Let's \\\"quote\\\" this\"}",
		new ToStringBuilder(base).append("name","Let's \"quote\" this").toString());
}
\end{lstlisting}

%% GSON
\textsc{gson\#44cad04}\footnote{\url{https://github.com/google/gson/commit/44cad04}}: This commit allows Gson to deserialize a number represented as a string.
\autoref{fig:ampl_gson} shows the relevant part of the test generated by \DCII, based on \texttt{testNumberDeserialization} of \texttt{PrimitiveTest} as a seed.
The \DCI test detects the behavioral changes at lines 3 and 4.
On the pre-commit version, line 3 throws a \texttt{JsonSyntaxException}.
On the post-commit version, line 4 throws a \texttt{NumberFormatException}.
In other words, the behavioral change is detected by a different exception (different type and not thrown at the same line).
\footnote{Interestingly, the number is parsed lazily, only when needed. 
Consequently, the exception is thrown when invoking the \texttt{longValue()} method and not when invoking \texttt{parse()}}.

\begin{lstlisting}[float,language=java,caption=Test generated by DCI that detects the behavioral change of commit \textsc{44cad04} in Gson.,label=fig:ampl_gson]
public void testNumberDeserialization_literalMutationString8_failAssert0() throws Exception {
	try {
		String json = "dhs";
		actual = gson.fromJson(json, Number.class);
		actual.longValue();
		junit.framework.TestCase.fail(
			"testNumberDeserialization_literalMutationString8 should have thrown JsonSyntaxException");
	} catch (JsonSyntaxException expected) {
		TestCase.assertEquals("Expecting number, got: STRING", expected.getMessage());
	}
}
\end{lstlisting}
The amplified test methods is now compared against the developer-written ground-truth method, shown in \autoref{fig:diff_gson}. 
This short test verifies that the program handles a number-as-string correctly.
For this example, the \DCI test does indeed detect the behavioral change, but in an indirect way.
On the contrary, the developer test is shorter and directly targets the changed behavior, which is better.

\begin{lstlisting}[float,language=java,caption=Provided test by the developer for \textsc{44cad04} of Gson.,label=fig:diff_gson]
public void testNumberAsStringDeserialization() {
	Number value = gson.fromJson("\"18\"", Number.class);
	assertEquals(18, value.intValue());
}
\end{lstlisting}

%% JSOUP
\textsc{jsoup\#3676b13}\footnote{\url{https://github.com/jhy/jsoup/commit/3676b13}}: This change is a pull request (\ie a set of commits) and introduces 5 new behavioral changes.
There are two improvements: skip the first new lines in pre tags and support deflate encoding, and three bug fixes: throw exception when parsing some URLs, add spacing when output text, and no collapsing of attribute with empty values.
\autoref{fig:ampl_jsoup} shows an amplified test obtained using \DCII.
This amplified test has 15 assertions and a duplication of method call.
Thanks to this duplication and these generated assertions on the \texttt{toString()} method, this test is able to capture the behavioral change introduced by the commit.

\begin{lstlisting}[float,language=java,caption=Test generated by \DCII that detects the behavioral change of \textsc{3676b13} of Jsoup.,label=fig:ampl_jsoup]
@Test(timeout = 10000)
public void parsesBooleanAttributes_add4942() {
	String html = "<a normal=\"123\" boolean empty=\"\"></a>";
	Element el = Jsoup.parse(html).select("a").first();
	List<Attribute> attributes = el.attributes().asList();
	Attribute o_parsesBooleanAttributes_add4942__15 = 
	attributes.get(1);
	Assert.assertEquals("boolean=\"\"", 
		((BooleanAttribute) (o_parsesBooleanAttributes_add4942__15)).toString());
}
\end{lstlisting}

As before, the amplified test method is compared to the developer's test. 
The developer uses the \texttt{Element} and \texttt{outerHtml()} methods rather than \texttt{Attribute} and \texttt{toString()}.
However, the method \texttt{outerHtml()} in \texttt{Element} will call the \texttt{toString()} method of \texttt{Attribute}.
For this behavioral change, it concerns the \texttt{Attribute} and not the \texttt{Element}.
So, the amplified test is arguably better, since it is closer to the change than the developer's test.
But, \DCII generates amplified tests that detect 2 of 5 behavioral changes: adding spacing when output text and no collapsing of attribute with empty values only, so regarding the quantity of changes, the human tests are more complete.

\begin{lstlisting}[float,language=java,caption=Provided test by the developer for \textsc{3676b13} of Jsoup.,label=fig:diff_jsoup]
@Test
public void booleanAttributeOutput() {
	Document doc = Jsoup.parse("<img src=foo noshade='' nohref async=async autofocus=false>");
	Element img = doc.selectFirst("img");

	assertEquals("<img src=\"foo\" noshade nohref async autofocus=\"false\">", img.outerHtml());
}
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% MUSTACHE JAVA
\textsc{Mustache.java\#774ae7a}\footnote{\url{https://github.com/spullara/mustache.java/commit/774ae7a}}: This commit fixes an issue with the usage of a dot in a relative path on Window in the method \texttt{getReader} of class \texttt{ClasspathResolver}.
The test method \texttt{getReaderNullRootDoesNotFindFileWithAbsolutePath} has been used as seed by \DCI.
It modifies the existing string literal with another string used somewhere else in the test class and generates 3 new assertions.
The behavioral change is detected thanks to the modified strings: it produces the right test case containing a space.

\begin{lstlisting}[float,language=java,caption=Test generated by \DCII that detects the behavioral change of \textsc{774ae7a} of Mustache.java.,label=fig:ampl_mustache]
@Test(timeout = 10000)
public void getReaderNullRootDoesNotFindFileWithAbsolutePath_litStr4() {
	ClasspathResolver underTest = new ClasspathResolver();
	Reader reader = underTest.getReader(" does not exist");
	Assert.assertNull(reader);
	Matcher<Object> 
	o_getReaderNullRootDoesNotFindFileWithAbsolutePath_litStr4__5 =
		Is.is(CoreMatchers.nullValue());
	Assert.assertEquals("is null", 
		((Is) (o_getReaderNullRootDoesNotFindFileWithAbsolutePath_litStr4__5))
		.toString()
	);
	Assert.assertNull(reader);
}
\end{lstlisting}

The developer proposed two tests that verify that the object reader is not null when getting it with dots in the path.
There are shown in \autoref{fig:diff_mustache}.
These tests invoke the method \texttt{getReader} which is the modified method in the commit.
%
The difference is that the \DCII's amplified test method provides a non longer valid input for the method \texttt{getReader}.
However, providing such inputs produce errors afterward which signal the behavioral change.
In this case, the amplified test is complementary to the human test since it verifies that the wrong inputs are no longer supported and that the system immediately throws an error.

\begin{lstlisting}[float,language=java,caption=Developer test for \textsc{774ae7a} of Mustache.java.,label=fig:diff_mustache]
@Test
public void getReaderWithRootAndResourceHasDoubleDotRelativePath() throws Exception {
	ClasspathResolver underTest = new ClasspathResolver("templates");
	Reader reader = underTest.getReader("absolute/../absolute_partials_template.html");
	assertThat(reader, is(notNullValue()));
}

@Test
public void getReaderWithRootAndResourceHasDotRelativePath() throws Exception {
	ClasspathResolver underTest = new ClasspathResolver("templates");
	Reader reader = underTest.getReader("absolute/./nested_partials_sub.html");
	assertThat(reader, is(notNullValue()));
}
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% XWIKI COMMONS
\textsc{xwiki-commons\#d3101ae}\footnote{\url{https://github.com/xwiki/xwiki-commons/commit/d3101ae}}: This commit fixes a bug in the \texttt{merge} method of class \texttt{DefaultDiffManager}.
\autoref{fig:ampl_xwiki} shows the amplified test method obtained by \DCIA.
\DCI used \texttt{testMergeCharList} as a seed for the amplification process, and generates 549 new assertions.
Among them, 1 assertion captures the behavioral change between the two versions of the program: 
``assertEquals(0, result.getLog().getLogs(LogLevel.ERROR).size());''.
The behavioral change that is detected is the presence of a new logging statement in the diff. 
After verification, there is indeed such a behavioral change in the diff, with the addition of a call to ``logConflict'' in the newly handled case.

\begin{lstlisting}[float,language=java,caption=Test generated by \DCIA that detects the behavioral change of \textsc{d3101ae} of XWiki.,label=fig:ampl_xwiki]
@Test(timeout = 10000)
public void testMergeCharList() throws Exception {
	MergeResult<Character> result;
	result = this.mocker.getComponentUnderTest()
		.merge(AmplDefaultDiffManagerTest.toCharacters("a"), 
			AmplDefaultDiffManagerTest.toCharacters(""), 
			AmplDefaultDiffManagerTest.toCharacters("b"), 
			null
		);
	int o_testMergeCharList__9 = result.getLog().getLogs(LogLevel.ERROR).size();
	Assert.assertEquals(1, ((int) (o_testMergeCharList__9)));
	List<Character> o_testMergeCharList__12 = AmplDefaultDiffManagerTest.toCharacters("b");
	Assert.assertTrue(o_testMergeCharList__12.contains('b'));
	result.getMerged();
	result = this.mocker.getComponentUnderTest()
		.merge(AmplDefaultDiffManagerTest.toCharacters("bc"), 
			AmplDefaultDiffManagerTest.toCharacters("abc"), 
			AmplDefaultDiffManagerTest.toCharacters("bc"), 
			null
		);
	int o_testMergeCharList__21 = result.getLog().getLogs(LogLevel.ERROR).size();
	Assert.assertEquals(0, ((int) (o_testMergeCharList__21)));
}
\end{lstlisting}

The developer's test is shown in \autoref{fig:diff_xwiki}.
This test method directly calls method \texttt{merge}, which is the method that has been changed. 
What is striking in this test is the level of clarity: 
the variable names, the explanatory comments and even the vertical space formatting are impossible to achieve with \DCIA and makes the human test clearly of better quality but also longer to write.

Yet, \DCIA's amplified tests capture a behavioral change that was not specified in the human test.
In this case, amplified tests can be complementary.

\begin{lstlisting}[float,language=java,caption=Developer test for \textsc{d3101ae} of XWiki.,label=fig:diff_xwiki]
@Test
public void testMergeWhenUserHasChangedAllContent() throws Exception
{
	MergeResult<String> result;

	// Test 1: All content has changed between previous and current
	result = mocker.getComponentUnderTest().merge(Arrays.asList("Line 1", "Line 2", "Line 3"),
	Arrays.asList("Line 1", "Line 2 modified", "Line 3", "Line 4 Added"),
	Arrays.asList("New content", "That is completely different"), null);

	Assert.assertEquals(Arrays.asList("New content", "That is completely different"), result.getMerged());

	// Test 2: All content has been deleted 
	// between previous and current
	result = mocker.getComponentUnderTest().merge(Arrays.asList("Line 1", "Line 2", "Line 3"),
	Arrays.asList("Line 1", "Line 2 modified", "Line 3", "Line 4 Added"),
	Collections.emptyList(), null);

	Assert.assertEquals(Collections.emptyList(), result.getMerged());
}
\end{lstlisting}


\begin{mdframed}
	% commons-io is better
	% commons-lang is correct (but not as good as the human's)
	% gson is kind of valid but indirect
	% jsoup is partially better, it test better some new behaviors but not all of them // complementary
	% mustche is complementary
	% xwikiis complementary
Answer to \textbf{RQ4}: 
In 3 of 6 cases, the \DCI test is complementary to the human test.
In 1 case, the \DCI test can be considered better than the human test.
In 2 cases, the human test is better than the \DCI test.
Even though human tests can be better, \DCI can be complementary and catch missed cases, or can provide added-value when developers do not have the time to add a test.
\end{mdframed}

%\pagebreak


% -----------------------
%  Main Observed Limitation
% -----------------------
\section{Discussion about the scope of DCI}
\label{sec:limitation}

In this section, we overview the current scope of DCI and the key challenges that limit DCI.

\textbf{Focused applicability}
From the benchmark, \DCI is applicable to limited proportion of commits: 9.93\% of the commits analyzed  on average.
This low proportion is the first limit of \DCI usage.
However, Once \DCI is setup, there is no manual overhead.
Even if \DCI is not used at each commit, it costs nothing more.

\textbf{Adoption}
The evaluation showed that \DCI is able to obtain amplified test methods that detect a behavioral change.
But, it does not provide any evidence on the fact that developers would exploit such test method.
However, from the previous chapter \autoref{chap:test-improvement}, software developers value the amplified test methods.
This provides strong evidence on the potential adoption of \DCI.

\textbf{Performance}
From our experiments, we see that the time  to complete the amplification is the main limitation of DCI. For example \DCI took almost 5 hours on
\textsc{jsoup\#2c4e79b},  with no result.
For the sake of our experimentation, we choose to use a pre-defined number of iteration to bound the exploration.
In practice, we recommend to set a time budget (\eg at most one hour per pull-request).

\textbf{Importance of test seeds}
By construction, DCI's effectiveness is correlated to the test methods used as seed.
For example, see the row of \texttt{commons-lang\#c8e61af} in \autoref{tab:overall_result_iteration}, where one can observe that whatever the number of iteration, DCI takes the same time to complete the amplification.
The reason is that the seed tests are only composed of assertions statements.
Such tests are bad seeds for DCI, and they prevent any good input amplification.
Also, \DCI requires to have at least one test method that executes the code changes.
If the project is poorly tested and does not have any test method that execute the code changes, \DCI cannot be applied.

\textbf{False positives}
The risk of false positives is a potential  limitation of our approach.
A false positive would be an amplified test method that passes or fails on both versions, which means that the amplified test method does not detect the behavioral difference between both versions.
I manually analyzed 6 commits and none of them are false positives.
This increases our confidence that DCI produces a limited number of such confusing test methods.

% -----------------------
%  THREATS
% -----------------------
\section{Threats to validity}
\label{sec:dci:threats}

% bug in the implementation
An internal threat is the potential bugs in the implementation of \DCI.
However, it is heavily tested, with \junit test suite to mitigate this threat.

% benchmark
In the benchmark, there are 60 commits. 
The result may be not be generalizable to all programs. 
But real and diverse applications from \gh have been carefully selected, all having a strong test suite. 
The benchmark reflects real programs, and provides an high confidence in the result.

% interval confidence
The experiments are stochastic, and randomness is a threat accordingly.
To mitigate this threat, I have computed a confidence interval that estimates the number of successes that \DCI would obtain.

\section{Conclusion}
\label{sec:dci:conclusion}

This chapter presented the evaluation of \DCI, which aim at setting up \dspot inside the CI.
The goal of \DCI is to produce automatically test methods that detect a behavioral change, \ie a behavioral difference between two versions of the same program.
This is done by selecting test methods that execute the changes, then amplify them with \dspot.
In addition to this, \dspot keeps amplified test methods that detect the behavioral change, approximate by the fact that the amplified test methods pass on the pre-changes version but fail on the post-change version of the program.

\DCI can be integrated to the continuous integration to achieve two major tasks:

1) \DCI can improve the regression detection ability of the test suite with respect to a changes.
When the behavioral changes highlighted by the test suite is not desired, it means that the developers introduced a regression or a new bug inside the program.
Using these amplified test methods, the developers can identify and fix the problem faster than without it.
Thus it prevents the merge of hidden bug that could cost a lot of money if users face it when using the application.

2) \DCI can help the developers to make evolve the test suite by providing amplified test methods that detect the behavioral change.
When this behavioral change is the one desired, the developers just need to negate, manually or using automatic approach such as ReAssert~\cite{ReAssert}, the assertions.
The developer will obtain amplified test methods that strengthen the test suite according to her change.

To evaluate \DCI, I selected 60 commits that introduce behavioral changes from 6 open-source projects, from past-evaluation~\cite{descartes,Danglot2018}.
Then, I executed \DCI on these 60 commits and observe whether or not \DCI can generate amplified test methods that detect the behavioral changes.
For these 60 commits, \DCI has been able to detect 25 behavioral change.
To the best of my knowledge, this is the first benchmark of real behavioral changes  from open-source projects.
This evaluation showed that \DCI is able to generate amplified test methods to detect real behavioral changes, introduced by commits.
This means that \DCI can be used in an industrial context since the selected changes are complex and required deep knowledge of the application, and the selected application are wide-spread and used programs across the open-source community.

In the next chapter, I expose 3 transverval contributions of this thesis:

First, the study of the correctness of program under runtime perturbation:
In the previous chapter \autoref{chap:dspot} and this chapter, I evaluated \dspot in two different contexts: offline amplification and amplification in the CI.
\dspot generates amplified test methods, using the state of the program as oracle to build assertions.
It means that the current behavior of the program is considered correct by \dspot.
What does correct mean? In the first transversal contribution, I studied the correctness program.
In particular, how do programs behave under run-time perturbation?

Second, the study of pseudo-tested methods;
Pseudo-tested methods are source methods that when the body is removed, the whole test suite pass, while some test methods are executing this methods.
In \autoref{chap:test-improvement}, I use \ms to measure test suite's quality.
The detection of pseudo-tested methods can be done using \ms and more particularly specifically designed mutants.

And Third, the study of test generation for automatic repair.
Automatic repair aims at fixing automatically bugs.
One family of automatic repair is the test suite based.
This family uses the test suite as oracle to know whether or not the bug have been fix.
One can use test generation techniques to enhances these automatic repair techniques and see it as a test amplification process.