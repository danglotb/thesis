\chapter{Introduction}
\label{chap:introduction}

\minitoc

\graphicspath{{.}{chapitres/introduction/}}

Nowadays software is omnipresent in people's life: banking, e-commerce, communication, etc. 
Critical aspects such as aeronautics, voting system or even health care rely on software.

In one of his famous lectures~\cite{DijkstraLecture1989}, Dijkstra has stated that in software: 
\begin{center}
	\emph{``the smallest possible perturbations - \ie changes of a single bit - can have the most drastic consequences.''.}
\end{center}

Dijkstra highlights the fact that a small fault in software might affect lives.
For example, a flight crash happened in 1993 due to an error in the flight-control software of the Swedish JAS 39 Gripen fighter aircraft\footnote{see: \url{https://www.flightglobal.com/FlightPDFArchive/1989/1989\%20-\%200734.PDF}}.

To avoid such situations, software engineers adopted testing philosophies: model-checking~\cite{10.1007/978-3-540-28644-8_3}, proof and code verification~\cite{4544862}, automatic testing.
The two former are out of the scope of this thesis, I focus on the latter: the tester writes code to verify that the program is doing what the developer expects.
Over the last decade, strong unit testing has become an essential component of any serious software project, whether in industry or academia~\cite{BellerTSE,beller2015when,beller2015howmuch}.
The agile development movement has contributed to this cultural change with the global dissemination of test-driven development techniques~\cite{beck2003test}.
More recently, the DevOps movement has further strengthened the testing practice with an emphasis on continuous and automated testing~\cite{Roche2013Devops}.

However, testing is tedious and costly for industry: it is hard to evaluate return on investment.
Thus, developers under pressure or by lack of discipline or time might skip writing the tests.

To overcome this problem, research investigates the automation of creating strong tests.
Automatic generation of tests has been well studied in the last past years~\cite{ESECFSE11, PachecoE2005}.
The dream was that a command-line would give you a complete test suite, that verifies the whole program.

Researchers investigated the usage of such generated test suite by the industry~\cite{TOSEM_userstudy}.
Even if automatically generated test suites achieve high coverage, generated test suites do not seem to really help developers.
The difficulties to understand generated test suites~\cite{TOSEM_userstudy} can be the obstacles on the adoption of such techniques by the industry.
Also, most of the tools rely on weak or partial oracles, \eg absence of run-time errors, which limits their ability to find bugs.

In this thesis, I aim at addressing the lack of a tool that assists developers in regression testing.
More precisely, the ultimate goal of this thesis is to provide a tool that supports developers in terms of maintenance of their test suite.

To do so, I use test suite amplification.
Test suite amplification consists of exploiting the existing manually-written tests, treated as a seed, to achieve a given engineering goal.
This seed test method can be seed as a starting point for the test amplification.
Test amplification would use all the information carried by this seed test method to perform a given task.
For example, a test amplification approach would create variants of this seed test method in order to execute new path in the program, that is to say increasing the code coverage of the test suite.
Test suite amplification includes analysis and operation on existing test suites.

In this thesis, I define precisely test suite amplification to leverage this lexical ambiguity and I devise a technique that performs code insertions, deletions or modifications on the seed test method.
By construction, test suite amplification's output is close to the test methods used as seed since they share a common-part.
It means that for developers that know very-well their software and their test suite, it is easier to understand amplified test methods than test methods that have been generated from scratch without any connection with the human-test methods.

For example, in the context of collaborative project such as projects on \gh, one developer fixes a bug.
If the developer does not provide a test method that exposes the bug fixing, \ie that fails on the version of the program before the fix but passes on the fixed program, the patch might be removed without noticing it.
Every changes should come with a test method that characterizes and specifies the changes.
For this second objective, the tool would improve the test suite ``online'', \ie inside the continuous integration service.
Each time a developer makes changes, the tool would provide automatically a test method that specifies these changes inside the continuous integration, without any human intervention but the validation of the amplified test method.

The remaining of this chapter is as follow:

I expose the global vision  of this thesis in \autoref{sec:intro:scientific-problem} and its outline in \autoref{sec:intro:contributions};
Then in \autoref{sec:intro:stamp} present the context of the thesis, the H2020 European project \emph{STAMP};
Eventually, I list the resulting software of this thesis in \autoref{sec:intro:software} and my publications in \autoref{sec:intro:publications}.

\section{Scientific Problem}
\label{sec:intro:scientific-problem}

Due the to the democratization of DevOps and agile development methodology, developing teams are committing changes in parallel on the same code base.
This way of development brings the advantage to reach the market earlier and adapt the product quickly.
However, it has also a major weakness: each change is a potential threat on the correctness of the program.
When a developer modify the program code, he might also introduce a bug and/or break an existing feature, \ie add a regression.
This can prevent the adoption of the product by customer if clients encounter the bug before any developers.
Continuous Integration (CI) mitigates these threats by testing the application often, \eg at each changes, in order to prevent errors in the final product.
But the effectiveness of the CI relies on effectiveness of others components such as test suite.
If the test suite is poor quality, \eg it covers a small part of the program, the CI might not be able to catch errors.
Since developers may cut corners when writing test, \textbf{would we be able to provide them a tool to carry automatically the important responsibility that is testing the application?}

First, would this tool be able to improve the test suite in an offline fashion? 
That is to say, during development, the engineer could use this tool in order to increase the overall quality of the test suite with respect to a given test-criterion, such as \ms.
The \ms emulates faults that a developer is susceptible to do.
In essence, it injects small and artificial behavioral changes that aims at measuring the test suite's capacity to detect them.
The challenge is to detect mutants that test methods written by humans do not kill.
Moreover, this must be done on an arbitrary programs, using as seed arbitrary test methods.
Potentially, the remainder undetected mutants are the most difficult to detect.

Second, could this tool  be a part of the continuous integration?
That is to say, the tool would automatically run each time that the CI is triggered in order to increase the overall quality of the test suite with respect to the change.
In this scenario, the behavioral changes is real, complex and larger than a mutant in \ms parlance.
The challenge is to detect a complex behavioral change the might required a specific knowledge of the program.
Thus, the new behavior might require to set the program into a very specific and unknown state, that even the developer is not aware of.
In the context of regression testing, the behavioral change, \ie the hidden regression is unknown and might be very difficult to reach.
In this thesis, I aim to compete these challenges.

\section{Thesis Contributions}
\label{sec:intro:contributions}

This thesis makes 5 contributions:
\begin{itemize}
    \item The definition of ``test amplification'' and a survey of the literature in \autoref{chap:sota}.
    the challenge is that in the litterature, ``test amplification'' is referred by a various lexicon which makes difficult to understand all the works that use test amplification.
    Note that this chapter has been published as journal paper~\cite{DANGLOT2019110398}.
    \item The definition of the test suite amplification algorithm and its implementation called \dspot in \autoref{chap:dspot}.
    The challenge is to design a new algorithm that explores the very large space over the variations of the original methods and to build an oracle.
    Over the three years of the thesis I have built a tool that addresses these challenges.
    The tool has been constantly tested and evaluated by industry partners, which are respectable and big companies.
    This continuous quality feedback has ensured the development of a sound, efficient and useful tool for both researchers and developers.
    \item The evaluation of \dspot's ability to improve the quality of test suites in \autoref{chap:test-improvement}.
    The challenge is to produce amplified test methods that improve the quality of test suites.
    To do this, I used as study subjects open-source projects that have high quality test suites.
    \item The evaluation of \dspot's ability to generate amplified tests inside CI in \autoref{chap:dci}.
    The challenge is to produce amplified test methods that detect a behavioral difference between two versions of the same programs.
    For this, I used real commits that change the program's behavior from open-source projects.
\end{itemize}

In \autoref{chap:transversal-contributions}, I expose 3 side contributions made during the thesis.
These contributions have been done thanks to the expertise that I acquired during my thesis.
However, the reader can skip this chapter because it is additional materials.

\section{STAMP-project}
\label{sec:intro:stamp}

My thesis takes place within the STAMP project, which is has been funded from the European Union's H2020 research and innovation programme under the grant agreement 731529.

STAMP stands for \textbf{S}oftware \textbf{T}esting \textbf{AMP}lification.
STAMP leverages advanced research in test generation and innovative methods of test amplification to push automation in DevOps one step further.
The main goal of STAMP techniques are to reduce the number and cost of regression bugs at unit level, configuration level and production stage, by acting at all level of the development cycle.

The project gathers four academic partners with strong software testing expertise, five software companies (in: e-Health, Content Management, Smart Cities and Public Administration), and an open source consortium. 

This industry-near research addresses concrete, business-oriented objectives. Thank to STAMP, I could met developers from the industry which are experts in their domain.
Thus, It gave me access to case studies that are representative of industrial software.

\section{Publications}
\label{sec:intro:publications}

In this section, I list my publications. 
There are 2 lists: my main contributions and the transversal contributions in which I participated.

These publications have been published in 2 journals:
\begin{itemize}
	\item Empirical Software Engineering is a journal for applied software engineering research articles with a strong empirical evaluation. Empirical studies in this journal involve large amount of data and an analysis that can be used to evaluate new software practices and technologies.
	\item Journal of Systems and Software publishes papers on all software engineering and hardware-software-systems issues. These articles must show evidence of their claims, with validations like empirical studies, simulation, etc.
\end{itemize}

\paragraph{Main contributions:}
~\\
\renewcommand\bibname{~}
\vspace{-2.8cm}
\let\oldaddcontentsline\addcontentsline% Store \addcontentsline
\renewcommand{\addcontentsline}[3]{}% Make \addcontentsline a no-op
\begingroup
\let\clearpage\relax
\begin{thebibliography}{xxxx}
	
	\bibitem[Danglot~2019b]{DANGLOT2019110398}
	Benjamin Danglot, Oscar Vera-Perez, Zhongxing Yu, Andy Zaidman, Martin
	Monperrus and Benoit Baudry.
	\newblock {\em A Snowballing Literature Study on Test Amplification}.
	\newblock Journal of Systems and Software, page 110398, 2019.
	
	\bibitem[Danglot~2019]{Danglot2019}
	Benjamin Danglot, Oscar~Luis Vera-P{\'e}rez, Benoit Baudry and Martin
	Monperrus.
	\newblock {\em Automatic test improvement with DSpot: a study with ten mature
		open-source projects}.
	\newblock Empirical Software Engineering, Apr 2019.
	
	\bibitem[Danglot~2019a]{DBLP:journals/corr/abs-1902-08482}
	Benjamin Danglot, Martin Monperrus, Walter Rudametkin and Benoit Baudry.
	\newblock {\em An Approach and Benchmark to Detect Behavioral Changes of
		Commits in Continuous Integration}.
	\newblock CoRR, vol.~abs/1902.08482, 2019.
	
\end{thebibliography}
\endgroup
\let\addcontentsline\oldaddcontentsline% Restore \addcontentsline

\paragraph{Transversal contributions:}
~\\
\renewcommand\bibname{~}
\vspace{-2.8cm}
\let\oldaddcontentsline\addcontentsline% Store \addcontentsline
\renewcommand{\addcontentsline}[3]{}% Make \addcontentsline a no-op
\begingroup
\let\clearpage\relax
\begin{thebibliography}{xxxx}
	
	\bibitem[Danglot~2018]{Danglot2018}
	Benjamin Danglot, Philippe Preux, Benoit Baudry and Martin Monperrus.
	\newblock {\em Correctness attraction: a study of stability of software
		behavior under runtime perturbation}.
	\newblock Empirical Software Engineering, vol.~23, no.~4, pages 2086--2119, Aug
	2018.
	
	\bibitem[Vera-P{\'e}rez~2018]{descartes}
	Oscar~Luis Vera-P{\'e}rez, Benjamin Danglot, Martin Monperrus and Benoit
	Baudry.
	\newblock {\em A comprehensive study of pseudo-tested methods}.
	\newblock Empirical Software Engineering, Sep 2018.
	
	\bibitem[Yu~2019]{Yu2019}
	Zhongxing Yu, Matias Martinez, Benjamin Danglot, Thomas Durieux and Martin
	Monperrus.
	\newblock {\em Alleviating patch overfitting with automatic test generation: a
		study of feasibility and effectiveness for the Nopol repair system}.
	\newblock Empirical Software Engineering, vol.~24, no.~1, pages 33--67, Feb
	2019.
	
\end{thebibliography}
\endgroup
\let\addcontentsline\oldaddcontentsline% Restore \addcontentsline

\section{Software Developed During This Thesis}
\label{sec:intro:software}

During my thesis, I developed artifacts to evaluate the approaches.
These artifacts are strong enough to be applicable on real code base such open-source projects from \gh or the STAMP partners' codebases.
This shows strong evidence on the applicability and generalization of the results.
Following, the list of these artifacts and a small description, all of them are open-source and available on \gh.

\begin{itemize}
	\item[ ] \textbf{\dspot} is a test suite amplifier. 
	It takes as input a project and its test suite and will produce test cases according to a test criterion adequacy such as branch coverage or mutation score. The algorithm implemented by this software is detailed in \autoref{chap:dspot}.
	URL:  \url{https://github.com/STAMP-project/dspot.git}.
	\item[ ] \textbf{DSpot-diff-test-selection} is a maven plugin, based on OpenClover, that produces the list of test classes and their test methods that execute a provided diff. This software allows us to integrate \dspot in the CI. Its evaluation is exposed in \autoref{chap:dci}.
	URL: \url{https://github.com/STAMP-project/dspot/tree/master/dspot-diff-test-selection}.
	\item [ ] \textbf{Test-runner} is a library that allows developer to execute test in a new and clean JVM.
	From my experience, most of research prototypes reinvent the wheel by implementing such library in their own code base.
	This library provides a generic API to execute tests without any conflict of dependencies, allowing to research prototype to be executed on real programs.
	This library provides also a way to compute the instruction coverage by the test suite.
	URL: \url{https://github.com/STAMP-project/test-runner}.
\end{itemize}