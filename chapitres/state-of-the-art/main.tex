\chapter{State of the Art}
\label{chap:sota}

\begin{chaptersummary}
	In this chapter, I exposed a systematic review of the literature on the field of test suite amplification.
	I surveyed works that exploit this knowledge to enhance manually written tests with respect to an engineering goal (\eg improve coverage or refine fault localization).
	This chapter provides the following contributions:
	\begin{itemize}
		\item The first ever snowballing literature review on test amplification
		\item The classification of the related work into four main categories to help newcomers in the field  (students, industry practitioners)  understand this body of work.
		\item A discussion about the outstanding research challenges of test amplification.
	\end{itemize}
	Note that this chapter is a to be published article\cite{survey:amplification}.
	The remainder of this chapter is as follows:
	This chapter is structured according to the 4 main categories, each of them being presented in a dedicated section.
	Section \ref{sec:amp_add} presents techniques that synthesize new tests from manually-written tests.
	Section \ref{sec:amp_change} focuses on the works that synthesize new tests dedicated to a specific change in the application code (in particular a specific commit).
	Section \ref{sec:amp_exec} discusses the less-researched, yet powerful idea of modifying the execution of manually-written tests. 
	Section \ref{sec:amp_mod} is about the modification of existing tests to improve a specific property.
\end{chaptersummary}

\minitoc

\graphicspath{{.}{chapitres/state-of-the-art/}}

\section{Introduction}
\label{sec:sota:intro}

Software testing is the art of evaluating an attribute or capability of a program to determine that it meets its required results \cite{hetzel1988}. 

With the advent of agile development methodologies, which advocate testing early and often, a growing number of software projects develop and maintain a test suite~\cite{Madeyski2010}. 
Those test suites are often large and have been written thanks to a lot of human intelligence and domain knowledge~\cite{azaidmanEMSE2011,DBLP:conf/icst/ZaidmanRDD08}. 
Developers spend a lot of time in writing the tests~\cite{BellerTSE,beller2015when,beller2015howmuch}, so that those tests exercise interesting cases (including corner cases), and so that an oracle verifies as much as possible the program behavior~\cite{hilton2018coverageevolution}.

The wide presence of valuable manually written tests has triggered a new thread of research that consists of leveraging the value of existing manually-written tests to achieve a specific engineering goal.
This has been coined ``test amplification''. 
The term \emph{amplification} is introduced as an umbrella for the various activities that analyze and operate on existing test suites and that are referred to as augmentation, optimization, enrichment, or refactoring in the literature. 

This chapter surveys the research literature so that existing research efforts are characterized, can be compared and new research opportunities can be identified.
Furthermore, the conjecture is that with good foundations and maturation, test amplification has the potential to bring software testing to the next level in terms of efficiency and efficacy among practitioners by introducing new automatic processes that improve the manually written tests.

The reviewing methodology is based on backward- and forward- snowballing on the citation graph \cite{jalali2012systematic}.
To the best of my knowledge, this review is the first that draws a comprehensive picture of the different engineering techniques and goals proposed in the literature for test amplification.

\section{Method}
\label{sec:sota:method}
This section presents the methodology of the systematic literature review.

\subsection{Definition}
\label{subsec:sota:method:defition}
Test amplification is defined as follow:

\begin{mdframed}
	\textbf{Definition}: Test amplification consists of exploiting the knowledge of a large number of test methods, in which developers embed meaningful input data and expected properties in the form of oracles, in order to enhance these manually written tests with respect to an engineering goal (\eg improve coverage of changes or increase the accuracy of fault localization).
\end{mdframed}

\emph{Example:} 
A form of test amplification is the addition of test methods automatically generated from the existing manual test methods to increase the coverage of a test suite over the main source code.

\emph{Relation to related work:} 
Test amplification is complementary, yet, significantly different from most works on test generation.
The key difference is what is given as input to the system.
Most test generation tools take as input:
the program under test or a formal specification of the testing property.
\textbf{In contrast, test amplification is defined as taking as primary input test cases written by developers}. 


\subsection{Methodology}
\label{subsec:sota:method:methodology}

Literature studies typically rigorously follow a methodology to ensure both completeness and replication. 
Cooper's book is taken as reference for a general methodological discussion on literature studies~\cite{cooper1998synthesizing}. 
Specifically for the field of software engineering, well-known methodologies are systematic literature reviews (SLR)~\cite{kitchenham2004procedures}, systematic mapping studies (SMS)~\cite{petersen2008systematic} and snowballing studies~\cite{wohlin2014guidelines}.
For the specific area of \emph{test amplification}, there is no consensus on the terminology used in literature. 
This is an obstacle to using the SLR and SMS methodologies, which both heavily rely on searching~\cite{Brereton2007}. 
As snowballing studies are less subject to suffering from the use of diverse terminologies, this study is performed per Wohlin's guidelines \cite{wohlin2014guidelines,jalali2012systematic}.

First, I run the search engine of DBLP for all papers containing ``test'' and ``amplification'' in their title (using stemming, which means that ``amplifying'' is matched as well).
This has resulted in 70 papers at the date of the search (March 27, 2018)\footnote{the data is available at \url{https://github.com/STAMP-project/docs-forum/blob/master/scientific-data/}}.
Each of papers has been reviewed one by one to see whether they fit in our scope according to the definition of \autoref{sec:core-definition}. 
This has resulted in 4 articles~\cite{HamletV93,zhang2012,leung12,Joshi07}, which are the seed papers of this literature study. 
The reason behind this very low proportion (4/70) is that most articles in this DBLP search are in the hardware research community, and hence do not fall in the scope of our paper.

Following a breve description of these 4 seed papers:
\begin{itemize}
	\item \cite{HamletV93} Hamlet and Voas introduce study how different testing planning strategies can amplify testability properties of a software system.
	\item \cite{zhang2012} Zhang and Elbaum explore a new technique to amplify a test suite for finding bugs in exception handling code. Amplification consists in triggering unexpected exceptions in sequences of API calls.
	\item \cite{leung12} Leung et al propose to modify the test execution by using information gathered from a first test execution. The information is used to derive a formal model used to detect data races in later executions.
	\item \cite{Joshi07} Joshi et al  try to amplify the effectiveness of testing by executing both concretely and symbolically the tests.
\end{itemize}
More details are given in the following sections.

From the seed papers, a backward snowballing search \rev{step} \cite{jalali2012systematic} has been performed, \ie, I have looked at all their references, going backward in the citation graph. 
2 of the authors have reviewed the papers, independently. 
%Then, these 2 authors cross-checked the outcome of their literature review, and kept each paper for which they both \rev{agreed that it} fits the definition of test amplification (cf. \autoref{sec:core-definition}).
Then, a forward literature search \rev{step} has been performed, using the Google scholar search engine and ``cited by'' filter, from the set of papers, in order to find the most recent contributions in this area.
A backward snowballing search step and a forward snowballing search step constitute what is called an ``iteration''.
With each iteration, a set of papers is selected for the study, obtained through the snowballing action.
These iterations continue until this set of selected paper is empty, \ie, when no paper can be kept, the snowballing process is stopped in both ways: backward and forward.

%% Categorization methodology
Once the selection papers is done, 4 key approaches to amplification has been distinguish, which used to classify the literature : 
Amplification by Adding New Tests as Variants of Existing Ones (\autoref{sec:amp_add});
Amplification by Modifying Test Execution (\autoref{sec:amp_exec});
Amplification by Synthesizing New Tests with Respect to \rev{Changes (\autoref{sec:amp_change})}; 
Amplification by Modifying Existing Test Code (\autoref{sec:amp_mod}).
The missing terminological consensus mentioned previously prevented the design of a classification according to Petersen's guidelines
\cite{petersen2008systematic}.
The four categories has been incrementally refined by analyzing the techniques and goals in each paper.
The methodology is as follows: a work is assigned to a category if the key technique of the paper corresponds to it.
If no category captures the gist of the paper, a new category is created.
Two categories that are found to be closely related are merged to create a new one.
The incremental refinement of these findings led to the definition of 4 categories to organize this literature study.

\subsection{Novelty}

\todo{this section will be removed}

There are a number of notable surveys in software testing \cite{edvardsson1999survey,mcminn2004search,anand2013orchestrated}. 
However none of them is dedicated to test amplification.
For instance, Edvardsson's et al's \cite{edvardsson1999survey} and McMinn et al's \cite{mcminn2004search} articles are surveys on test generation.
Yoo and Harman have structured the work on test minimization, selection and prioritization \cite{yoo2012survey} .
In the prolific literature on symbolic execution for testing, the reader can refer to the survey of 
Păsăreanu and Visser \cite{puasuareanu2009survey}.

In general, test optimization, test selection, test prioritization, test minimization, test reduction is out of the scope of test amplification.

Similarly, the work on test refactoring is related, but not in scope. 
In particular, the work from Van Deursen \etal\cite{vandeursen2001refactoring,DBLP:series/springer/MoonenDZB08} and Mesaros\cite{Meszaros2006} focuses on improving the structural and diagnosability qualities of software tests, and is a mainly manual activity. 
In contrast, test amplification is meant to be fully automated, as other technical amplification such as sound amplification. 
Its goal is also different, in that its aim is to test more effectively with regard to a a given target criterion.

Harrold et \etal\cite{harrold2008retesting} discusses the problem of ``retesting software'', where there is a section related to amplification.
However, it is only a light account on the topic which is now outdated.

Yusifoğlu et al. \cite{GAROUSIYUSIFOGLU2015123} discuss the new trends in software test-code engineering, and discuss the implications for researchers and practitioners in this area. 
To do this, they use a systematic mapping to identify areas that require more attention.
Their work covers a larger scope than our work, since they study all software test-code engineering research, methods and empirical study, while we focus specifically on test amplification, with more depth.

\section{Amplification by Adding New Tests as Variants of Existing Ones}
\label{sec:sota:category-1}

The most intuitive form of test amplification is to consider an existing test suite, then generate variants of the existing test cases and add those new variants into the original test suite. 
This kind of test amplification is denoted as $AMP_{add}$.

\medskip
\textbf{Definition: A test amplification technique $AMP_{add}$ consists of creating new tests from existing ones to achieve a given engineering goal.
	The most commonly used engineering goal is to improve coverage according to a coverage criterion.}

The works listed in this section fall into this category and have been divided according to their main engineering goal.

\subsection{Example}
\label{subsec:sota:category-1:example}

In this section I present an example of $AMP_{add}$ to illustrate this category of work.
Let us consider the single Java method, presented in \autoref{lst:example}.

\begin{lstlisting}[caption={Example of a toy method},label=lst:example,float,language=java,numbers=left]
class Computer {
	public void compute(int integer) {
		if (integer > 2) {
			return integer + 2;
		} else {
			return integer + 1;
		}
	}
}
\end{lstlisting}

This method contains an if statement. 
The conditional expression tests the value passed through the parameter. 
If the value is greater than 2, then the method returns the value plus 2, otherwise it returns the value plus 1.
Applying $AMP_{add}$ requires to have existing tests. 
Consider the test method in \autoref{lst:example_test_method}.
This test method ensures the behavior of the program when the parameter is lower than 2, \ie when the else branch of the if statement is executed.

\begin{lstlisting}[caption={Example of toy test method},label=lst:example_test_method,float,language=java,numbers=left] 
@Test
public void test_compute() {
	Computer computer = new Computer();
	int actualValue = computer.compute(1);
	assertEquals(2, actualValue);
}
\end{lstlisting}

According to this test, one can say that this program is ``poorly'' tested, since only one of the two branches is covered.
One potential goal of an $AMP_{add}$ technique is to increase this branch coverage. 

\begin{lstlisting}[caption={Example of amplified toy test method},label=lst:example_test_method_amplified,float,language=java,numbers=left] 
@Test
public void amplified_test_compute() {
	Computer computer = new Computer();
	int actualValue = computer.compute(3);
	assertEquals(5, actualValue);
}
\end{lstlisting}

Now, an $AMP_{add}$ technique may be able to generate the amplified test method shown in \autoref{lst:example_test_method_amplified}.
The test \autoref{lst:example_test_method_amplified} is easily derivable from the existing test \autoref{lst:example_test_method} because only one literal and the assertion differ.
This new test method executes the \textit{then} branch of the if statement (see \autoref{lst:example} line 2 and 3) that was not executed before. 
That is to say, applying $AMP_{add}$ improves the test suite, by increasing the branch coverage of the program.

\subsection{Coverage or Mutation Score Improvement}
\label{subsec:sota:category-1:coverage-vs-mutation}

Baudry \ie \cite{Baudry05a} \cite{Baudry05d} improve the mutation score of an existing test suite by generating variants of existing tests through the application of specific transformations of the test cases. 
They iteratively run these transformations, and propose an adaptation of genetic algorithms (GA), called a bacteriological algorithm (BA), to guide the search for test cases that kill more mutants.  
The results demonstrate the ability of search-based amplification to significantly increase the mutation score of a test suite.
They evaluated their approach on 2 case studies that are .NET classes.
The evaluation shows promising results, however the result have little external validity since only 2 classes are considered.

Tillmann and Schulte \cite{tillmann2006unit} describe a technique that can generalize existing unit tests into parameterized unit tests. 
The basic idea behind this technique is to refactor the unit test by replacing the concrete values that appear in the body of the test with parameters, which is achieved through symbolic execution. 
Their technique's evaluation has been conducted on 5 .NET classes.

The problem of generalizing unit tests into parameterized unit tests is also studied by Thummalapenta et \etal\cite{marri2010retrofitting}. 
Their empirical study shows that unit test generalization can be achieved with feasible effort, and can bring the benefits of additional code coverage.
They evaluated their approach on 3 applications from 1 600 to 6 200 lines of code. 
The result shows an increase of the branch coverage and a slight increase of the bug detection capability of the test suite.

To improve the cost efficiency of the test generation process, Yoo and Harman \cite{yoo2012} propose a technique for augmenting the input space coverage of the existing tests with new tests. 
The technique is based on four transformations on numerical values in test cases, \ie shifting ($\lambda x.x+1$ and  $\lambda x.x-1$ ) and data scaling (multiply or divide the value by 2).
In addition, they employ a hill-climbing algorithm based on the number of fitness function evaluations, where a fitness is the computation of the euclidean distance between two input points in a numerical space. 
The empirical evaluation shows that the technique can achieve better coverage than some test generation methods which generate tests from scratch.
The approach has been evaluated  on the triangle problem.
They also evaluated their approach on two specific methods from two large and complex libraries.

To maximize code coverage, Bloem \etal \cite{6958388} propose an approach that alters existing tests to get new tests that enter new terrain, \ie uncovered features of the program.
The approach first analyzes the coverage of existing tests, and then selects all test cases that pass a yet uncovered branch in the target function.
Finally, the approach investigates the path conditions of the selected test cases one by one to get a new test that covers a previously uncovered branch. 
To vary path conditions of existing tests, the approach uses symbolic execution and model checking techniques. 
A case study has shown that the approach can achieve 100\% branch coverage fully automatically.
They first evaluate their prototype implementation on two open source examples and then present a case study on a real industrial program of a Java Card applet firewall.
For the real program, they applied their tool on 211 test cases, and produce 37 test cases to increase the code coverage.
The diversity of the benchmark allows to make a first generalization.

Rojas et al.~\cite{rojas2016seeding} have investigated several seeding strategies for the test generation tool Evosuite. 
Traditionally, Evosuite generates unit test cases from scratch.
In this context, seeding consists in feeding Evosuite with initial material from which the automatic generation process can start. 
The authors evaluate different sources for the seed: constants in the program, dynamic values, concrete types and existing test cases. 
In the latter case, seeding analogizes to amplification. 
The experiments with 28 projects from the Apache Commons repository show a 2\% improvement of code coverage, on average, compared to a generation from scratch.
The evaluation based on Apache artifacts is stronger than most related work, because Apache artifacts are known to be complex and well tested.

Patrick and Jia \cite{Patrick201736} propose \emph{Kernel Density Adaptive Random Testing} (KD-ART) to improve the effectiveness of random testing.
This technique takes advantage of run-time test execution information to generate new test inputs. 
It first applies \emph{Adaptive Random Testing} (ART) to generate diverse values uniformly distributed over the input space. 
Then, they use \emph{Kernel Density Estimation} for estimating the distribution of values found to be useful; in this case, that increases the mutation score of the test suite. 
KD-ART can intensify the existing values by generating inputs close to the ones observed to be more useful or diversify the current inputs by using the ART approach. 
The authors explore the trade-offs between diversification and intensification in a benchmark of eight C programs. 
They achieve an 8.5\% higher mutation score than ART for programs that have simple numeric input parameters, but their approach does not show a significant increase for programs with composite inputs. 
The technique is able to detect mutants 15.4 times faster than ART in average.

Instead of operating at the granularity of complete test cases, Yoshida et \etal\cite{Yoshida2016} propose a novel technique for automated and fine-grained incremental generation of unit tests through minimal augmentation  of an existing test suite. 
Their tool, \emph{FSX}, treats each part of existing cases, including the test driver, test input data, and oracles, as “test intelligence", and attempts to create tests for uncovered test targets by copying and minimally modifying existing tests wherever possible. 
To achieve this, the technique uses iterative, incremental refinement of test-drivers and symbolic execution.
They evaluated \emph{FSX} using four benchmarks, from 5K to 40K lines of code. 
This evaluation is adequate and reveals that FSX' result can be generalized.

\subsection{Fault Detection Capability Improvement}
\label{subsec:sota:category-1:fault-detection}

Starting with the source code of test cases, Harder et \etal\cite{Harder03} propose an approach that dynamically generates new test cases with good fault detection ability.
A generated test case is kept only if it adds new information to the specification.
They define ``new information'' as adding new data for mining invariants with Daikon, hence producing new or modified invariants. 
What is unique in the paper is the augmentation criterion: helping an invariant inference technique.
They evaluated Daikon on a benchmark of 8 C programs. 
These programs vary from 200 to 10K line of code. 
It is left to future work to evaluate the approach on a real and large software application.

Pezze et \etal\cite{pezze2013} observe that method calls are used as the atoms to construct test cases for both unit and integration testing, and that most of the code in integration test cases
appears in the same or similar form in unit test cases. 
Based on this observation, they propose an approach which uses the information provided in unit test cases about object creation and initialization to build composite cases that focus on testing the interactions between objects. 
The evaluation results show that the approach can reveal new interaction faults even in well tested applications. 

Writing web tests manually is time consuming, but it gives the developers the advantage of gaining domain knowledge. 
In contrast, most web test generation techniques are automated and systematic, but lack the domain knowledge required to be as effective. 
In light of this, Milani \etal\cite{milani2014} propose an approach which combines the advantages of the two. 
The approach first extracts knowledge such as event sequences and assertions from the human-written tests, and then combines the knowledge with the power of automated crawling. 
It has been shown that the approach can effectively improve the fault detection rate of the original test suite.
They conducted an empirical evaluation on 4 open-source and large JavaScript systems. 

\subsection{Oracle Improvement}
\label{subsec:sota:category-1:oracle-improvement}

Pacheco and Ernst implement a tool called Eclat \cite{Pacheco2005}, which aims to help the tester with the difficult task of creating effective new test inputs with constructed oracles. 
Eclat first uses the execution of some available correct runs to infer an operational model of the software's operation. 
By making use of the established operational model, Eclat then employs a classification-guided technique to generate new test inputs. 
Next, Eclat reduces the number of generated inputs by selecting only those that are most likely to reveal faults. 
Finally, Eclat adds an oracle for each remaining test input from the operational model automatically. 
They evaluated their approach on 6 small programs. 
They compared Eclat's result to the result of JCrasher, a state of the art tool that has the same goal than Eclat. 
In their experimentation, they report that Eclat perform better than JCrasher: Eclat reveals 1.1 faults on average against 0.02 for JCrasher.

Given that some test generation techniques just generate sequences of method calls but do not contain oracles for these method calls, Fraser and Zeller \cite{fraser2011generating} propose an approach to generate parametrized unit tests containing symbolic pre- and post-conditions. 
Taking concrete inputs and results as inputs, the technique uses test generation and mutation to systematically generalize pre- and post-conditions. 
Evaluation results on five open source libraries show that the approach can successfully generalize a concrete test to a parameterized unit test, which is more general and expressive, needs fewer computation steps, and achieves a higher code coverage than the original concrete test.
They used 5 open-source and large programs to evaluate the approach.
According to their observation, this technique is more expensive than simply generating unit test cases.

\subsection{Debugging Effectiveness Improvement}
\label{subsec:sota:category-1:debugging-improvement}

Baudry \etal \cite{Baudry:2006:ITS:1134285.1134299} propose the test-for-diagnosis criterion (TfD) to evaluate the fault localization power of a test suite, and identify an attribute called Dynamic Basic Block (DBB) to characterize this criterion.
A Dynamic Basic Block (DBB) contains the set of statements that are executed by the same test cases, which implies all statements in the same DBB are indistinguishable.
Using an existing test suite as a starting point, they apply a search-based algorithm to optimize the test suite with new tests so that the test-for-diagnosis criterion can be satisfied. 
They evaluated their approach on two programs: a toy program and a server that simulates business meetings over the network. 
These two programs are less than 2K line of code long, which can be considered as small.

R{\"o}$\beta$ler \etal\cite{robetaler2012isolating} propose BugEx, which leverages test case generation to systematically isolate failure causes. 
The approach takes a single failing test as input and starts generating additional passing or failing tests that are similar to the failing test. 
Then, the approach runs these tests and captures the differences between these runs in terms of the observed facts that are likely related with the pass/fail outcome. 
Finally, these differences are statistically ranked and a ranked list of facts is produced.
In addition, more test cases are further generated to confirm or refute the relevance of a fact. 
It has been shown that for six out of seven real-life bugs, the approach can accurately pinpoint important failure explaining facts.
To evaluate BugEx, they use 7 real-life case studies from 68 to 62K lines of code. The small number of considered bugs, 7, calls for more research to improve external validity.

Yu \etal\cite{Yu2013} aim at enhancing fault localization under the scenario where no appropriate test suite is available to localize the encountered fault. 
They propose a mutation-oriented test case augmentation technique that is capable of generating test suites with better fault localization capabilities. 
The technique uses some mutation operators to iteratively mutate some existing failing tests to derive new test cases potentially useful to localize the specific encountered fault. 
Similarly, to increase the chance of executing the specific path during crash reproduction, Xuan \etal\cite{Xuan:2015:CRV:2786805.2803206} propose an approach based on test case mutation. 
The approach first selects relevant test cases based on the stack trace in the crash, followed by eliminating assertions in the selected test cases, and finally uses a set of predefined mutation operators to produce new test cases that can help to reproduce the crash. 
They evaluated MuCrash on 12 bugs for Apache Commons Collections, which is  26 KLoC of source code and 29 KLoC of test code length. 
The used program is quite large and open-source which increases the confidence. but using a single subject is a threat to generalization.

\subsection{Summary}
\label{subsec:sota:category-1:subsection}

\emph{Main achievements:}
The works discussed in this section show that adding new test cases based on existing ones can make the test generation process more targeted and cost-effective. 
On the one hand, the test generation process can be geared towards achieving a specific engineering goal better based on how existing tests perform with respect to the goal. 
For instance, new tests can be intentionally generated to cover those program elements that are not covered by existing tests. 
Indeed, it has been shown that tests generated in this way are effective in achieving multiple engineering goals, such as improving code coverage, fault detection ability, and debugging effectiveness. 
On the other hand, new test cases can be generated more cost-effectively by making use of the structure or components of the existing test cases. 

\emph{Main Challenges:}
While existing tests provide a good starting point, there are some difficulties in how to make better use of the information they contain.
First, the number of new tests synthesized from existing ones can sometimes be large and hence an effective strategy should be used to select tests that help to achieve the specific engineering goal;
the concerned works are: \cite{Baudry05a, Baudry05d, Yoshida2016}.
Second, the synthesized tests have been applied to a specific set of programs and the generalization of the related approaches could be limited. 
The concerned works are: \cite{tillmann2006unit, marri2010retrofitting, yoo2012, 6958388, Patrick201736, Harder03, Pacheco2005, Baudry:2006:ITS:1134285.1134299, robetaler2012isolating, Xuan:2015:CRV:2786805.2803206}.
Third, some techniques have known performance issues and do not scale well: \cite{milani2014, fraser2011generating}.

\section{Amplification by Synthesizing New Tests with Respect to Changes}
\label{sec:sota:category-2}

Software applications are typically not tested at a single point in time; they are rather tested incrementally, along with the natural evolution of the code base: 
new tests are typically added together with a change or a commit~\cite{azaidmanEMSE2011,DBLP:conf/icst/ZaidmanRDD08}, to verify, for instance, that a bug has been fixed or that a new feature is correctly implemented. 
In the context of test amplification, it directly translates to the idea of synthesizing new tests as a reaction to a change. 
This can be seen as a specialized form $AMP_{add}$, which considers a specific change, in addition to the existing test suite, to guide the amplification.
This kind of test amplification is denoted as $AMP_{change}$. 

\medskip
\textbf{Definition: Test amplification technique $AMP_{change}$ consists of adding new tests to the current test suite, by creating new tests that cover and/or observe the effects of a change in the application code.}

I first present a series of works by Xu \etal, who develop and compare two alternatives of test suite augmentation, one based on genetic algorithms and the other on concolic execution. 
A second subsection presents the work of a group of authors that center the attention on finding testing conditions to exercise the portions of code that exhibit changes. 
A third subsection exposes works that explore the adaptation and evolution of test cases to cope with code changes. 
The last subsection shows other promising works in this area.

\subsection{Example}
\label{subsec:sota:category-2:example}

\autoref{lst:example:ampchange:original} shows a toy class and two test cases designed to verify its code.
At some point in development, the code of the method is modified as shown in \autoref{lst:example:ampchange:modified}. 
The change consists of the addition of a new block in line \ref{line:example:ampchange:modified}. 

\begin{lstlisting}[caption={Initial version of a class and two test cases},label=lst:example:ampchange:original,float,language=java,numbers=left]
class Computer{
	public int computeValue(int input) {
		if(input < 3) {
			return input/2;
		}
		return 0;
	}
}

class ComputerTest {
	int threshold = 4;

	@Test
	public testSmallInput() {
		Computer comp = new Computer();
		assertTrue(comp.computeValue(2) < threshold);
	}

	@Test
	public testDefault() {
		Computer comp = new Computer();
		assertEquals(comp.computeValue(10), 0);
	}
}
\end{lstlisting}

\begin{lstlisting}[caption={Modified version of the initial class},label=lst:example:ampchange:modified,float,language=java,numbers=left]
class Computer{
	public int computeValue(int input) {
		if(input < 3) {
			return input/2;
		}
		if (input <= 5) { (*@ \label{line:example:ampchange:modified} @*)
			return 2*input;
		}
		return 0;
	}
}
\end{lstlisting}

The existing test cases do not execute the new code.
There is no test input in the $[3,5]$ interval. 
An $AMP_{change}$ technique would increment the test suite with a new test case, like the one shown in \autoref{lst:example:ampchange:amplified}, that covers the new code. 
The technique should be able to generate an input that meets the requirement to reach the new or changed code and the right oracle given the new conditions. 

\begin{lstlisting}[caption={A test case that covers the new portion of  code.},label=lst:example:ampchange:amplified,float,language=java,numbers=left]
@Test
public testInput() {
	Computer comp = new Computer();
	assertTrue(comp.computeValue(4) > threshold);
}
\end{lstlisting}


\subsection{Search-based vs. Concolic Approaches}
\label{subsec:sota:category-2:search-based-vs-concolic}

In their work, Xu \etal\cite{xu2009directed} focus on the scenario where a program has evolved into a new version through code changes in development. 
They consider techniques as 
(i) the identification of coverage requirements for this new version, given an existing test suite;
and (ii) the creation of new test cases that exercise these requirements. 
Their approach first identifies the parts of the evolved program that are not covered by the existing test suite. 
In the same process they gather path conditions for every test case. 
Then, they exploit these path conditions with a concolic testing method to find new test cases for uncovered branches, analyzing one branch at a time.

Symbolic execution is a program analysis technique to reason about the execution of every path and to build a symbolic expression for each variable. 
Concolic testing also carries a symbolic state of the program, but overcomes some limitations of a fully symbolic execution by also considering certain concrete values. 
Both techniques are known to be computationally expensive for large programs.

Xu \etal avoid a full concolic execution by only targeting paths related to uncovered branches. 
This improves the performance of the augmentation process.
They applied their technique to 22 versions of a small arithmetic program from the SIR \cite{SIR} repository and achieved branch coverage rates between 95\% and 100\%. 
They also show that a full concolic testing is not able to obtain such high coverage rates and needs a significantly higher number of constraint solver calls.

In subsequent work, Xu \etal\cite{xu2010factors} address the same problem with a genetic algorithm. 
Each time the algorithm runs, it targets a branch of the new program that is not yet covered.
The fitness function measures how far a test case falls from the target branch during its execution. 
The authors investigate if all test cases should be used as population, or only a subset related to the target branch or, if newly generated cases should be combined with existing ones in the population. 
Several variants are compared according to their efficiency and effectiveness, that is, whether the generated test cases achieve the goal of exercising the uncovered branches.
The experimentation targets 3 versions of \emph{Nanoxml}, an XML parser implemented in Java with more than 7 KLoC and included in the SIR \cite{SIR} repository.
The authors conclude that considering all tests achieves the best coverage, but also requires more computational effort. 
They imply that the combination of new and existing test cases is an important factor to consider in practical applications. 

Xu \etal then dedicate a paper to the comparison of concolic execution and genetic algorithms for test suite amplification \cite{xu2010directed}. 
The comparison is carried out over four small (between 138 and 516 LoC) C programs from the SIR \cite{SIR} repository.
They conclude that both techniques benefit from reusing existing test cases at a cost in efficiency. 
The authors also state that the concolic approach can generate test cases  effectively in the absence of complex symbolic expressions. 
Nevertheless, the genetic algorithm is more effective in the general case, but could be more costly in test case generation. 
Also, the genetic approach is more flexible in terms of scenarios where it can be used, but the quality of the obtained results is heavily influenced by the definition of the fitness function, mutation test and crossover strategy. 

The same authors propose a hybrid approach \cite{xu2011hybrid}. 
This new approach incrementally runs both the concolic and genetic methods. 
Each round applies first the concolic testing and the output is passed to the genetic algorithm as initial population. 
Their original intention was to get a more cost-effective approach. 
The evaluation is done over three of the C programs from their previous study.
The authors conclude that this new proposal outperforms the other two in terms of branch coverage, but in the end is not more efficient. 
They also speculate about possible strategies for combining both individual approaches to overcome their respective weaknesses and exploit their best features. 
A revised and extended version of this work is given in \cite{xu2015directed}.

\subsection{Finding Test Conditions in the Presence of Changes}
\label{subsec:sota:category-2:finding-test-conditions}

Another group of authors have worked under the premise that achieving only coverage may not be sufficient to adequately exercise changes in code. 
Sometimes these changes manifest themselves only when particular conditions are met by the input. 
The following papers address the problem of finding concrete input conditions that not only can execute the changed code, but also propagate the effects of this change to an observable point that could be the output of the involved test cases. 
However, their work does not create concrete new test cases. 
Their goal is to provide guidance, in the form of conditions that can be leveraged to create new tests with any generation method.

It is important to notice that they do not achieve test generation. 
Their goal is to provide guidance to generate new test cases independently of the selected generation method.

Apiwattanapong \etal\cite{apiwattanapong2006matrix} target the problem of finding test conditions that could propagate the effects of a change in a program to a certain execution point. 
Their method takes as input two versions of the same program. 
First, an alignment of the statements in both versions is performed. 
Then, starting from the originally changed statement and its counterpart in the new version, all statements whose execution is affected by the change are gathered up to a certain distance. 
The distance is computed over the control and data dependency graph.  
A partial symbolic execution is performed over the affected instructions to retrieve the states of both program versions, which are in turn used to compute testing requirements that can propagate the effects of the original change to the given distance. 
As said before, the method does not deal with test case creation, it only finds new testing conditions that could be used in a separate generation process and is not able to handle changes to several statements unless the changed statements are unrelated. 
The approach is evaluated on Java translations of two small C programs (102 Loc and 268 LoC) originally included in the Siemens program dataset \cite{hutchins1994experiments}. 
The authors conclude that, although limited to one change at a time, the technique can be leveraged to generate new test cases during regular development.

Santelices \etal\cite{santelices2008test} continue and extend the previous work by addressing changes to multiple statements and considering the effects they could have on each other. 
In order to achieve this they do not compute state requirements for changes affected by others. 
This time, the evaluation is done in one of the study subjects form their previous study and two versions of \emph{Nanoxml} from SIR.

In another paper \cite{santelices2011applying} the same authors address the problems in terms of efficiency of applying symbolic execution. 
They state that limiting the analysis of affected statements up to a certain distance from changes reduces the computational cost, but scalability issues still exist. 
They also explain that their previous approach often produces test conditions which are unfeasible or difficult to satisfy within a reasonable resource budget. 
To overcome this, they perform a dynamic inspection of the program during test case execution over statically computed slices around changes. 
The technique is evaluated over five small Java programs, comprising \emph{Nanoxml} with 3 KLoC and translations of C programs from SIR having between 283 LoC and 478 LoC.
This approach also considers multiple program changes. 
Removing the need of symbolic execution leads to a less expensive method. 
The authors claim that propagation-based testing strategies are superior to coverage-based in the presence of evolving software.

\subsection{Other Approaches}
\label{subsec:sota:category-2:others-approach}

Other authors have also explored test suite augmentation for evolving programs with propagation-based approaches. 
Qui \etal\cite{qi2010test} propose a method to add new test cases to an existing test suite ensuring that the effects of changes in the new program version are observed in the test output. 
The technique consists of a two step symbolic execution. 
First, they explore the paths towards a change in the program guided by a notion of distance over the control dependency graph. 
This exploration produces an input able to reach the change. 
In a second moment they analyze the conditions under which this input may affect the output and make changes to the input accordingly. 
The technique is evaluated using 41 versions of the \emph{tcas} program from the SIR repository (179 LoC) with only one change between versions. 
The approach was able to generate tests reaching the changes and affected the program output for 39 of the cases. 
Another evaluation was also included for two consecutive versions of the \emph{libPNG} library (28 KLoC) with a total of 10 independent changes between them. 
The proposed technique was able to generate tests that reached the changes in all cases and the output was affected in nine of the changes. 
The authors conclude that the technique is effective in the generation of test inputs to reach a change in the code and expose the change in the program output.

Wang \etal\cite{xwang2014directed} exploit existing test cases to generate new ones that execute the change in the program. 
These new test cases should produce a new program state, in terms of variable values, that can be propagated to the test output. 
An existing test case is analyzed to check if it can reach the change in an evolved program. 
The test is also checked to see if it produces a different program state at some point and if the test output is affected by the change. 
If some of these premises do not hold then the path condition of the test is used to generate a new path condition to achieve the three goals. 
Further path exploration is guided and narrowed using a notion of the probability for the path condition to reach the change. 
This probability is computed using the distance between statements over the control dependency graph. 
Practical results of test cases generation in three small Java programs (from 231 LoC to 375 LoC) are exhibited. 
The method is compared to \emph{eXpress} and \emph{JPF-SE} two state of the art tools and is shown to reduce the number of symbolic executions by 45.6\% and 60.1\% respectively. 
As drawback, the technique is not able to deal with changes on more than one statement. 

Mirzaaghaei \etal\cite{Mirzaaghaei2012,mirzaaghaei2014automatic} introduce an approach that leverages information from existing test cases and automatically adapts test suites to code changes.
Their technique can repair, or evolve test cases in front of signature changes (\ie changing the declaration of method parameters or return values), the addition of new classes to the hierarchy, addition of new interface implementations, new method overloads and new method overrides. 
Their effective implementation \emph{TestCareAssitance} (TCA) first diffs the original program with its modified version to detect changes and searches in the test code similar patterns that could be used to complete the missing information or change the existing code. 
They evaluate TCA for signature changes in 9 Java projects of the Apache foundation and repair in average 45\% of modifications that lead to compilation errors. 
The authors further use five additional open source projects to evaluate their approach when adding new classes to the hierarchy. 
TCA is able to generate test cases for 60\% of the newly added classes.
This proposal could also fall in the category of test repairing techniques. 
Section \autoref{sec:amp_mod} will explore alternatives in a similar direction that produce test changes instead of creating completely new test cases.

In a different direction, Böhme \etal\cite{bohme2013regression} explain that changes in a program should not be treated in isolation. 
Their proposal focuses on potential interaction errors between software changes. 
They propose to build a graph containing the relationship between changed statements in two different versions of a program and potential interaction locations according to data and control dependency. 
This graph is used to guide a symbolic execution method and find path conditions for exercising changes and their potential interactions and use a Satisfiability Modulo Solver to generate a concrete test input. 
They provide practical results on six versions the \emph{GNU Coreutils} toolset that introduce 11 known errors. They were able to find 5 unknown errors in addition to previously reported issues.

Marinescu and Cadar \cite{marinescu2013katch} present a system, called \emph{Katch}, that aims at covering the code included in a patch. 
Instead of dealing with one change to one statement, as most of the previous works, this approach first determines the differences of a program and its previous version after a commit, in the form of a code patch. 
Lines included in the patch are filtered by removing those that contain non-executable code (\ie comments, declarations). 
If several lines belong to the same basic program block, only one of them is kept as they will all be executed together. 
From the filtered set of lines, those not covered by the existing test suite are considered as targets. 
The approach then selects the closest input to each target from existing tests using the static minimum distance over the control flow graph. 
Edges on this graph that render the target unreachable are removed by inspecting the data flow and gathering preconditions to the execution of basic blocks. 
To generate new test inputs, they combine symbolic execution with heuristics that select branches by their distance to the target, regenerate a path by going back to the point where the condition became unfeasible or changing the definition of variables involved in the condition. 
The proposal is evaluated using the \emph{GNU findutils}, \emph{diffutils} and \emph{binutils} which are distributed with most Unix-based distributions. \\
They examine patches from a period of 3 years. 
In average, they automatically increase coverage from 35\% to 52\% with respect to the manually written test suite.

A posterior work of the same group~\cite{palikareva2016shadow} also targets patches of code, focusing on finding test inputs that execute different behavior between two program versions. 
They consider two versions of the same program, or the old version with the patch of changed code, and a test suite. 
The code should be annotated in places where changes occur in order to unify both versions of the program for the next steps. 
Then they select from the test suite those test cases that cover the changed code. 
If there is no such test case, it can be generated using \emph{Katch}.  
The unified program is used in a two stage dynamic symbolic execution guided by the selected test cases: look for branch points where two semantically different conditions are evaluated in both program versions; bounded symbolic execution for each point previously detected. 
At those points all possible alternatives in which program versions execute the same or different branch blocks are considered and used to make the constraint solver generate new test inputs for divergent scenarios. 
The program versions are then normally executed with the generated inputs and the result is validated to check the presence of a bug or an intended difference. 
In their experiments this validation is mostly automatic but in general should be performed by developers.
The evaluation of the proposed method is based on the \emph{CoREBench}~\cite{bohme2014corebench} data set that contains documented bugs and patches of the \emph{GNU Coreutils} program suite. 
The authors discuss successful and unsuccessful results but in general the tool is able to produce test inputs that reveal changes in program behaviour.

\subsection{Summary}
\label{subsec:sota:category-2:summary}

\emph{Main achievements:}
$AMP_{change}$ techniques often rely on symbolic and concolic execution. 
Both have been successfully combined with other techniques in order to generate test cases that reach changed or evolved parts of a program \cite{xu2011hybrid,xu2015directed,marinescu2013katch}.
Those hybrid approaches produce new test inputs that increase the coverage of the new program version. Data and control dependency has been used in several approaches to guide symbolic execution and reduce its computational cost \cite{bohme2013regression,marinescu2013katch,xwang2014directed}. 
The notion of distance from statements to observed changes has been also used for this matter \cite{marinescu2013katch,apiwattanapong2006matrix}.


\emph{Main challenges:}
Despite the progress made in the area, a number of challenges remain open. 
The main challenge relates to the size of the changes considered for test amplification: many of the works in this area consider a single change in a single statement \cite{apiwattanapong2006matrix,qi2010test,xwang2014directed}. 
While this is relevant and important to establish the foundations for $AMP_{change}$, this cannot fit current development practices where a change, usually a commit, modifies the code at multiple places at once. 
A few papers have started investigating multi-statement changes for test suite amplification \cite{santelices2008test,marinescu2013katch,palikareva2016shadow}. 
Now, $AMP_{change}$ techniques should fit into the revision process and be able to consider a commit as the unit of change. 

Another challenge relates to scalability. 
The use of symbolic and concolic execution has proven to be effective in test input generation targeting program changes. 
Yet,  these two techniques are computationally expensive \cite{xu2009directed,xu2011hybrid,xu2015directed,apiwattanapong2006matrix,santelices2008test,palikareva2016shadow}. 
Future works shall consider more efficient ways for exploring input requirements that  exercise program changes or new uncovered parts. 
Santelices and Harrold~\cite{santelices2011applying} propose to get rid of symbolic execution by observing the program behavior during test execution. 
However, they do not generate test cases.

Practical experimentation and evaluation remains confined to a very small number of programs, in most cases less than five study subjects, and even small programs in terms of effective lines of code. 
A large scale study on the subject is still missing.

\section{Amplification by Modifying Test Execution}
\label{sec:sota:category-3}

In order to explore new program states and behavior, it is possible to interfere with the execution at runtime so as to modify the execution of the program under test. 

\medskip
\textbf{Definition: Test amplification technique $AMP_{exec}$ consists of modifying the test execution process or the test harness in order to maximize the knowledge gained from the testing process.}

One of the drawbacks of automated tests is the hidden dependencies that may exist between different unit test cases. 
In fact, the order in which the test cases are executed may affect the state of the program under test. 
A good and strong test suite should have no implicit dependencies between test cases.

The majority of test frameworks are deterministic, \ie between two runs the order of execution of test is the same~\cite{DBLP:conf/icsm/PalombaZ17,PalombaEMSE2019}.

An $AMP_{exec}$ technique would randomize the order in which the tests are executed to reveal hidden dependencies between unit tests and potential bugs derived from this situation.

\subsection{Amplification by Modifying Test Execution}
\label{subsec:sota:category-3:modifying-test-execution}

Zhang and Elbaum \cite{zhang2012,ZhangE14} describe a technique to validate exception handling in programs making use of APIs to access external resources such as databases, GPS or bluetooth.
The method mocks the accessed resources and amplifies the test suite by triggering unexpected exceptions in sequences of API calls.
Issues are detected during testing by observing abnormal terminations of the program or abnormal execution times. 
They evaluated their approach on 5 Android artifacts. 
Their sizes vary from 6k to 18k line of codes, with 39 to 117 unit tests in the test suite. 
The size of the benchmark seems quite reasonable. 
The approach is shown to be cost-effective and able to detect real-life problems in 5 Android applications.

Cornu \etal\cite{cornu2015exception} work in the same line of exception handling evaluation. 
They propose a method to complement a test suite in order to check the behaviour of a program in the presence of unanticipated scenarios. 
The original code of the program is modified with the insertion of \texttt{throw} instructions inside \texttt{try} blocks. 
The test suite is considered as an executable specification of the program and therefore used as an oracle in order to compare the program execution before and after the modification.
Under certain conditions, issues can be automatically repaired by catch-stretching.
The authors used 9 Java open-source projects to create a benchmark and evaluate their approach. 
This benchmark is big enough to conclude the generalization of the results. 
The selected artifacts are well-known, modern and large: Apache artifacts, joda-time and so on.
Their empirical evaluation shows that the short-circuit testing approach of exception contracts increases the knowledge of software.

Leung \etal\cite{leung12} are interested in finding data races and non-determinism in GPU code written in the CUDA programming language.
In their context, test amplification consists of generalizing the information learned from a single dynamic run. 
The main contribution is to formalize the relationship between the trace of the dynamic run and statically collected information flow. 
The authors leverage this formal model to define the conditions under which they can generalize the absence of race conditions for a set of input values, starting from a run of the program with a single input.
They evaluated their approach using 28 benchmarks in the NVIDIA CUDA SDK Version 3.0. 
They removed trivial ones and some of them that they cannot handle. 
The set of benchmarks is big enough and contains a diversity of applications to be convinced that the approach can be generalized.


Fang \etal\cite{fang2015perfblower} develop a performance testing system named \emph{Perfblower}, which is able to detect and diagnose memory issues by observing the execution of a set of test methods. 
The system includes a domain-specific language designed to describe memory usage symptoms. 
Based on the provided descriptions, the tool evaluates the presence of memory problems. 
The approach is evaluated on 13 Java real-life projects. 
The tool is able to find real memory issues and reduce the number of false positives reported by similar tools.
They used the small workload of the DaCapo~\cite{DaCapo} benchmark. 
They argue that developers will not use large workloads and it is much more difficult to reveal performance bugs under small workloads. 
These two claims are legit, however the authors do not provide any evidence of the scalability of the approach.

Zhang \etal\cite{Zhang2016Isomorphic} devise a methodology to improve the capacity of the test suite to detect regression faults. 
Their approach is able to exercise uncovered branches without generating new test cases. 
They first look for identical code fragments between a program and its previous version. 
Then, new variants of both versions are generated by negating branch conditions that force the test suite to execute originally uncovered parts. 
The behavior of version variants are compared through test outputs. 
An observed difference in the output could reveal an undetected fault. 
An implementation of the approach is compared with \emph{EvoSuite} \cite{fraser2011evosuite} on 10 real-life Java projects. 
In the experiments, known faults are seeded by mutating the original program code. 
The results show that \emph{EvoSuite} obtains better branch coverage, while the proposed method is able to detect more faults. 
The implementation is available in the form of a tool named \emph{Ison}.

\subsection{Summary}
\label{subsec:sota:category-3:summary}

\emph{Main achievements:} $AMP_{exec}$ proposals provide cost-effective approaches to observe and modify a program execution to detect possible faults.
This is done by instrumenting the original program code to place observations at certain points or mocking resources to monitor API calls and explore unexpected scenarios.
It adds no prohibitive overheads to regular test execution and provides means to gather useful runtime information. 
Techniques in this section were used to analyze real-life projects of different sizes and they are shown to match other tools that pursue the same goal and obtain better results in some cases. 

\emph{Main challenges:} 
As shown by the relatively small number of papers discussed in this section, techniques for test execution modification have not been widely explored. 
The main challenge is to get this concept known so as to enlarge the research community working on this topic.
The concerned works are: \cite{zhang2012,ZhangE14,cornu2015exception,leung12,fang2015perfblower,Zhang2016Isomorphic}.