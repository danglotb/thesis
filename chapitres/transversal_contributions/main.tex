\chapter{Transversal Contributions}
\label{chap:transversal-contributions}

\minitoc

\graphicspath{{.}{chapitres/transversal_contributions/}}

In this chapter, I expose 3 transversal contributions that I made during my thesis.
Thanks to the expertise that I developed during my thesis, I worked with my researchers colleagues on side topics yet linked to this thesis.
\begin{itemize}
	\item The first transversal contribution studies the correctness of program under runtime perturbations in \autoref{sec:transversal-contributions:correctness}.
	This work has been done during my master degree.
	\item The second transversal contribution studies the pseudo-tested methods \autoref{sec:transversal-contributions:descartes}. 
	\item The third transversal contribution studies patches' overfitting in test-based repair techniques in \autoref{sec:transversal-contributions:test-for-repair}.
\end{itemize}
These 3 transversal contributions are supported by published articles~\cite{Danglot2018,descartes,Yu2019} in \emph{Empirical Software Engineering}.
However, the reader can skip this chapter since it is additional materials and this chapter does not expose core contributions.

\section{Study of Program Correctness}
\label{sec:transversal-contributions:correctness}

In this thesis, I study the correctness of programs through test methods.
\dspot, that has been presented in \autoref{chap:dspot}, is a test suite amplification tool that generates new test methods using the current behavior of the program as oracle to build assertions.
In other words, \dspot considers considers the current values that compose the state of the program as \textbf{correct}.
But, what does correct mean? In this first transversal contribution, I study the correctness program under run-time perturbations.

\subsection{Problem Statement}
\label{subsec:transversal-contributions:correctness:introduction}

Recalling that in \autoref{chap:introduction}, I mentioned a quote of Dijkstra:
\begin{center}
	\emph{``the smallest possible perturbations - \ie changes of a single bit - can have the most drastic consequences.''.}
\end{center}

This first study aims at verifying this statement is true or not.
Dijkstra considers software as a system that is in an unstable equilibrium, or to put it more precisely, that the correctness of a program output is unstable with respect to perturbations.
However, previous works, \eg\cite{Rinard:2005:EAE:1094855.1094866,Li2007Correctness}) suggest the opposite, \ie suggest that programs can accommodate perturbations.

In the context of my thesis, the correctness of programs relies on test suite.
However, the incompleteness of test suites make them poor oracles to verify the truthfulness of the Dijkstra's hypothesis.

This is why I devise \perturb, an experimental protocol to study the stability of program correctness under execution perturbation.
It consists in perturbing the execution of programs according to a perturbation model and observing whether this has an impact on the correctness of the output.
The two observable different outcomes are: 
the perturbation breaks the computation and results in an incorrect output (unstable under perturbation), 
or the correctness of the output is stable despite the perturbation.
This protocol has two key-attributes:

1) It is based on perfect oracle, \ie it verifies the output of the perturbed program is completly correct, \ie bit-wise equals to the output of the unpertubed, or original, program;

2) It explores exhaustively in time and space the perturbation envelop of software.

The remaining of this section is organized as follow: 
first, I present \perturb protocol in \autoref{subsec:transversal-contributions:correctness:protocol}.
second, I present the experimentation that has been carried out in \autoref{subsec:transversal-contributions:correctness:experimentation};
and eventually, \autoref{subsec:transversal-contributions:correctness:conclusion} conclude this section.

\subsection{\perturb protocol}
\label{subsec:transversal-contributions:correctness:protocol}

To actually perform perturbations, \perturb adds \textbf{perturbation points} to the program under study where a \textbf{perturbation point} (denoted $pp$) is an expression of a given data type.
For instance, if one perturbs integer values, the \textbf{perturbation points} will be all integer expressions (literals as well as compound expressions). 
In \autoref{lst:example_pp_underbrace}\footnote{$|$ is the bitwise or operator. $>>$ is the binary right shift operator. The assignment $|=$ is the bitwise or operator between the left operand and the right operand, then the result is affected to the left operand.}, there are 3 potential integer perturbation points in a single statement, indicated with braces.
\begin{lstlisting}[basicstyle=\small, caption=Three integer \textbf{perturbation points} in a single statement., label=lst:example_pp_underbrace]
acc |= (*@$\underbrace{i}$@*) >> (*@$\underbrace{mask}$@*);
								(*@$pp_1$@*)     (*@$pp_2$@*)
acc |= (*@$\underbrace{i >> mask}$@*);
										(*@$pp_3$@*)  
\end{lstlisting}

\perturb statically locates \textbf{perturbation points} and automatically adds perturbation code around them using a code transformation. 
The transformation consists of wrapping all compatible expressions into a function call $p$ (for ``perturb'')\footnote{In the experimentation, it is implemented on Java programs using the Spoon transformation library \cite{pawlak:hal-01169705}.}.

\begin{lstlisting}[basicstyle=\small, caption=The same statement with perturbation code injected., label=lst:example_pp]
acc |= p(3, p(1,i) >> p(2,mask));
\end{lstlisting}

In \autoref{lst:example_pp}, each integer expression of \autoref{lst:example_pp_underbrace} is wrapped into a call to function $p$.
Function $p$ takes two parameters: a unique index to identify the perturbation point and the expression being perturbed.
If $p$ returns  the second parameter, the transformed program is semantically equivalent to the original program. 
The identifier argument enables \perturb to perturb only one location at a time. 
In this example, this identifier ranges from 1 to 3 corresponding to the index given in \autoref{lst:example_pp_underbrace} under perturbation point $pp$.

\subsubsection{Core Algorithm}
\label{subsubsec:transversal-contributions:correctness:protocol:algo}

The goal of this algorithm is to systematically explore the perturbation space. 
\hyperref{alg:systexplor} first records the number of executions of each perturbation point for each input in a matrix $R_{ref}$ (for reference run) without injecting any perturbation.
$R_{ref}[pp,i]$ refers to the number of executions of perturbation point $pp$ for a given input $i$.
Then, it re-executes the program for each input, with one perturbation for each point so that each point is perturbed at least and at most once per input.
The oracle asserts the correctness of the perturbed execution (output $o$) for the given input ($i$).
A perturbed execution can have three outcomes:
a success, meaning that the correctness oracle validates the output;
a failure meaning that the correctness oracle is violated (also called an oracle-broken execution);
a runtime error (an exception in Java) meaning that the perturbation has produced a state for which the computation becomes impossible at some point (\eg a division-by-zero).

This algorithm performs a systematic exploration of the perturbation space for a given program and a set of inputs according to a perturbation model.

\begin{algorithm}[h]
	\algorithmicrequire{ \\
		$prog$: program,\\
		$model$: perturbation model,\\
		$I$: set of inputs for program $prog$,\\
		$oracle$: a perfect oracle for program $prog$}\\
	\algorithmicensure{ \\
		$exc$: counters of execution per perturbation point,\\
		$s$: counters of success per perturbation point,\\
		$ob$: counters of oracle broken per perturbation point}
	\begin{algorithmic}
		\State{instrument($prog$)}
		\For{each input i in I} 
			\For{perturbation point $pp$ in $prog$}
				\State $R_{ref}[,i] \leftarrow runWithoutPerturbation(prog, i)$
				\For{$j = 0$, to $ R_{ref}[pp,i]$}
					\State $o \leftarrow runWithPerturbationAt(prog,model,i,pp,j)$
					\If{exception is thrown}
						\State {$exc[pp] \leftarrow exc[pp] + 1$} 
					\ElsIf{$oracle.assert(i,o)$} 
						\State {$s[pp] \leftarrow s[pp] + 1$} 
					\Else 
						\State {$ob[pp] \leftarrow ob[pp] + 1$}
					\EndIf
				\EndFor
			\EndFor
		\EndFor
	\end{algorithmic}
	\caption{Core Algorithm of \perturb.}
	\label{alg:systexplor}
\end{algorithm}

In \autoref{alg:systexplor}, $R_{ref}[,i]$ denotes column $i$ of matrix $R_{ref}$. 
The statement \emph{runWithoutPerturbation(prog, i)} returns a column vector which is assigned to the $i\thb$ column of matrix $R_{ref}$; each element is one perturbation point: it contains the number of times each perturbation point is executed in the program \emph{prog} for each input \emph{i}. 
On the other hand, the statement \emph{runWithPerturbationAt(prog, model, i, pp, j)} runs the program \emph{prog} while using the perturbation model \emph{model}, the perturbation point \emph{pp} at its $j\thb$ execution for the given input \emph{i}.

\subsection{Evaluation}
\label{subsec:transversal-contributions:correctness:experimentation}

The experimentation with a dataset of 10 programs. 
This dataset has been created following this methodology:
first and foremost, the programs can be specified with a perfect oracle;
second, they are written in Java;
third, they come from diverse application domains in order to maximize external validity.

\subsubsection{The PONE Experiment}
\label{subsubsec:transversal-contributions:correctness:experimentation:PONE}

I now present the PONE experiment. 
Its goal is to explore correctness attraction according to increments ($+ 1$) of integer values at runtime.

Point that a single perturbation always breaks the output correctness are qualified as \textbf{fragile} because a single perturbation at runtime breaks the whole computation.
Other points that can be systematically perturbed without any consequence on the correctness of the final output of the program are qualified as \textbf{antifragile} (in opposition to fragile).
The remainder are in between; those with a correctness ratio larger or equal than 75\% are qualified as \textbf{robust}.

In the PONE experiment, integer expressions are perturbed.
The PONE perturbation model is a small perturbation on an integer expression: a single increment of an integer value only once during an execution. 
An equivalently small perturbation model is MONE consisting of decrementing integers. 
An experimentation as been also performed using MONE, however, the results are not reported in this thesis.
For more information, see the dedicated article.\cite{Danglot2018}

\begin{table}[h]
	\caption{PONE Results. The correctness ratio may not correspond directly to the number of Antifragile and Robust expressions because it is computed over all executions.}
	\label{tab:results-PONE}
	\rowcolors{2}{white}{gray!25}
	\def\arraystretch{0.55}%  1 is the default, change whatever you need
	\setlength\tabcolsep{0.45pt} % default value: 6pt
	\centering\begin{tabular}{lllllll}
		\toprule
		Subject & $N^{int}_{pp}$ & |Search space| & \# Fragile exp. & \#Robust exp.& \# Antifragile exp. & Correctness ratio\\
		\midrule
		quicksort&41&151444&6&10&19&\textendash\textendash\textendash\textendash\textendash\textendash\textendash { }  77 \%\\
		zip&19&38840&5&2&5&\textendash\textendash\textendash\textendash\textendash\textendash\textendash { }  76 \%\\
		sudoku&89&98211&12&27&8&\textendash\textendash\textendash\textendash\textendash\textendash { }  68 \%\\
		md5&164&237680&102&24&7&\textendash\textendash { }  29 \%\\
		rsa&117&2576&55&8&32&\textendash\textendash\textendash\textendash\textendash { }  54 \%\\
		rc4&115&165140&60&7&12&\textendash\textendash\textendash { }  38 \%\\
		canny&450&616161&58&129&133&\textendash\textendash\textendash\textendash\textendash\textendash\textendash\textendash\textendash { }  94 \%\\
		lcs&79&231786&10&47&13&\textendash\textendash\textendash\textendash\textendash\textendash\textendash\textendash { }  89 \%\\
		laguerre&72&423454&15&24&15&\textendash\textendash\textendash\textendash\textendash\textendash\textendash\textendash\textendash { }  90 \%\\
		linreg&75&543720&43&18&11&\textendash\textendash\textendash\textendash { }  47 \%\\
		total & 1221&2509012&366&296&255&\textendash\textendash\textendash\textendash\textendash\textendash { }  66 \%\\
		\bottomrule
	\end{tabular}
\end{table}

\autoref{tab:results-PONE} gives the results of the systematic exploration of the PONE perturbation space.
For each subject, this table gives:
the number of integer perturbation points $N^{int}_{pp}$;
the number of perturbed executions (equal to the size of the PONE perturbation space);
the number of \textbf{fragile} integer expressions;
the number of \textbf{robust} integer expressions;
the number of \textbf{antifragile} integer expressions;
the \textbf{correctness ratio} (percentage of correct outputs) over all perturbed executions.

\begin{mdframed}
To sum up, the main conclusions of the PONE experiment are:
\begin{itemize}
	\item The considered programs are perturbable according to the PONE perturbation model.
	\item There are very few fully fragile integer expressions in the considered programs.
	\item There is a majority of highly perturbable integer expressions which results in a high level of correctness attraction.
	\item Dijkstra's view that software is fragile is not always true, correctness is rather a stable equilibrium than an unstable one.
\end{itemize}
\end{mdframed}

\subsection{Discussion}
\label{subsec:transversal-contributions:correctness:conclusion}

I have devised a protocol called \perturb to study the stability of programs under perturbation.
\perturb exhaustively explores the perturbation space of a given program for a set of inputs according to a perturbation model.
An experimentation have been conducted on 10 subjects using the PONE perturbation model.
In total, 2509012 perturbed executions have been performed and studied, which makes it one of the largest perturbation experiments ever made.
From this experimentation, the presence of ``correctness attraction'' has been observed. Over all perturbed execution, 66\% of them do not break the correctness of the output. 

Studying correctness attraction can have divers applicability.
One of them is to identify points that can be randomized and protect the software from external and malicious attacks.
Also, if one could engineer techniques to automatically improve correctness attraction, in order to obtain zones that accommodate more perturbations of the runtime state, and those zones could be deemed ``bug absorbing zones''.

To conclude, I imagine two ways to combine both \dspot and \perturb:
First, using \perturb as a test-criterion to amplify test suites, \ie \dspot would keep amplified test methods that detect more perturbations than the original test suite.
Second, in \perturb, I used perfect oracles.
The problem they have been manually devised, and this could be approximated using the test suite.
To strengthen it, one could amplifies its test suite with \dspot to have a better approximation of the perfect oracle and thus study deeper the correctness of its program.

\section{Study of Pseudo-tested Methods}
\label{sec:transversal-contributions:descartes}

Pseudo-tested methods are source methods that when the body is removed, the whole test suite passes, while some test methods are executing this methods.
In \autoref{chap:test-improvement}, I use \ms to measure the quality of a test suite.
The detection of pseudo-tested methods can be done using \ms and specifically designed mutants.
In this second transversal contributions, we study the presence of pseudo-tested methods and the developers assessment if it is worthy or not to fix them.
In \dspot, the default test-criterion used is the pseudo-tested methods, \ie \dspot keeps amplified test methods that specify pseudo-tested methods.

\subsection{Problem Statement}
\label{subsec:transversal-contributions:descartes:introduction}

Niedermayr and colleagues \cite{niedermayr_will_2016} recently introduced the concept of \emph{\pseudotested{} methods}. 
These methods are covered by the test suite, but no test case fails even if all behaviors of the method are removed at once, \ie when the body is completely stripped off.
This work is novel and intriguing:
such \pseudotested{} methods are present in all projects, even those with test suites that have high coverage ratio.

If those results hold, it calls for more research on this topic.
This is the motivation of this paper: 
first, we challenge the external validity of Niedermayr \etal's experiment with new study subjects, 
second we perform an in-depth qualitative empirical study of \pseudotested{} methods. 
In particular, we want to determine if \pseudotested{} methods are indicators of badly tested code. 
While this seems to be intuitively true, we aim at quantifying this phenomenon.
Second, we want to know whether \pseudotested{} methods are relevant indicators for developers who wish to improve their test suite. 
In fact, these methods may encapsulate behaviors that are poorly specified by the test suite, but are not relevant functionalities for the project.

\subsection{Definition and Implementation}
\label{subsec:transversal-contributions:descartes:definitions}

Let $P$ be a program and $m$ be a method; $S=\cup_{m \in P}{\mathit{effects}(m)}$ the set of effects of all methods in $P$; 
$\mathit{effects}(m)$ a function $\mathit{effects}:P \rightarrow S$ that returns all the effects of a method $m$; $detect$, a predicate $TS \times S \rightarrow \{\top, \bot\}$ that determines if an effect is detected by $TS$.
Here, we consider the following possible effects that a method can produce: 
change the state of the object on which it is called, change the state of other objects (by calling other methods), return a value as a result of its computation.

\begin{definition}
	\label{def:pseudo}
	A method is said to be \pseudotested{} with respect to a test suite, if the test suite covers the method and does not assess any of its effects.
\end{definition}

A ``pseudo-tested'' method, as defined previously, is an idealized concept.
We now describe an algorithm that implements a practical way of collecting a set of \pseudotested{} methods in a program $P$, in the context of the test suite $TS$, based on the original proposal of \theoriginalauthors. 
It relies on the idea of ``extreme code transformations'', which consists in completely stripping out the body of a method.

Algorithm \autoref{alg:detection} starts by analyzing all methods of $P$ that are covered by the test suite and fulfill a predefined selection criterion (predicate $\mathrm{ofInterest}$ in line \ref{of-interest}).
This critetion is based on the structure of the method and aims at reducing the number of false positives detected by the procedure. 
It eliminates uninteresing methods such as trivial setter and getters or empty void methods.
If the method returns a value, the body of the method is stripped out and we generate a few variants that simply return predefined values (line \ref{if-return}).\footnote{Compared to \theoriginalauthors, we add two new transformations, one to return $null$ and another to return an empty array. These additions allow to expand the scope of methods to be analyzed.}
If the method is void, we strip the body without further action (line \ref{no-return}).
Once we have a set of variants, we run the test suite on each of them, if no test case fails on any of the variants of a given method, we consider the method as \pseudotested{} (line \ref{is-pseudo}). 
One can notice in line \ref{alg-porig} that all extreme transformations are applied to the original program and are analyzed separately.

\begin{algorithm}[t]
	\begin{algorithmic}[1]
		\Require{Program $P$}
		\Require{Test suite $TS$}
		\Require{Test criterion $TC$}
		\Ensure{$pseudo$: \{\pseudotested{} methods in $P$\}}
		\For{$m \in P in \mathrm{covered}(m,\mathit{TS}) \land \mathrm{ofInterest}(m)$}\label{of-interest}
			\State $variants$ : \{extreme variants of $m$\}
			\If{$\mathrm{returnsValue}(m)$}\label{if-return}
				\State{stripBody($m$)}
				\State{checkReturnType($m$)}
				\State{$variants \leftarrow \mathrm{fixReturnValues}(m)$}
			\Else\label{no-return}
				\State{$variants \leftarrow \mathrm{stripBody}(m)$}
			\EndIf
		\EndFor
		\State{$failure \leftarrow false$}
		\For{$v \in variants$}
			\State{$P' \leftarrow \mathrm{replace}(m,v,P)$}\label{alg-porig}
			\State{$failure \leftarrow failure \lor \mathrm{run}(\mathit{TS},P')$}
			\If{$\lnot failure$}\label{is-pseudo}
				\State{$pseudo \leftarrow pseudo \cup m$}
			\EndIf
		\EndFor
		\Return $pseudo$
	\end{algorithmic}
	\caption{Procedure to detect \pseudotested{} methods}
	\label{alg:detection}
\end{algorithm}

To conduct our study, we have implemented Algorithm \ref{alg:detection} in an open source tool called Descartes\footnote{\url{https://github.com/STAMP-project/pitest-descartes}}. 
The tool can detect \pseudotested{} methods in Java programs tested with a JUnit test suite.
Descartes is developed as an extension of \pit{} \cite{coles_pit_2016}, and ''extreme transformation`` can be seen as extreme mutations, in \pit parlance.
It leverages the maturity of \pit{} and handles the discovery of points where extreme transformations can be applied and the creation of the new program variants \cite{veraperez2018descartes}.
Being open-source, we hope that Descartes  will be used by future research on the topic of \pseudotested{} methods .

\subsection{Evaluation}
\label{subsec:transversal-contributions:descartes:evaluation}

We selected 21 open source projects in a systematic manner to conduct our experiments. 
We considered active projects written in \java{}, that use maven as main build system, \junit{} as the main testing framework and their code is available in a version control hosting service, mostly \gh.

\subsubsection{Frequency of \pseudotested Methods}
\label{subsubsec:transversal-contributions:descartes:evaluation:frequency}

With this evaluation, we aim at characterizing the prevalence of \pseudotested{} methods. 
It is a conceptual replication of the work by \theoriginalauthors, with a larger set of study objects and a different tool support for the detection of \pseudotested{} methods.

We analyzed each study subject following the procedure described in Section \ref{subsec:transversal-contributions:descartes:definitions}. The results are summarized in Table \ref{tab:testedclass}. The second column shows the total number of methods excluding constructors. The third, lists the methods covered by the test suite. The following column shows the ratio of covered methods. The \textit{\#MUA} column shows the number of methods under analysis. The last two columns give the number of \pseudotested{} methods (\#PSEUDO) and their ratio to the methods under analysis (PS\_RATE).

\begin{table}
	\caption{Number of methods in each project, number of methods under analysis and number of \pseudotested{} methods}
	\label{tab:testedclass}
	\def\arraystretch{1}%  1 is the default, change whatever you need
	\setlength\tabcolsep{1.2pt} % default value: 6pt
    \rowcolors{2}{white}{gray!25}
	\begin{tabular}{lrrrrrr}
			\toprule
			Project & {\#Methods} & {\#Covered} & C\_RATE & {\#MUA} & {\#PSEUDO}  & PS\_RATE  \\
			\midrule
			\texttt{authzforce}          &    697 &  325 & 47\% &  291 &   13 &  4\% \\  
			\texttt{aws-sdk-java}        & 177449 & 2314 &  1\% & 1800 &  224 & 12\% \\
			\texttt{commons-cli}         &    237 &  181 & 76\% &  141 &    2 &  1\% \\  
			\texttt{commons-codec}       &    536 &  449 & 84\% &  426 &   12 &  3\% \\  
			\texttt{commons-collections} &   2729 & 1270 & 47\% & 1232 &   40 &  3\% \\  
			\texttt{commons-io}          &    875 &  664 & 76\% &  641 &   29 &  5\% \\  
			\texttt{commons-lang}        &   2421 & 1939 & 80\% & 1889 &   47 &  2\% \\  
			\texttt{flink-core}          &   4133 & 1886 & 46\% & 1814 &  100 &  6\% \\  
			\texttt{gson}                &    624 &  499 & 80\% &  477 &   10 &  2\% \\    
			\texttt{jaxen}               &    958 &  616 & 64\% &  569 &   11 &  2\% \\  
			\texttt{jfreechart}          &   7289 & 3639 & 50\% & 3496 &  476 & 14\% \\  
			\texttt{jgit}                &   6137 & 3702 & 60\% & 2539 &  296 & 12\% \\  
			\texttt{joda-time}           &   3374 & 2783 & 82\% & 2526 &   82 &  3\% \\  
			\texttt{jopt-simple}         &    298 &  265 & 89\% &  256 &    2 &  1\% \\  
			\texttt{jsoup}               &   1110 &  844 & 76\% &  751 &   28 &  4\% \\  
			\texttt{sat4j-core}          &   2218 &  613 & 28\% &  585 &  143 & 24\% \\  
			\texttt{pdfbox}              &   8164 & 2418 & 30\% & 2241 &  473 & 21\% \\  
			\texttt{scifio}              &   3269 &  895 & 27\% &  158 &   72 & 46\% \\  
			\texttt{spoon}               &   4470 & 2976 & 67\% & 2938 &  213 &  7\% \\  
			\texttt{urbanairship}        &   2933 & 2140 & 73\% & 1989 &   28 &  1\% \\  
			\texttt{xwiki-rendering}     &   5002 & 2232 & 45\% & 2049 &  239 & 12\% \\
			\midrule
			Total               & 234923 & 32650 & 14\% & 28808 & 2540 & 9\% \\
			\bottomrule
	\end{tabular}
\end{table}

We have made the first independent replication of \theoriginalauthors's study. 
Our replication confirms that all Java projects contain \pseudotested{} methods, even the very well tested ones. 
This improves the external validity of this empirical fact. 
The ratio of \pseudotested{} methods with respect to analyzed methods ranged from 1\% to 46\% in our dataset.

\subsubsection{Developer's Assessment}
\label{subsubsec:transversal-contributions:descartes:evaluation:developer}

Also, we want to know which \pseudotested{} methods do developers consider worth an additional testing action.
Following our exchange with the developers, we expand the qualitative analysis to a sample of 101 \pseudotested{} methods distributed across three of our study subjects. 
We consulted developers to characterize the \pseudotested{} methods that are worth an additional testing action and the ones that are not worth it.

We contact the development teams directly. 
We select three projects for which  the developers have accepted to discuss with us: \texttt{authzforce}, \texttt{sat4j-core} and \texttt{spoon}. 
We set up a video call with the head of each development team. 
The goal of the call is to present and discuss a selection of \pseudotested{} methods in approximately 90 minutes. 
With this discussion, we seek to know which \pseudotested{} methods developers consider relevant enough to trigger additional work on the test suite and approximate their ratio on each project.

Table \ref{tab:todevelopers} shows the projects involved, footnotes with links to the online summary of the interviews, the number of \pseudotested{} methods included in the random sample, the number of methods worth an additional testing action and the percentage they represent with respect to the sample. We also show how much time we spent in the discussion.

\begin{table}
	\caption{The \pseudotested{} methods systematically analyzed by the lead developers, through a video call.}
	\label{tab:todevelopers}
	\centering
    \rowcolors{2}{white}{gray!25}
	\begin{tabular}{lrrrrrr}
		\toprule
		Project             & Sample size & Worth & Percentage  & Time spent (HH:MM)  \\
		\midrule
		\texttt{authzforce}\footnote{\url{https://github.com/STAMP-project/descartes-experiments/blob/master/actionable-hints/authzforce-core/sample.md}} &  13 (100\%) &                    6 &        46\% & 29 min              \\  
		\texttt{sat4j-core}\footnote{\url{https://github.com/STAMP-project/descartes-experiments/blob/master/actionable-hints/sat4j-core/sample.md}} &  35  (25\%) &                    8 &        23\% & 1 hr 38 min         \\
		\texttt{spoon}\footnote{\url{https://github.com/STAMP-project/descartes-experiments/blob/master/actionable-hints/spoon/sample.md}}      &  53  (25\%) &                   16 &        23\% & 1 hr 14 min         \\
		\midrule
		Total               & 101         &                   30 &        30\% & 3 hr 21 min         \\
		\bottomrule
	\end{tabular}
\end{table}

In a sample of 101 \pseudotested{} methods, systematically analyzed by the lead developers of 3 mature projects, 30 methods (30\%) were considered worth of additional testing actions. 
The developer decisions are based on a deep understanding of the application domain and design of the application. 
This means that it is not reasonable to prescribe the absolute absence (zero) of \pseudotested{} methods.

\subsection{Discussion}
\label{subsec:transversal-contributions:descartes:conclusion}

To conclude, our replication confirms that all Java projects contain \pseudotested{} methods, even the very well tested ones, ranging from 1\% to 46\% in our dataset.
Developers of 3 projects consider that 30\% methods were considered worth of additional testing actions.

In the light of these conclusions, the immediate next step in our research agenda is to investigate an automatic test improvement technique targeted towards \pseudotested{} methods.
This technique shall kill two birds with one stone: improve the adequacy of the test suite for \pseudotested{} methods; 
let the developers focus their efforts on core features and relieve them from the test improvement task.

Descartes has been integrated as default test-criterion in \dspot.
It means that, \dspot amplifies the test suite in order to detect more extreme mutant and thus reduce the number of \pseudotested methods.

\section{Study of Test Generation for Repair}
\label{sec:transversal-contributions:test-for-repair}

Automatic repair aims at fixing bugs automatically.
One family of automatic repair is the test suite based.
This family uses the test suite as an oracle to know whether or not the bug has been fix.
In this transversal contribution, we use test generation process to enhance automatic repair process and one can see it as a test amplification process.
As a perspective of this work, we could use \dspot and a specifically designed test-criterion to improve the outcome of test suite based repair techniques.

\subsection{Problem Statement}
\label{subsec:transversal-contributions:test-for-repair:introduction}

The first role of test suites is to verify that the program behaves as expected.
However, this was without reckoning with daring researchers, test suites have been used for others purpose such as automated program repair.

Automated program repair holds out the promise of saving debugging costs and patching buggy programs more quickly than humans. 
Given this great potential, there has been a surge of research on automated program repair in recent years and several different techniques have been proposed (\cite{genprog,semfix,nopol,tsepei,long2017automatic}).

Test suite based repair is a widely studied family of techniques among many different techniques proposed.
Test suite based repair uses the test suite as oracle to verify whether a patch, obtained using the automated program repair technique, fix the bug or not.
To do so, the test suite has at least one test method that fail and others that pass.
The goal of test suite based repair is to make all the test methods pass, \ie fixes the bug by making the failing test method pass, and does not break others component by keeping other test methods passing (avoiding regression).

However, test suites are in essence input-output specifications and are therefore typically inadequate for completely specifying the expected behavior.
That is to say, that a patch that makes all test methods pass can be still incorrect according to the expected behavior of the program.
The patches that are overly specific to the used test suite and fail to generalize to other tests are called \emph{overfitting} patches (\cite{smith2015cure}).

This study is focused on synthesis-based, a category of test suite based repair technique.
Synthesis-based techniques first use test execution information to build a repair constraint, and then use a constraint solver to synthesize a patch. 
Typical examples in this category include SemFix (\cite{semfix}), Nopol (\cite{nopol}), and Angelix (\cite{Mechtaev:2016:ASM:2884781.2884807}).
Thus, an approach is proposed that try to alleviate overfitting problem for synthesis-based techniques called UnsatGuided.
It makes use of automatic test case generation technique to obtain additional tests and try to solve the overfitting problem.

The remainder of this section is organized as follows:

\subsection{UnsatGuided Technique}
\label{subsec:transversal-contributions:test-for-repair:definitions}

\subsubsection{Definitions}
\label{subsubsec:transversal-contributions:test-for-repair:unsat-guided:definitions}

Let us consider the input domain $I$ of a program $P$.
In a typical repair scenario, the program is almost correct.
There is a bug that only affects a portion of the input domain, called the ``buggy input domain'' $I_{bug}$.
We call the rest of the input domain, considered correct, $I_{correct}$. 
By definition, a patch changes the behaviors of a portion of the input domain.
This portion is called $I_{patch}$.

\subsubsection{Algorithm}
\label{subsubsec:transversal-contributions:test-for-repair:unsat-guided:algo}

The overfitting problem for synthesis-based repair techniques arises because the repair constraint established using an incomplete test suite is not strong enough to fully express the intended semantics of a program.
The idea is to strengthen the initial repair constraint by augmenting the initial test suite with additional automatically generated tests. 
A stronger repair constraint would guide synthesis-based repair techniques towards better patches, \ie patches that are correct or at least suffer less from overfitting.

UnsatGuided is proposed to overcome this problem, which gradually makes use of the new information provided by each automatically generated test to build a possibly stronger final repair constraint. 
The key underlying idea is that if the additional repair constraint enforced by an automatically generated test has logical contradictions with the repair constraint established so far, then the generated test is likely to have its input points lying in $I_{bug}$
Such tests are called ``bug-exposing test'' and are discarded, the others are used to strengthen the repair constraints.

\autoref{alg:2} describes the approach in detail. 
It takes as input a buggy program \emph{P} to be repaired, a manually written test suite \emph{TS} which contains some passing tests and at least one failing test, a synthesis-based repair technique $T_{synthesis}$, a time budget \emph{TB} allocated for the execution of $T_{synthesis}$, and finally an automatic test case generation tool $T_{auto}$ which uses a certain kind of automatic test case generation technique $T_{reg}$. 
The output of the algorithm is a patch \emph{pt} to the buggy program \emph{P}.

\begin{algorithm}[t]
	\begin{algorithmic}[1]
		\Require{A buggy program $P$ and its manually written test suite $TS$}
		\Require{A synthesis-based repair technique $T_{synthesis}$ and the time budget $TB$}
		\Require{An automatic test case generation tool $T_{auto}$}
		\Ensure{A patch $pt$ to the buggy program $P$}
		\State{$pt_{initial} \leftarrow T_{synthesis}(P, TS, TB)$}
		\If{$pt_{initial} = null$}
			\State{$pt \leftarrow null$}
		\Else
			\State{$AGTS \leftarrow \emptyset$}
			\State{$pt \leftarrow pt_{initial}$}
			\State{$TS_{aug} \leftarrow TS$}
			\State{$t_{initial} \leftarrow getPatchGenTime(T_{synthesis}(P, TS, TB))$}
			\State{$\{file_{i}\}(i=1,2,...,n) \leftarrow getInvolvedFiles(pt_{initial})$ }
			\For{$i=1$ to $n$}
				\State{$AGTS \leftarrow AGTS \cup T_{auto}(P, file_{i})$}
			\EndFor
			\For{$j=1$ to $|AGTS|$}
				\State{$t_j \leftarrow AGTS(j)$}
				\State{$TS_{aug} \leftarrow TS_{aug} \cup \{t_j\}$}
				\State{$pt_{intern} \leftarrow T_{synthesis}(P, TS_{aug}, t_{initial} \times 2)$}
				\If{$pt_{intern} \neq null$}
					\State{$pt \leftarrow pt_{intern}$}
				\Else
					\State{$TS_{aug} \leftarrow TS_{aug} - \{t_j\} $}
				\EndIf
			\EndFor
		\EndIf
		\Return{$pt$}
	\end{algorithmic}
	\caption{: Algorithm for the Proposed Approach UnsatGuided}
	\label{alg:2}
\end{algorithm}

The algorithm directly returns an empty patch if $T_{synthesis}$ generates no patches within the time budget (lines 2-3).
In case $T_{synthesis}$ generates an initial patch $pt_{initial}$, the algorithm first conducts a set of initialization steps as follows: 
it sets the automatically generated test suite \emph{AGTS} to be an empty set (line 5), sets the returned patch \emph{pt} to be the initial patch $pt_{initial}$ (line 6), sets the augmented test suite $TS_{aug}$ to be the manually written test suite \emph{TS} (line 7), and gets the time used by $T_{synthesis}$ to generate the initial patch $pt_{initial}$ and sets $t_{initial}$ to be the value (line 8).
\autoref{alg:2} then identifies the set of files \{$file_i$\}(\emph{i}=1, 2,..., \emph{n}) involved in the initial patch $pt_{initial}$ (line 9) and for each identified file, it uses the automatic test case generation tool $T_{auto}$ to generate a set of tests that target behaviors related with the file and adds the generated tests to the automatically generated test suite \emph{AGTS} (lines 10-12). 

Next, the algorithm will use the test suite \emph{AGTS} to refine the initial patch $pt_{initial}$. 
For each test $t_j$ in the test suite \emph{AGTS} (line 14), the algorithm first adds it to the augmented test suite $TS_{aug}$ (line 15) and runs technique $T_{synthesis}$ with test suite $TS_{aug}$ and new time budget $t_{initial} \times 2$ against program \emph{P} (line 16).
The new time budget is used to quickly identify tests that can potentially contribute to strengthening the repair constraint, and thus improve the scalability of the approach. 
Then, if the generated patch $pt_{intern}$ is not an empty patch, the algorithm updates the returned patch \emph{pt} with $pt_{intern}$ (lines 17-18). 
In other words, the algorithm deems test $t_j$ as a good test that can help improve the repair constraint. 
Otherwise, test $t_j$ is removed from the augmented test suite $TS_{aug}$ (lines 19-20) as $t_j$ is either a bug-exposing test or it slows down the repair process too much. 
After the above process has been completed for each test in the test suite \emph{AGTS}, the algorithm finally returns patch \emph{pt} as the desirable patch (line 24). 

\subsection{Evaluation}
\label{subsec:transversal-contributions:test-for-repair:evaluations}

Defects4J (\cite{JustJE2014}) has been selected, a known database of real faults from real-world Java programs, as the experimental benchmark.
For the approach UnsatGuided to be implemented, Nopol~\cite{nopol} has been chosen to represent synthesis-based repair techniques. 
The automatic test case generation tool used in this study is EvoSuite~\cite{ESECFSE11}.

We evaluate the effectiveness of UnsatGuided from two points: its impact on the overfitting issue and correctness of the original patch generated by Nopol.

\autoref{tab:nopol-results} displays the experimental results on combining Nopol with UnsatGuided (hereafter referred to as Nopol+\-Unsat\-Guided).
This table only shows the Defects4J bugs that can be originally repaired by Nopol, and their identifiers are listed in column \emph{Bug ID}. 

The test generation results by running EvoSuite are shown in the two columns under the column \emph{Tests}, among which the \emph{\#EvoTests} column shows the total number of tests generated by EvoSuite for all seeds and the \emph{\#Bug-expo} column shows the number of bug-exposing tests among all of the generated tests. 

The results obtained by running just Nopol are shown in the columns under the column \emph{Nopol}. 
The \emph{Time} column shows the time used by Nopol to generate the initial patch. 
The \emph{incomplete fix (\#failing)} column shows what is the overfitting issue of incomplete fixing for the original Nopol patch. 
Each cell in this column is of the form X (Y), where X can be ``Yes'' or ``No'' and Y is a digit number. 
The ``Yes'' and ``No'' mean that the original Nopol patch has and does not have overfitting issue of incomplete fixing respectively. 
The digit number in parentheses shows the number of bug-exposing tests on which the original Nopol patch fails. 
Similarly, the \emph{regression (\#failing)} column tells what is the overfitting issue of regression introduction for the original Nopol patch, and each cell in this column is of the same form with the column \emph{incomplete fix (\#failing)}. 
The ``Yes'' and ``No'' for this column mean that the original Nopol patch has and does not have overfitting issue of regression introduction respectively. 
The digit number in parentheses shows the number of normal tests on which the original Nopol patch fails. 
Finally, the column \emph{correctness} shows whether the original Nopol patch is correct, with ``Yes'' representing correct and ``No'' representing incorrect.

\input{tab-result-nopol2.tex}

This study aims to assess the effectiveness of UnsatGuided. 
It can be seen from the column \emph{Change ratio (\#unique)} of \autoref{tab:nopol-results} that for the 42 buggy versions that can be initially repaired by Nopol, the patches generated for 34 buggy versions have been changed at least for one seed after running Nopol+UnsatGuided. 
If we consider all executions (one per seed per buggy version), we obtain a total of 1220 patches with Nopol+UnsatGuided. 
Among the 1220 patches, 702 patches are different from the original patches generated by running Nopol only. 
Thus, UnsatGuided can significantly impact the output of the Nopol repair process. 
We will further investigate the quality difference between the new Nopol+UnsatGuided patches and the original Nopol patches. 

The results for alleviating the two kinds of overfitting issues by running Nopol+UnsatGuided are displayed in the columns \emph{fix completeness change (Avg \#Removedinc)} and \emph{regression change (Avg\#Removedreg)} of \autoref{tab:nopol-results}.

With regard to alleviating the overfitting issue of incomplete fixing, we can see from the column \emph{fix completeness change (Avg\#Removedinc)} that UnsatGuided has an effect on 4 buggy program versions (Math\_50, Math\_80, Math\_87 and Time\_4). 
For all those 4 buggy versions, the original Nopol patch already has the overfitting issue of incomplete fixing. 
With UnsatGuided, the overfitting issue of incomplete fixing has been alleviated in 2 cases (Math\_50, Time\_4) and worsened for 2 other cases (Math\_80, Math\_87). 
This means UnsatGuided is likely to have a minimal positive impact on alleviating overfitting issue of incomplete fixing and can possibly have a negative impact on it.

In terms of alleviating overfitting issue of regression introduction, we can see from the column \emph{regression change (Avg\#Removedreg)} that UnsatGuided has an effect on 18 buggy program versions. 
Among the 18 original Nopol patches for these 18 buggy program versions, UnsatGuided has alleviated the overfitting issue of regression introduction for 16 patches. 
In addition, for 6 buggy program versions, the overfitting issue of regression introduction of the original Nopol patch has been completely removed.  
These 6 cases are indicated with $(\star)$ in \autoref{tab:nopol-results}. 
Meanwhile, UnsatGuided worsens the overfitting issue of regression introduction for two other original Nopol patches (Math\_33 and Time\_7). 
It can possibly happen as even though the repair constraint for input points within $I_{correct}$ has been somewhat strengthened (but not completely correct), yet the solution of the constraint happens to be more convoluted.
Overall, with 16 positive versus 2 negative cases, UnsatGuided can be considered as effective in alleviating overfitting issue of regression introduction. 

\subsection{Discussion}
\label{subsec:transversal-contributions:test-for-repair:conclusion}

To sum up, UnsatGuided can effectively alleviate the overfitting issue of regression introduction (16/19 cases), but has minimal positive impact on reducing the overfitting issue of incomplete fixing.

In this study, we used Evosuite, a state-of-the-art test generation tool.
An alternative would be to use a test amplification tool, such as \dspot in order to overcome the overfitting problems.

\dspot could amplify the test suite, while discarded bug-exposing amplified test methods, and improve the constraint around the program repair problem.
This can be implemented as a specific test-criterion.
Thus, some input-amplification operators are well suited to this task such as literals modifications.

\section{Conclusion}
\label{sec:transversal-contributions:conclusion}

In this chapter, I exposed 3 transversal contributions that I made thanks to the skills that I developed during my thesis.
I worked with my researchers colleagues on side topics yet linked to this thesis.

First, the study of program correctness under runtime perturbation.
It highlighted the existence of the correctness attraction phenomenon.
This first work could be used jointly with \dspot, by integrating the correctness ratio as a test-criterion in order to strengthen the ability of the test suite to detect more runtime perturbation.

Second, the study of pseudo-tested methods and extreme mutations.
It showed the prevalence of pseudo-tested methods, all the tests pass even if the whole behavior, \ie body, of such methods is removed.
This extreme mutations has been already implemented in \dspot as a test-criterion in order to reduce the number of pseudo-tested methods.

Third, the study of patch overfitting in test-based repair techniques and an approach to overcome it.
In this study, we used a test generation tool but using \dspot with a dedicated and specifically designed test-criterion could give a different outcome.

The next chapter gives the short- and long-term perspectives and concludes this thesis.