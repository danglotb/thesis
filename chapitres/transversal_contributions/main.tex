\chapter{Transversal Contributions}
\label{chap:transversal-contributions}

\begin{chaptersummary}
	In this chapter, I expose 3 transversal contributions thanks to specific skills obtained during my thesis.
	\begin{itemize}
		\item \cite{Danglot2018} The first transversal contribution studies the correctness of program under runtime perturbations.
		\item The second transversal contribution studies extreme mutants, \eg a subclass of mutants such as done in mutation analysis, in open-source project. 
		\item The third transversal contribution studies the overfitting of patches in test-based automatic repair techniques and how to overcome using test generation.
	\end{itemize}
	These 3 transversal contributions are supported by published articles.
\end{chaptersummary}

\minitoc

\graphicspath{{.}{chapitres/state-of-the-art/}}

\section{Study of Program Correctness}
\label{sec:transversal-contributions:correctness}

\subsection{Introduction}
\label{subsec:transversal-contributions:correctness:introduction}

Recalling that in \autoref{chap:introduction}, I mentioned a quote of Dijkstra:
\begin{center}
	\emph{``the smallest possible perturbations - \ie changes of a single bit - can have the most drastic consequences.''.}
\end{center}

This first study aims at verifying this statement is true or not.
Dijkstra considers software as a system that is in an unstable equilibrium, or to put it more precisely, that the correctness of a program output is unstable with respect to perturbations.
However, previous works, \eg\cite{Rinard:2005:EAE:1094855.1094866,Li2007Correctness}) suggest the opposite, \ie suggest that programs can accommodate perturbations.

In the context of my thesis, the correctness of programs relies on test suite.
However, the incompleteness of test suites make them poor oracles to verify the truthfulness of the Dijkstra's hypothesis.

This is why I devise \perturb, an experimental protocol to study the stability of program correctness under execution perturbation.
It consists in perturbing the execution of programs according to a perturbation model and observing whether this has an impact on the correctness of the output.
The two observable different outcomes are: 
the perturbation breaks the computation and results in an incorrect output (unstable under perturbation), 
or the correctness of the output is stable despite the perturbation.
This protocol has two key-attributes:

1) It is based on perfect oracle, \ie it verifies the output of the perturbed program is completly correct, \ie bit-wise equals to the output of the unpertubed, or original, program;

2) It explores exhaustively in time and space the perturbation envelop of software.

The remaining of this section is organized as follow: 
first, I present \perturb protocol in \autoref{subsec:transversal-contributions:correctness:protocol}.
second, I present the experimentation that has been carried out in \autoref{subsec:transversal-contributions:correctness:experimentation};
and eventually, \autoref{subsec:transversal-contributions:correctness:conclusion} conclude this section.

\subsection{\perturb protocol}
\label{subsec:transversal-contributions:correctness:protocol}

To actually perform perturbations, \perturb adds \textbf{perturbation points} to the program under study where a \textbf{perturbation point} (denoted $pp$) is an expression of a given data type.
For instance, if one perturbs integer values, the \textbf{perturbation points} will be all integer expressions (literals as well as compound expressions). 
In \autoref{lst:example_pp_underbrace}\footnote{$|$ is the bitwise or operator. $>>$ is the binary right shift operator. The assignment $|=$ is the bitwise or operator between the left operand and the right operand, then the result is affected to the left operand.}, there are 3 potential integer perturbation points in a single statement, indicated with braces.
\begin{lstlisting}[basicstyle=\small, caption=Three integer \textbf{perturbation points} in a single statement., label=lst:example_pp_underbrace]
acc |= (*@$\underbrace{i}$@*) >> (*@$\underbrace{mask}$@*);
								(*@$pp_1$@*)     (*@$pp_2$@*)
acc |= (*@$\underbrace{i >> mask}$@*);
										(*@$pp_3$@*)  
\end{lstlisting}

\perturb statically locates \textbf{perturbation points} and automatically adds perturbation code around them using a code transformation. 
The transformation consists of wrapping all compatible expressions into a function call $p$ (for ``perturb'')\footnote{In the experimentation, it is implemented on Java programs using the Spoon transformation library \cite{pawlak:hal-01169705}.}.

\begin{lstlisting}[basicstyle=\small, caption=The same statement with perturbation code injected., label=lst:example_pp]
acc |= p(3, p(1,i) >> p(2,mask));
\end{lstlisting}

In \autoref{lst:example_pp}, each integer expression of \autoref{lst:example_pp_underbrace} is wrapped into a call to function $p$.
Function $p$ takes two parameters: a unique index to identify the perturbation point and the expression being perturbed.
If $p$ returns  the second parameter, the transformed program is semantically equivalent to the original program. 
The identifier argument enables \perturb to perturb only one location at a time. 
In this example, this identifier ranges from 1 to 3 corresponding to the index given in \autoref{lst:example_pp_underbrace} under perturbation point $pp$.

\subsubsection{Core Algorithm}
\label{subsubsec:transversal-contributions:correctness:protocol:algo}

The goal of this algorithm is to systematically explore the perturbation space. 
\hyperref{alg:systexplor} first records the number of executions of each perturbation point for each input in a matrix $R_{ref}$ (for reference run) without injecting any perturbation.
$R_{ref}[pp,i]$ refers to the number of executions of perturbation point $pp$ for a given input $i$.
Then, it re-executes the program for each input, with one perturbation for each point so that each point is perturbed at least and at most once per input.
The oracle asserts the correctness of the perturbed execution (output $o$) for the given input ($i$).
A perturbed execution can have three outcomes:
a success, meaning that the correctness oracle validates the output;
a failure meaning that the correctness oracle is violated (also called an oracle-broken execution);
a runtime error (an exception in Java) meaning that the perturbation has produced a state for which the computation becomes impossible at some point (\eg a division-by-zero).

This algorithm performs a systematic exploration of the perturbation space for a given program and a set of inputs according to a perturbation model.

\begin{algorithm}[h]
	\algorithmicrequire{ \\
		$prog$: program,\\
		$model$: perturbation model,\\
		$I$: set of inputs for program $prog$,\\
		$oracle$: a perfect oracle for program $prog$}\\
	\algorithmicensure{ \\
		$exc$: counters of execution per perturbation point,\\
		$s$: counters of success per perturbation point,\\
		$ob$: counters of oracle broken per perturbation point}
	\begin{algorithmic}
		\State{instrument($prog$)}
		\For{each input i in I} 
			\For{perturbation point $pp$ in $prog$}
				\State $R_{ref}[,i] \leftarrow runWithoutPerturbation(prog, i)$
				\For{$j = 0$, to $ R_{ref}[pp,i]$}
					\State $o \leftarrow runWithPerturbationAt(prog,model,i,pp,j)$
					\If{exception is thrown}
						\State {$exc[pp] \leftarrow exc[pp] + 1$} 
					\ElsIf{$oracle.assert(i,o)$} 
						\State {$s[pp] \leftarrow s[pp] + 1$} 
					\Else 
						\State {$ob[pp] \leftarrow ob[pp] + 1$}
					\EndIf
				\EndFor
			\EndFor
		\EndFor
	\end{algorithmic}
	\caption{Core Algorithm of \perturb.}
	\label{alg:systexplor}
\end{algorithm}

In \autoref{alg:systexplor}, $R_{ref}[,i]$ denotes column $i$ of matrix $R_{ref}$. 
The statement \emph{runWithoutPerturbation(prog, i)} returns a column vector which is assigned to the $i\thb$ column of matrix $R_{ref}$; each element is one perturbation point: it contains the number of times each perturbation point is executed in the program \emph{prog} for each input \emph{i}. 
On the other hand, the statement \emph{runWithPerturbationAt(prog, model, i, pp, j)} runs the program \emph{prog} while using the perturbation model \emph{model}, the perturbation point \emph{pp} at its $j\thb$ execution for the given input \emph{i}.

\subsection{Experimentation}
\label{subsec:transversal-contributions:correctness:experimentation}

The experimentation with a dataset of 10 programs. 
This dataset has been created following this methodology:
first and foremost, the programs can be specified with a perfect oracle;
second, they are written in Java;
third, they come from diverse application domains in order to maximize external validity.
The resulting programs are summarized in \autoref{tab:description-dataset}. 
The first column displays the name of the program; 
the second is the number of lines of code; 
the third is a short description of the purpose of the program; 
the last column describes the perfect oracle used to evaluate the correctness of the output.

\subsubsection{The PONE Experiment}
\label{subsubsec:transversal-contributions:correctness:experimentation:PONE}

I now present the PONE experiment. 
Its goal is to explore correctness attraction according to increments ($+ 1$) of integer values at runtime.

Point that a single perturbation always breaks the output correctness are qualified as \textbf{fragile} because a single perturbation at runtime breaks the whole computation.
Other points that can be systematically perturbed without any consequence on the correctness of the final output of the program are qualified as \textbf{antifragile} (in opposition to fragile).
The remainder are in between; those with a correctness ratio larger or equal than 75\% are qualified as \textbf{robust}.

In the PONE experiment, integer expressions are perturbed.
The PONE perturbation model is a small perturbation on an integer expression: a single increment of an integer value only once during an execution. 
An equivalently small perturbation model is MONE consisting of decrementing integers. 
An experimentation as been also performed using MONE, however, the results are not reported in this thesis.
For more information, see the dedicated article.\cite{Danglot2018}

\begin{table}[h]
	\caption{PONE Results. The correctness ratio may not correspond directly to the number of Antifragile and Robust expressions because it is computed over all executions. Some points are executed much more than others, as explained in \autoref{subsec:realization}.}
	\label{tab:results-PONE}
	\def\arraystretch{0.55}%  1 is the default, change whatever you need
	\setlength\tabcolsep{0.45pt} % default value: 6pt
	\centering\begin{tabular}{lllllll}
		Subject & $N^{int}_{pp}$ & |Search space| & \# Fragile exp. & \#Robust exp.& \# Antifragile exp. & Correctness ratio\\
		\hline
		quicksort&41&151444&6&10&19&\textendash\textendash\textendash\textendash\textendash\textendash\textendash { }  77 \%\\
		zip&19&38840&5&2&5&\textendash\textendash\textendash\textendash\textendash\textendash\textendash { }  76 \%\\
		sudoku&89&98211&12&27&8&\textendash\textendash\textendash\textendash\textendash\textendash { }  68 \%\\
		md5&164&237680&102&24&7&\textendash\textendash { }  29 \%\\
		rsa&117&2576&55&8&32&\textendash\textendash\textendash\textendash\textendash { }  54 \%\\
		rc4&115&165140&60&7&12&\textendash\textendash\textendash { }  38 \%\\
		canny&450&616161&58&129&133&\textendash\textendash\textendash\textendash\textendash\textendash\textendash\textendash\textendash { }  94 \%\\
		lcs&79&231786&10&47&13&\textendash\textendash\textendash\textendash\textendash\textendash\textendash\textendash { }  89 \%\\
		laguerre&72&423454&15&24&15&\textendash\textendash\textendash\textendash\textendash\textendash\textendash\textendash\textendash { }  90 \%\\
		linreg&75&543720&43&18&11&\textendash\textendash\textendash\textendash { }  47 \%\\
		total & 1221&2509012&366&296&255&\textendash\textendash\textendash\textendash\textendash\textendash { }  66 \%
	\end{tabular}
\end{table}

\autoref{tab:results-PONE} gives the results of the systematic exploration of the PONE perturbation space.
For each subject, this table gives:
the number of integer perturbation points $N^{int}_{pp}$;
the number of perturbed executions (equal to the size of the PONE perturbation space);
the number of \textbf{fragile} integer expressions;
the number of \textbf{robust} integer expressions;
the number of \textbf{antifragile} integer expressions;
the \textbf{correctness ratio} (percentage of correct outputs) over all perturbed executions.

\begin{mdframed}
To sum up, the main conclusions of the PONE experiment are:
\begin{itemize}
	\item The considered programs are perturbable according to the PONE perturbation model.
	\item There are very few fully fragile integer expressions in the considered programs.
	\item There is a majority of highly perturbable integer expressions which results in a high level of correctness attraction.
	\item Dijkstra's view that software is fragile is not always true, correctness is rather a stable equilibrium than an unstable one.
\end{itemize}
\end{mdframed}

\subsection{Conclusion}
\label{subsec:transversal-contributions:correctness:conclusion}

I have devised a protocol called \perturb to study the stability of programs under perturbation.
\perturb exhaustively explores the perturbation space of a given program for a set of inputs according to a perturbation model.
An experimentation have been conducted on 10 subjects using the PONE perturbation models.
In total, 2509012 perturbed executions have been performed and studied, which makes it one of the largest perturbation experiments ever made.
From this experimentation, the presence of ``correctness attraction'' has been observed. Over all perturbed execution, 66\% of them do not break the correctness of the output. 

Studying correctness attraction can have divers applicability.
One of them is to identify points that can be randomized and protect the software from external and malicious attacks.
Also, if one could engineer techniques to automatically improve correctness attraction, in order to obtain zones that accommodate more perturbations of the runtime state, and those zones could be deemed ``bug absorbing zones''.

To conclude, I imagine two ways to combine both \dspot and \perturb:
First, using \perturb as a test-criterion to amplify test suites, \ie \dspot would keep amplified test methods that detect more perturbations than the original test suite.
Second, in \perturb, I used perfect oracles.
The problem they have been manually devised, and this could be approximated using the test suite.
To strengthen it, one could amplifies its test suite with \dspot to have a better approximation of the perfect oracle and thus study deeper the correctness of its program.

\section{Study of Pseudo-tested Methods with Extreme Mutations}
\label{sec:transversal-contributions:descartes}
\cite{descartes}

\subsection{Introduction}
\label{subsec:transversal-contributions:descartes:introduction}

Niedermayr and colleagues \cite{niedermayr_will_2016} recently introduced the concept of \emph{\pseudotested{} methods}. 
These methods are covered by the test suite, but no test case fails even if all behaviors of the method are removed at once, \ie when the body is completely stripped off.
This work is novel and intriguing:
such \pseudotested{} methods are present in all projects, even those with test suites that have high coverage ratio.

If those results hold, it calls for more research on this topic.
This is the motivation of this paper: 
first, we challenge the external validity of Niedermayr \etal's experiment with new study subjects, 
second we perform an in-depth qualitative empirical study of \pseudotested{} methods. 
In particular, we want to determine if \pseudotested{} methods are indicators of badly tested code. 
While this seems to be intuitively true, we aim at quantifying this phenomenon.
Second, we want to know whether \pseudotested{} methods are relevant indicators for developers who wish to improve their test suite. 
In fact, these methods may encapsulate behaviors that are poorly specified by the test suite, but are not relevant functionalities for the project.

\subsection{Definition and Implementation}
\label{subsec:transversal-contributions:descartes:definitions}

Let $P$ be a program and $m$ be a method; $S=\cup_{m \in P}{\mathit{effects}(m)}$ the set of effects of all methods in $P$; 
$\mathit{effects}(m)$ a function $\mathit{effects}:P \rightarrow S$ that returns all the effects of a method $m$; $detect$, a predicate $TS \times S \rightarrow \{\top, \bot\}$ that determines if an effect is detected by $TS$.
Here, we consider the following possible effects that a method can produce: 
change the state of the object on which it is called, change the state of other objects (by calling other methods), return a value as a result of its computation.

\begin{definition}
	\label{def:pseudo}
	A method is said to be \pseudotested{} with respect to a test suite, if the test suite covers the method and does not assess any of its effects.
\end{definition}

A ``pseudo-tested'' method, as defined previously, is an idealized concept.
We now describe an algorithm that implements a practical way of collecting a set of \pseudotested{} methods in a program $P$, in the context of the test suite $TS$, based on the original proposal of \theoriginalauthors. 
It relies on the idea of ``extreme code transformations'', which consists in completely stripping out the body of a method.

Algorithm \autoref{alg:detection} starts by analyzing all methods of $P$ that are covered by the test suite and fulfill a predefined selection criterion (predicate $\mathrm{ofInterest}$ in line \ref{of-interest}).
This critetion is based on the structure of the method and aims at reducing the number of false positives detected by the procedure. 
It eliminates uninteresing methods such as trivial setter and getters or empty void methods.
If the method returns a value, the body of the method is stripped out and we generate a few variants that simply return predefined values (line \ref{if-return}).\footnote{Compared to \theoriginalauthors, we add two new transformations, one to return $null$ and another to return an empty array. These additions allow to expand the scope of methods to be analyzed.}
If the method is void, we strip the body without further action (line \ref{no-return}).
Once we have a set of variants, we run the test suite on each of them, if no test case fails on any of the variants of a given method, we consider the method as \pseudotested{} (line \ref{is-pseudo}). 
One can notice in line \ref{alg-porig} that all extreme transformations are applied to the original program and are analyzed separately.

\begin{algorithm}[t]
	\begin{algorithmic}[1]
		\Require{Program $P$}
		\Require{Test suite $TS$}
		\Require{Test criterion $TC$}
		\Ensure{$pseudo$: \{\pseudotested{} methods in $P$\}}
		\For{$m \in P in \mathrm{covered}(m,\mathit{TS}) \land \mathrm{ofInterest}(m)$}\label{of-interest}
			\State $variants$ : \{extreme variants of $m$\}
			\If{$\mathrm{returnsValue}(m)$}\label{if-return}
				\State{stripBody($m$)}
				\State{checkReturnType($m$)}
				\State{$variants \leftarrow \mathrm{fixReturnValues}(m)$}
			\Else\label{no-return}
				\State{$variants \leftarrow \mathrm{stripBody}(m)$}
			\EndIf
		\EndFor
		\State{$failure \leftarrow false$}
		\For{$v \in variants$}
			\State{$P' \leftarrow \mathrm{replace}(m,v,P)$}\label{alg-porig}
			\State{$failure \leftarrow failure \lor \mathrm{run}(\mathit{TS},P')$}
			\If{$\lnot failure$}\label{is-pseudo}
				\State{$pseudo \leftarrow pseudo \cup m$}
			\EndIf
		\EndFor
		\Return $pseudo$
	\end{algorithmic}
	\caption{Procedure to detect \pseudotested{} methods}
	\label{alg:detection}
\end{algorithm}

To conduct our study, we have implemented Algorithm \ref{alg:detection} in an open source tool called Descartes\footnote{\url{https://github.com/STAMP-project/pitest-descartes}}. 
The tool can detect \pseudotested{} methods in Java programs tested with a JUnit test suite.
Descartes is developed as an extension of \pit{} \cite{coles_pit_2016}, and ''extreme transformation`` can be seen as extreme mutations, in \pit parlance.
It leverages the maturity of \pit{} and handles the discovery of points where extreme transformations can be applied and the creation of the new program variants \cite{veraperez2018descartes}.
Being open-source, we hope that Descartes  will be used by future research on the topic of \pseudotested{} methods .

\subsection{Evaluation}
\label{subsec:transversal-contributions:descartes:evaluation}

We selected 21 open source projects in a systematic manner to conduct our experiments. 
We considered active projects written in \java{}, that use maven as main build system, \junit{} as the main testing framework and their code is available in a version control hosting service, mostly \gh.

\subsubsection{Frequency of \pseudotested Methods}
\label{subsubsec:transversal-contributions:descartes:evaluation:frequency}

With this evaluation, we aim at characterizing the prevalence of \pseudotested{} methods. 
It is a conceptual replication of the work by \theoriginalauthors, with a larger set of study objects and a different tool support for the detection of \pseudotested{} methods.

We analyzed each study subject following the procedure described in Section \ref{subsec:transversal-contributions:descartes:definitions}. The results are summarized in Table \ref{tab:testedclass}. The second column shows the total number of methods excluding constructors. The third, lists the methods covered by the test suite. The following column shows the ratio of covered methods. The \textit{\#MUA} column shows the number of methods under analysis, per the criteria described in Section \ref{sec:metrics}. The last two columns give the number of \pseudotested{} methods (\#PSEUDO) and their ratio to the methods under analysis (PS\_RATE).

\begin{table}
	\caption{Number of methods in each project, number of methods under analysis and number of \pseudotested{} methods}
	\label{tab:testedclass}
	\def\arraystretch{1}%  1 is the default, change whatever you need
	\setlength\tabcolsep{1.2pt} % default value: 6pt
	\begin{tabular}{rrrrrrr}
			\hline
			Project & {\#Methods} & {\#Covered} & C\_RATE & {\#MUA} & {\#PSEUDO}  & PS\_RATE  \\
			\hline
			\texttt{authzforce}          &    697 &  325 & 47\% &  291 &   13 &  4\% \\  
			\texttt{aws-sdk-java}        & 177449 & 2314 &  1\% & 1800 &  224 & 12\% \\
			\texttt{commons-cli}         &    237 &  181 & 76\% &  141 &    2 &  1\% \\  
			\texttt{commons-codec}       &    536 &  449 & 84\% &  426 &   12 &  3\% \\  
			\texttt{commons-collections} &   2729 & 1270 & 47\% & 1232 &   40 &  3\% \\  
			\texttt{commons-io}          &    875 &  664 & 76\% &  641 &   29 &  5\% \\  
			\texttt{commons-lang}        &   2421 & 1939 & 80\% & 1889 &   47 &  2\% \\  
			\texttt{flink-core}          &   4133 & 1886 & 46\% & 1814 &  100 &  6\% \\  
			\texttt{gson}                &    624 &  499 & 80\% &  477 &   10 &  2\% \\    
			\texttt{jaxen}               &    958 &  616 & 64\% &  569 &   11 &  2\% \\  
			\texttt{jfreechart}          &   7289 & 3639 & 50\% & 3496 &  476 & 14\% \\  
			\texttt{jgit}                &   6137 & 3702 & 60\% & 2539 &  296 & 12\% \\  
			\texttt{joda-time}           &   3374 & 2783 & 82\% & 2526 &   82 &  3\% \\  
			\texttt{jopt-simple}         &    298 &  265 & 89\% &  256 &    2 &  1\% \\  
			\texttt{jsoup}               &   1110 &  844 & 76\% &  751 &   28 &  4\% \\  
			\texttt{sat4j-core}          &   2218 &  613 & 28\% &  585 &  143 & 24\% \\  
			\texttt{pdfbox}              &   8164 & 2418 & 30\% & 2241 &  473 & 21\% \\  
			\texttt{scifio}              &   3269 &  895 & 27\% &  158 &   72 & 46\% \\  
			\texttt{spoon}               &   4470 & 2976 & 67\% & 2938 &  213 &  7\% \\  
			\texttt{urbanairship}        &   2933 & 2140 & 73\% & 1989 &   28 &  1\% \\  
			\texttt{xwiki-rendering}     &   5002 & 2232 & 45\% & 2049 &  239 & 12\% \\
			\hline
			Total               & 234923 & 32650 & 14\% & 28808 & 2540 & 9\% \\
			\hline
	\end{tabular}
\end{table}

We have made the first independent replication of \theoriginalauthors's study. 
Our replication confirms that all Java projects contain \pseudotested{} methods, even the very well tested ones. 
This improves the external validity of this empirical fact. 
The ratio of \pseudotested{} methods with respect to analyzed methods ranged from 1\% to 46\% in our dataset.

\subsubsection{Developer's Assessment}
\label{subsubsec:transversal-contributions:descartes:evaluation:developer}

Also, we want to know which \pseudotested{} methods do developers consider worth an additional testing action.
Following our exchange with the developers, we expand the qualitative analysis to a sample of 101 \pseudotested{} methods distributed across three of our study subjects. 
We consulted developers to characterize the \pseudotested{} methods that are worth an additional testing action and the ones that are not worth it.

We contact the development teams directly. 
We select three projects for which  the developers have accepted to discuss with us: \texttt{authzforce}, \texttt{sat4j-core} and \texttt{spoon}. 
We set up a video call with the head of each development team. 
The goal of the call is to present and discuss a selection of \pseudotested{} methods in approximately 90 minutes. 
With this discussion, we seek to know which \pseudotested{} methods developers consider relevant enough to trigger additional work on the test suite and approximate their ratio on each project.

Table \ref{tab:todevelopers} shows the projects involved, footnotes with links to the online summary of the interviews, the number of \pseudotested{} methods included in the random sample, the number of methods worth an additional testing action and the percentage they represent with respect to the sample. We also show how much time we spent in the discussion.

\begin{table}
	\caption{The \pseudotested{} methods systematically analyzed by the lead developers, through a video call.}
	\label{tab:todevelopers}
	\centering
	\begin{tabular}{lrrrrrr}
		\toprule
		Project             & Sample size & Worth & Percentage  & Time spent (HH:MM)  \\
		\midrule
		\texttt{authzforce}\footnote{\url{https://github.com/STAMP-project/descartes-experiments/blob/master/actionable-hints/authzforce-core/sample.md}} &  13 (100\%) &                    6 &        46\% & 29 min              \\  
		\texttt{sat4j-core}\footnote{\url{https://github.com/STAMP-project/descartes-experiments/blob/master/actionable-hints/sat4j-core/sample.md}} &  35  (25\%) &                    8 &        23\% & 1 hr 38 min         \\
		\texttt{spoon}\footnote{\url{https://github.com/STAMP-project/descartes-experiments/blob/master/actionable-hints/spoon/sample.md}}      &  53  (25\%) &                   16 &        23\% & 1 hr 14 min         \\
		\midrule
		Total               & 101         &                   30 &        30\% & 3 hr 21 min         \\
		\bottomrule
	\end{tabular}
\end{table}

In a sample of 101 \pseudotested{} methods, systematically analyzed by the lead developers of 3 mature projects, 30 methods (30\%) were considered worth of additional testing actions. 
The developer decisions are based on a deep understanding of the application domain and design of the application. 
This means that it is not reasonable to prescribe the absolute absence (zero) of \pseudotested{} methods.

\subsection{Conclusion}
\label{subsec:transversal-contributions:descartes:conclusion}

To conclude, our replication confirms that all Java projects contain \pseudotested{} methods, even the very well tested ones, ranging from 1\% to 46\% in our dataset.
Developers of 3 projects consider that 30\% methods were considered worth of additional testing actions

In the light of these conclusions, the immediate next step in our research agenda is to investigate an automatic test improvement technique targeted towards \pseudotested{} methods.
This technique shall kill two birds with one stone: improve the adequacy of the test suite for \pseudotested{} methods; 
let the developers focus their efforts on core features and relieve them from the test improvement task.
This technique shall kill two birds with one stone: improve the adequacy of the test suite for \pseudotested{} methods; let the developers focus their efforts on core features and relieve them from the test improvement task.

Descartes has been integrated as default test-criterion in \dspot.
It means that, \dspot amplifies the test suite in order to detect more extreme mutant and thus reduce the number of \pseudotested methods.

\section{Test For Repair}
\label{sec:transversal-contributions:test-for-repair}
\cite{Yu2019}

\subsection{Introduction}
\label{subsec:transversal-contributions:test-for-repair:introduction}
